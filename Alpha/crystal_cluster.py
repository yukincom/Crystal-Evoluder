"""
Crystal Cluster 
Knowledge Graph committer for Neo4j

"""

# ============================================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================
import json
import logging
import pickle
import numpy as np
import networkx as nx

from pathlib import Path
from typing import List, Dict, Any, Optional

from llama_index.llms.openai import OpenAI  
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.core import Document, KnowledgeGraphIndex, StorageContext

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from shared.logger import setup_logger, HierarchicalLogger
from shared.utils import load_and_validate_paths
from shared.error_handler import ErrorCollector, safe_execute

class CrystalCluster:
    """Crystal Cluster - Neo4jæŠ•å…¥å°‚ç”¨"""
    
    def __init__(self, log_level: int = logging.INFO):
        self.logger = setup_logger('CrystalCluster', log_level)
        self.hlogger = HierarchicalLogger(self.logger)

        self.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
            device="mps",
            embed_batch_size=16,
        )

        from llama_index.core import Settings
        Settings.embed_model = self.embed_model

        self.logger.info("Crystal Cluster v1.1 initialized")

    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg: Optional[nx.Graph] = None) -> List[Document]:
        """
        JSON ã¨ ç”Ÿãƒ†ã‚­ã‚¹ãƒˆä¸¡æ–¹ã‹ã‚‰ Document ã‚’ä½œã‚‹
        
        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            raw_docs: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            path_pickle: ãƒ‘ã‚¹æƒ…å ±ã®Pickleãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ‘ã‚¹æƒ…å ±çµ±åˆæ™‚ã«å¿…è¦ï¼‰
        
        Returns:
            Documentã®ãƒªã‚¹ãƒˆï¼ˆãƒ‘ã‚¹æƒ…å ±ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼‰
        """
        documents = []

        # --- JSON å´ ---
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # json_path ã® documents ã‚’è¿½åŠ 
        for i, doc in enumerate(data.get('documents', [])):
            documents.append(
                Document(
                    text=doc['text'],
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    }
                )
            )

        # --- ç”Ÿãƒ†ã‚­ã‚¹ãƒˆå´ ---
        if raw_docs:
            for i, text in enumerate(raw_docs):
                documents.append(
                    Document(
                        text=text,
                        metadata={
                            "source": "raw",
                            "raw_id": i
                        }
                    )
                )

        json_count = len(data.get('documents', []))
        raw_count = len(raw_docs) if raw_docs else 0
        
        self.logger.info(
            f"ğŸ“‚ Loaded {len(documents)} documents "
            f"({json_count} from JSON, {raw_count} raw texts)"
        )

        # --- ãƒ‘ã‚¹æƒ…å ±ã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰---
        if path_pickle and kg is not None:
            path_dicts = load_and_validate_paths(path_pickle, self.logger)
            if path_dicts:
                self.logger.info("Augmenting documents with path information...")
                documents = self.augment_documents_with_paths(
                    documents, 
                    path_dicts, 
                    kg,
                    entity_embeddings=getattr(self, 'entity_embeddings', None)
                )
                self.logger.info(f"âœ… Path information added to {len(documents)} documents")
            else:
                self.logger.warning("Path information could not be loaded, continuing without it")

        return documents

    def augment_documents_with_paths(
        self,
        documents: List[Document], 
        path_dicts: List[Dict], 
        kg: nx.Graph,
        entity_embeddings: Dict[str, np.ndarray] = None,
        match_key='question') -> List[Document]:
        """
        documents ã«å¯¾å¿œã™ã‚‹ path æƒ…å ±ã‚’æ³¨å…¥
        
        Args:
            documents: Documentã®ãƒªã‚¹ãƒˆ
            path_dicts: load_path_dicts ã®æˆ»ã‚Šå€¤
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            match_key: documents ã¨ path_dicts ã‚’çªãåˆã‚ã›ã‚‹ã‚­ãƒ¼
        
        Returns:
            ãƒ‘ã‚¹æƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸ documents

        """
        # defensive
        if entity_embeddings is None:
            entity_embeddings = {}

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼š path_dicts ã‚’ match_key ã§å¼•ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹
        pd_map = {}
        for p in path_dicts:
            key = p.get(match_key)
            if key is not None:
                pd_map[key] = p

        augmented = []
        matched_count = 0

        for doc in documents:
            meta = dict(getattr(doc, 'metadata', {}) or {})
            doc_key = meta.get(match_key)

            matched = None
            if doc_key is not None and doc_key in pd_map:
                matched = pd_map[doc_key]
                matched_count += 1
            else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆå†…ã« match_key ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ path_dict ã‚’æ¢ã™
                text = getattr(doc, 'text', '') or ''
                for k, p in pd_map.items():
                    if isinstance(k, str) and k in text:
                        matched = p
                        matched_count += 1
                        break

            paths_meta = []
            if matched:
                for path in matched.get('translated_paths', []):
                # path: list of node names (entities)
                    path_len = len(path)
                    edge_weights = []
                    path_node_pairs = list(zip(path[:-1], path[1:])) if path_len >= 2 else []
                    for u, v in path_node_pairs:
                        if kg.has_edge(u, v):
                            edge_weights.append(kg[u][v].get('weight', 0.0))
                        elif kg.has_edge(v, u):
                            edge_weights.append(kg[v][u].get('weight', 0.0))
                        else:
                        # edge ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 0.0 ã‚’å…¥ã‚Œã¦ãŠã
                            edge_weights.append(0.0)

                    avg_edge_weight = float(np.mean(edge_weights)) if edge_weights else 0.0
                    sum_edge_weight = float(np.sum(edge_weights)) if edge_weights else 0.0

                # path å†…ãƒãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°ã€ãƒãƒ¼ãƒ‰é–“é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆå¹³å‡ãƒšã‚¢é¡ä¼¼åº¦ï¼‰
                    pair_sims = []
                    for i in range(len(path) - 1):
                        e1 = entity_embeddings.get(path[i])
                        e2 = entity_embeddings.get(path[i + 1])
                        if e1 is not None and e2 is not None:
                        # safe numpy dot / norms
                            denom = (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-9)
                            pair_sims.append(float(np.dot(e1, e2) / denom))
                    avg_pair_sim = float(np.mean(pair_sims)) if pair_sims else None

                # æœ€çŸ­è·é›¢ï¼ˆkg ä¸Šï¼‰ â€” å­˜åœ¨ã—ãªã‘ã‚Œã° None
                    shortest = None
                    try:
                        if path_len >= 2:
                        # path ã®ç«¯åŒå£«ã®æœ€çŸ­é•·ã‚’è¨ˆç®—ï¼ˆä¾‹ï¼‰
                            s1, s2 = path[0], path[-1]
                            if kg.has_node(s1) and kg.has_node(s2):
                                shortest = int(nx.shortest_path_length(kg, s1, s2))
                    except (nx.NetworkXNoPath, nx.NodeNotFound):
                        pass
                    except Exception:
                        pass

                    paths_meta.append({
                        'path': path,
                        'path_length_nodes': path_len,
                        'avg_edge_weight': avg_edge_weight,
                        'sum_edge_weight': sum_edge_weight,
                        'avg_adjacent_node_sim': avg_pair_sim, 
                        'kg_shortest_between_ends': shortest
                    })

        # attach (æ—¢å­˜ metadata ã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼)
                new_meta = dict(meta)
                new_meta['paths'] = paths_meta
            # create a new Document preserving original text & adding metadata (or mutate in place if ok)
                new_doc = Document(text=getattr(doc, 'text', ''), metadata=new_meta)
                augmented.append(new_doc)
                
            return augmented

        # naive match: by ordering if no explicit key available
        if len(path_dicts) == 0:
            return documents

        if len(path_dicts) == len(documents):
            for i, doc in enumerate(documents):
                doc.metadata['paths'] = path_dicts[i].get('translated_paths', [])
                doc.metadata['path_distances'] = path_dicts[i].get('path_distances', [])
        else:
            # fallback: attach top global paths to every doc (still useful)
            sample_paths = path_dicts[0].get('translated_paths', [])
            for doc in documents:
                doc.metadata.setdefault('paths', sample_paths)
                doc.metadata.setdefault('path_distances', path_dicts[0].get('path_distances', []))
        return documents
    

    def commit_to_graph(self, documents: List[Document], graph_store: Neo4jGraphStore):
        """Neo4jã«ã‚°ãƒ©ãƒ•ã‚’æŠ•å…¥"""
        #ã€€æ¥ç¶šç¢ºèªã€€===========================================
        try:
            graph_store.query("RETURN 1")
            self.logger.info("âœ… Neo4j connection verified")
        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Neo4j connection failed: {type(e).__name__}\n"
                f"   Message: {str(e)[:200]}\n"
                f"   Check: URI, username, password, and service status"
            )
            raise  # æ¥ç¶šã§ããªã„ãªã‚‰å‡¦ç†ã‚’ä¸­æ–­
        # 2. ã‚°ãƒ©ãƒ•ç”Ÿæˆ ===========================================
        try:
            with self.hlogger.section("Graph Generation"):
                llm = OpenAI(model="gpt-4o-mini", timeout=120.0, output_version="v0")
                storage_context = StorageContext.from_defaults(graph_store=graph_store)
            
                index = KnowledgeGraphIndex.from_documents(
                    documents,
                    storage_context=storage_context,
                    llm=llm,
                    transformations=[SimpleNodeParser.from_defaults(chunk_size=512)],
                    embed_model=HuggingFaceEmbedding(model_name="BAAI/bge-m3"),
                    show_progress=True,
                    max_triplets_per_chunk=10
                )
            
                kg = index.get_networkx_graph()
                self.logger.info(f"âœ… Initial graph: {len(kg.nodes)} nodes, {len(kg.edges)} edges")
        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Graph generation failed: {type(e).__name__}\n"
                f"   Message: {str(e)[:200]}\n"
                f"   This might be due to: LLM API issues, invalid documents, or embedding model errors"
            )
            raise
        # 3. ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã«çµ±åˆã€€================================

        try:
            with self.hlogger.section("Merging Path Information"):
                self.merge_paths_into_kg(kg, documents)
                self.logger.info(f"âœ… Path info merged: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

        except Exception as e:
            self.logger.warning(
                f"âš ï¸  Path merging failed: {type(e).__name__} - {str(e)[:100]}\n"
                f"   Continuing without path information..."
            )

        # ============================================================
        # RAPLé¢¨æœ€é©åŒ–: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆé–“ã®ç›¸äº’ä½œç”¨ã‚’è¨ˆç®—
        # ============================================================
        try:
            with self.hlogger.section("Graph Optimization (RAPL)"):
                optimized_kg = self._optimize_graph_rapl(kg, documents)
                self.logger.info(
                    f"âœ… Optimized graph: {len(optimized_kg.nodes)} nodes, "
                    f"{len(optimized_kg.edges)} edges"
                )
        except Exception as e:
            self.logger.error(  
                        f"ğŸš¨ Graph optimization failed: {type(e).__name__}\n"
                        f"   Message: {str(e)[:200]}\n"
                        f"   Using unoptimized graph instead..."
            )
            optimized_kg = kg 
            # æœ€é©åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’Neo4jã«åæ˜ 
        try:
            with self.hlogger.section("Updating Neo4j"):
                result = self._update_neo4j_structure(optimized_kg, graph_store)
            
            # result ãŒ None ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if result is None:
                    result = {'updated': 0, 'skipped': 0, 'failed': 0, 'error_details': []}
                    self.logger.warning("âš ï¸  _update_neo4j_structure returned None")

            # çµæœã‚µãƒãƒªãƒ¼
                self.logger.info(
                    f"âœ… Neo4j update complete:\n"
                    f"   - Updated: {result.get('updated', 0)} edges\n"
                    f"   - Skipped: {result.get('skipped', 0)} edges\n"
                    f"   - Failed: {result.get('failed', 0)} edges"
                )
            
            # å¤±æ•—ç‡ãŒé«˜ã„å ´åˆã¯è­¦å‘Š
                total = result.get('updated', 0) + result.get('failed', 0)
                if total > 0 and result.get('failed', 0) / total > 0.3:
                    self.logger.warning(
                        f"âš ï¸  High failure rate ({result.get('failed', 0)/total:.1%}). "
                        f"Check Neo4j constraints and data format."
                    )
    
        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Neo4j update failed: {type(e).__name__}\n"
                f"   Message: {str(e)[:200]}\n"
                f"   Graph was optimized but not persisted to database!"
            )
            raise

    def merge_paths_into_kg(self, kg, documents: List[Document]):
        """
        kg: networkx.Graph (triples turned into nodes/edges)
        documents: the same documents that have metadata['paths'] etc.
        This will:
          - count how many times each entity appears in top-k paths
          - add edge/node attributes: top_path_count, avg_path_length
        """
        from collections import Counter, defaultdict
        path_entity_counts = Counter()
        entity_path_lengths = defaultdict(list)

        for doc in documents:
            paths = doc.metadata.get('paths', [])  # each path is a str like "A -> B -> C" OR list; adapt if needed
            distances = doc.metadata.get('path_distances', [])
            for i, p in enumerate(paths):
                # normalize path representation
                if isinstance(p, str):
                    nodes = [n.strip() for n in p.split('->') if n.strip()]
                elif isinstance(p, (list, tuple)):
                   nodes = list(p)
                else:
                    continue

                dist = distances[i] if i < len(distances) else len(nodes)-1
                for n in nodes:
                    path_entity_counts[n] += 1
                    entity_path_lengths[n].append(dist)

                # if the path describes relations, you could also add edges for consecutive nodes
                for a, b in zip(nodes, nodes[1:]):
                    if kg.has_edge(a, b):
                        # add a path_support counter on existing edge
                        kg[a][b].setdefault('path_support', 0)
                        kg[a][b]['path_support'] += 1
                    else:
                        kg.add_edge(a, b, relation='path_inferred', path_support=1)

        # inject aggregated attrs to nodes
        for n in kg.nodes():
            cnt = path_entity_counts.get(n, 0)
            lens = entity_path_lengths.get(n, [])
            avg_len = sum(lens)/len(lens) if lens else None
            kg.nodes[n]['path_top_count'] = cnt
            if avg_len is not None:
                kg.nodes[n]['path_avg_length'] = avg_len

    
    def _optimize_graph_rapl(self, kg, documents):
        """
        å®Œå…¨çµ±åˆç‰ˆ RAPL æœ€é©åŒ–
        """
        from collections import defaultdict
    
    # ============================================================
    # 1. Triples æŠ½å‡º
    # ============================================================
        doc_triples = {}
        for idx, doc in enumerate(documents):
            triples = doc.metadata.get("triples", [])
            if triples:  # ç©ºãƒªã‚¹ãƒˆã¯é™¤å¤–
                doc_triples[idx] = triples
        
        all_triples = [t for lst in doc_triples.values() for t in lst]
    
        self.logger.info(f"Total triples: {len(all_triples)}")
    
    # Weight æ ¼ç´é ˜åŸŸã®åˆæœŸåŒ–
        for u, v in kg.edges():
            kg[u][v]["intra_raw"] = 0.0
            kg[u][v]["inter_raw"] = 0.0
    
    # ============================================================
    # 2. Intra: æ–‡æ›¸å†… triple é–“ç›¸äº’ä½œç”¨
    # ============================================================
        self.logger.info("Computing intra-interactions...")
        intra_collector = ErrorCollector(self.logger)
        intra_edges = 0
    
        for doc_id, triples in doc_triples.items():
            try:
                entities = set()
                for s, _, o in triples:
                    entities.add(s)
                    entities.add(o)
        
        # Triple é–“ã®ç›¸äº’ä½œç”¨ï¼ˆé–¢ä¿‚ã®ç›¸æ€§ã‚’è€ƒæ…®ï¼‰
                for i in range(len(triples)):
                    s1, r1, o1 = triples[i]
                    for j in range(i + 1, len(triples)):
                        s2, r2, o2 = triples[j]
                
                # é–¢ä¿‚ã®ç›¸æ€§
                        try:
                            rel_compat = self._compute_relation_compatibility(r1, r2)
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…±æœ‰åº¦
                            shared = len({s1, o1} & {s2, o2})
                            shared_score = shared * 0.5
                
                # çµ±åˆé‡ã¿
                            w = rel_compat * 0.6 + shared_score * 0.4
                
                            if w > 0.3:
                                if kg.has_edge(s1, o1):
                                    kg[s1][o1]["intra_raw"] += w
                                if kg.has_edge(s2, o2):
                                    kg[s2][o2]["intra_raw"] += w
                            intra_collector.add_success()
                        except Exception as e:
                            intra_collector.add_error(
                                context=f"doc_{doc_id}_triple_{i}_{j}",
                                error=e,
                                triple1=(s1, r1, o1),
                                triple2=(s2, r2, o2)
                            )
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢é–“ã®ã‚¨ãƒƒã‚¸è¿½åŠ 
                for e1 in entities:
                    for e2 in entities:
                        if e1 != e2:
                            try:
                                w = self._compute_intra_weight(e1, e2, triples, kg)
                                if w > 0.5:
                                    if kg.has_edge(e1, e2):
                                        kg[e1][e2]["weight"] = kg[e1][e2].get("weight", 0) + w
                                    else:
                                        kg.add_edge(e1, e2, relation="intra_doc", weight=w)
                                        intra_edges += 1
                            except Exception as e:
                                intra_collector.add_error(
                                    context=f"entity_pair_{e1}_{e2}",
                                    error=e
                                )
            except Exception as e:
                self.logger.error(f"Failed to process document {doc_id}: {type(e).__name__}")
                continue

        intra_collector.report("Intra-document processing", threshold=0.3)
        self.logger.info(f"Added {intra_edges} intra-document edges")
    
    # ============================================================
    # 3. Inter: å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é«˜é€ŸåŒ–
    # ============================================================
        self.logger.info("Computing inter-interactions (optimized)...")
        inter_collector = ErrorCollector(self.logger)
    
    # 3-1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£â†’Triple ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        entity_to_triples = defaultdict(set)
        for idx, (s, r, o) in enumerate(all_triples):
            entity_to_triples[s].add(idx)
            entity_to_triples[o].add(idx)
    
        self.logger.info(f"Built entity index: {len(entity_to_triples)} unique entities")
    
    # 3-2. å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹ Triple ãƒšã‚¢ã®ã¿è¨ˆç®—
        seen_pairs = set()
        inter_count = 0
    
        for entity, triple_indices in entity_to_triples.items():
            if len(triple_indices) < 2:
                continue  # 1ã¤ã® Triple ã«ã—ã‹å‡ºç¾ã—ãªã„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ã‚¹ã‚­ãƒƒãƒ—
        
            indices = list(triple_indices)
            for i in range(len(indices)):
                for j in range(i + 1, len(indices)):
                    idx1, idx2 = indices[i], indices[j]
                
                # ãƒšã‚¢ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ãƒšã‚¢ã‚’è¤‡æ•°å›è¨ˆç®—ã—ãªã„ï¼‰
                    pair = (min(idx1, idx2), max(idx1, idx2))
                    if pair in seen_pairs:
                        continue
                    seen_pairs.add(pair)
                
                # é‡ã¿è¨ˆç®—
                    try:
                        t1 = all_triples[idx1]
                        t2 = all_triples[idx2]
                        w = self._compute_inter_weight(t1, t2, kg=kg)
                
                        if w > 0.2:
                            s1, _, o1 = t1
                            s2, _, o2 = t2
                    
                    # åŒæ–¹å‘ã«é‡ã¿ã‚’åŠ ç®—
                            if kg.has_edge(s1, o1):
                                kg[s1][o1]["inter_raw"] = kg[s1][o1].get("inter_raw", 0.0) + w
                            if kg.has_edge(s2, o2):
                                kg[s2][o2]["inter_raw"] = kg[s2][o2].get("inter_raw", 0.0) + w
                    
                            inter_count += 1
                        inter_collector.add_success()

                    except Exception as e:
                        inter_collector.add_error(
                            context=f"triple_pair_{idx1}_{idx2}",
                            error=e
                        )

        inter_collector.report("Inter-document processing", threshold=0.3)
        self.logger.info(f"Added {inter_count} meaningful inter-interactions")
    
    # ============================================================
    # 4. Document-level linking
    # ============================================================
        self.logger.info("Computing document-level connections...")
    
        try:
            entity_docs = {}
            for doc_id, triples in doc_triples.items():
                for s, _, o in triples:
                    entity_docs.setdefault(s, set()).add(doc_id)
                    entity_docs.setdefault(o, set()).add(doc_id)
    
            doc_pairs = {}
            bridge_entities = []
    
            for entity_name, doc_set in entity_docs.items():
                if len(doc_set) > 1:
                    docs = list(doc_set)
                    for i, d1 in enumerate(docs):
                        for d2 in docs[i+1:]:
                            pair = (d1, d2)
                            doc_pairs[pair] = doc_pairs.get(pair, 0) + 1
            
                    if len(doc_set) > 2:
                        bridge_entities.append((entity_name, len(doc_set)))

                # ãƒ–ãƒªãƒƒã‚¸ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ­ã‚°
            if bridge_entities:
                bridge_entities.sort(key=lambda x: x[1], reverse=True)
                self.logger.info("Top bridge entities:")
                for entity_name, count in bridge_entities[:5]:
                    self.logger.info(f"  '{entity_name}': {count} documents")

            inter_doc_count = 0
            for (d1, d2), ct in doc_pairs.items():
                if ct > 2:
                    n1 = f"doc_{d1}"
                    n2 = f"doc_{d2}"
                
                    if not kg.has_node(n1):
                        kg.add_node(n1, type="document")
                    if not kg.has_node(n2):
                        kg.add_node(n2, type="document")
                
                    kg.add_edge(n1, n2, relation="inter_doc", weight=ct)
                    inter_doc_count += 1
            self.logger.info(f"Added {inter_doc_count} inter-document links")

        except Exception as e:
            self.logger.error(f"Document linking failed: {type(e).__name__} - {str(e)[:100]}")
 
    # ============================================================
    # 5. çµ±åˆé‡ã¿ï¼ˆRAPLå¼ï¼‰
    # ============================================================
        self.logger.info("Finalizing edge weights...")
    
        for u, v, d in kg.edges(data=True):
            intra = d.get("intra_raw", 0.0)
            inter = d.get("inter_raw", 0.0)
        
        # RAPLè«–æ–‡: intraé‡è¦– + interè£œå®Œ
            d["weight"] = min(0.7 * intra + 0.3 * inter, 1.0)    
        return kg
    
    def _group_triples_by_document(self, kg, documents):
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’Documentåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        # ç°¡æ˜“å®Ÿè£…: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®š
        doc_triples = {}
        
        for i, doc in enumerate(documents):
            doc_id = f"doc_{i}"
            doc_triples[doc_id] = []
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒDocumentå†…ã«å‡ºç¾ã™ã‚‹ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º
            for s, o, data in kg.edges(data=True):
                if s in doc.text or o in doc.text:
                    doc_triples[doc_id].append((s, data.get('relation', ''), o))
        return doc_triples
    
    def _compute_intra_weight(self, e1: str, e2: str, triples: List, kg=None) -> float:
        """
        åŒä¸€Documentå†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“é‡ã¿è¨ˆç®—
        
        Args:
            e1, e2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å
            triples: (s, r, o) ã®ãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
    # ------------------------------------------------------------
    # 1) å…±èµ·é »åº¦ï¼ˆåŸºæœ¬ï¼‰
    # ------------------------------------------------------------
        cooccur = sum(
            1 for s, _, o in triples
            if (s == e1 and o == e2) or (s == e2 and o == e1)
        )
        co_norm = min(cooccur / 5.0, 1.0)   # æ­£è¦åŒ–

    # ------------------------------------------------------------
    # 2) é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§
    # ------------------------------------------------------------
        rel_pairs = [
            (r, True) for s, r, o in triples
            if (s == e1 and o == e2)
        ] + [
            (r, False) for s, r, o in triples
            if (s == e2 and o == e1)  # é€†å‘ã
        ]
    
        if not rel_pairs:
            rel_bonus = 0.0
        else:
            # é–¢ä¿‚ã®å¤šæ§˜æ€§
            unique_rels = set(r for r, _ in rel_pairs)
            diversity_bonus = min(len(unique_rels) * 0.2, 0.6)

        # æ–¹å‘ã®ä¸€è²«æ€§ï¼ˆåŒã˜å‘ããŒå¤šã„ã»ã©å¼·ã„é–¢ä¿‚ï¼‰
            same_direction_count = sum(1 for _, is_forward in rel_pairs if is_forward)
            opposite_direction_count = len(rel_pairs) - same_direction_count

        # é–¢ä¿‚ã®è³ªï¼ˆåŒã˜å‘ãã‹é€†å‘ãã‹ã§è©•ä¾¡ï¼‰
            if same_direction_count > opposite_direction_count:
                direction_score = same_direction_count / len(rel_pairs)
            else:
            # é€†æ–¹å‘ãŒå¤šã„ = åŒæ–¹å‘ã®é–¢ä¿‚ï¼ˆã“ã‚Œã‚‚æœ‰ç”¨ï¼‰
                direction_score = 0.7  # ã‚„ã‚„é«˜ã‚ã«è©•ä¾¡

            rel_bonus = diversity_bonus * 0.5 + direction_score * 0.5

    # ------------------------------------------------------------
    # 3) ãƒ‘ã‚¹ã‚µãƒãƒ¼ãƒˆï¼ˆkgã« path_support ãŒã‚ã‚Œã°ï¼‰
    # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None and kg.has_edge(e1, e2):
            path_bonus = min(kg[e1][e2].get("path_support", 0) * 0.1, 0.5)

    # ------------------------------------------------------------
    # 4) åˆæˆ
    # ------------------------------------------------------------
        weight = co_norm * 0.5 + rel_bonus * 0.4 + path_bonus * 0.1
        return min(weight, 1.0)


    def _compute_inter_weight(self, t1: tuple, t2: tuple, kg=None, embed_fn=None) -> float:
        """
        Compute inter-triple interaction weight between two triples.
        t1, t2: (s, r, o)
        kg: optional networkx graph
        embed_fn: optional function to compute embeddings of entity names
        """

        s1, r1, o1 = t1
        s2, r2, o2 = t2

        # ------------------------------------------------------------
        # 1) å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆæœ€é‡è¦ï¼‰
        # ------------------------------------------------------------
        shared = len({s1, o1} & {s2, o2})
        shared_bonus = min(shared * 0.5, 1.0)
        # é–¢ä¿‚ã®ç›¸æ€§è¨ˆç®—
        rel_compatibility = safe_execute(
            self._compute_relation_compatibility,
            args=(r1, r2),
            default=0.3,  # å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            logger=self.logger,
            context=f"relation_compatibility({r1}, {r2})"
        )
        # ------------------------------------------------------------
        # 2) ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦ï¼ˆembeddingã‚’æ¸¡ã•ã‚ŒãŸã‚‰ä½¿ã†ï¼‰
        # ------------------------------------------------------------
        sim_bonus = 0.0
        if embed_fn is not None:
            try:        
                e1 = embed_fn(s1)
                e2 = embed_fn(s2)

                # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ãƒã‚§ãƒƒã‚¯
                norm1 = np.linalg.norm(e1)
                norm2 = np.linalg.norm(e2)
            
                if norm1 > 1e-9 and norm2 > 1e-9:
                    sim = (e1 @ e2) / (norm1 * norm2)
                    sim_bonus = max(sim, 0) * 0.3
                else:
                    self.logger.debug(
                        f"Zero embedding detected: {s1} (norm={norm1:.2e}) "
                        f"or {s2} (norm={norm2:.2e})"
                    )
            except Exception as e:
                self.logger.debug(
                    f"Embedding similarity failed for ({s1}, {s2}): "
                    f"{type(e).__name__}"
                )

        # ------------------------------------------------------------
        # 3) graph path-based supportï¼ˆkgãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
        #    2ã€œ3 hop ä»¥å†…ã«ç¹‹ãŒã‚‹ã‹ã‚’è¦‹ã‚‹
        # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None:
            try:
            # 2-hopä»¥å†…ã§ã¤ãªãŒã£ã¦ãŸã‚‰è©•ä¾¡        
                if kg.has_node(s1) and kg.has_node(s2):
                    length = nx.shortest_path_length(kg, s1, s2)
                    if length <= 2:
                        path_bonus = 0.3 * (1.0 - length / 3.0)  # è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            except nx.NetworkXNoPath:
                # ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0.0ã®ã¾ã¾ï¼ˆã“ã‚Œã¯æ­£å¸¸ï¼‰
                pass
            except nx.NodeNotFound as e:
                self.logger.debug(f"Node not found in graph: {e}")
            except Exception as e:
                self.logger.debug(
                    f"Path calculation failed ({s1}->{s2}): {type(e).__name__}"
                )

        # ------------------------------------------------------------
        # 4) ç·åˆ
        # ------------------------------------------------------------
        w = (
            shared_bonus * 0.4 +       # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            rel_compatibility * 0.3 +   # é–¢ä¿‚ã®ç›¸æ€§ï¼ˆã“ã“ã«çµ±åˆæ¸ˆã¿ï¼‰
            sim_bonus * 0.2 +           # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
            path_bonus * 0.1            # ãƒ‘ã‚¹è·é›¢
        )

        return min(w, 1.0)
    
    def _compute_relation_compatibility(self, r1: str, r2: str) -> float:
        """
        é–¢ä¿‚ã®ç›¸æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ‰‹å‹•ãƒ«ãƒ¼ãƒ« + embeddingãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢çµ±ä¸€ï¼‰
        r1 = r1.lower().replace('-', '_')
        r2 = r2.lower().replace('-', '_')
    # 1. å®Œå…¨ä¸€è‡´
        if r1 == r2:
            return 1.0
    
    # 2. é€†é–¢ä¿‚ã®ãƒšã‚¢ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        inverse_pairs = {
            ("cause_of", "caused_by"),
            ("cause_of", "effect_of"), 
            ("part_of", "has_part"),
            ("component_of", "has_component"),
            ("parent_of", "child_of"),
            ("author_of", "written_by"),
            ("owns", "owned_by"),
            ("manages", "managed_by"),
            ("teaches", "taught_by"),
            ("supervises", "supervised_by"),
        }
    
        if (r1, r2) in inverse_pairs or (r2, r1) in inverse_pairs:
            return 0.9
    
    # 3. é–¢é€£ã™ã‚‹é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        related_groups = [
        # å› æœé–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "cause_of", "caused_by", "leads_to", "results_in", 
                "triggers", "produces", "generates", "effect_of"
            },
        
        # æ§‹æˆè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "part_of", "has_part", "component_of", "has_component",
                "contains", "includes", "consists_of", "comprises"
            },
        
        # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "member_of", "has_member", "belongs_to", "works_at", 
                "employed_by", "affiliated_with"
            },
        
        # æ™‚é–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "before", "after", "during", "precedes", "follows",
                "happens_before", "happens_after"
            },
        
        # ç©ºé–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "located_in", "location_of", "near", "adjacent_to",
                "contains", "inside", "outside"
            },
        
        # å±æ€§ãƒ»æ€§è³ªã‚°ãƒ«ãƒ¼ãƒ—
            {
                "is_a", "type_of", "instance_of", "has_property",
                "characterized_by", "defined_by"
            },
        
        # ç›¸äº’ä½œç”¨ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "interacts_with", "collaborates_with", "competes_with",
                "influences", "affected_by"
            },
        ]
    
        for group in related_groups:
            if r1 in group and r2 in group:
                return 0.7
    
    # 4. åŒã˜ã‚«ãƒ†ã‚´ãƒªï¼ˆå‹•è©ã®æ€§è³ªã§åˆ¤å®šï¼‰
    # ä¾‹: action ç³»ã€state ç³»ãªã©
        action_verbs = {
            "creates", "builds", "develops", "produces", "makes",
            "constructs", "designs", "implements", "generates"
        }
    
        state_verbs = {
            "is", "has", "contains", "includes", "comprises",
            "exists", "represents", "defines"
        }
    
        relation_verbs = {
            "relates_to", "associated_with", "connected_to",
            "linked_to", "corresponds_to"
        }
    
        if (r1 in action_verbs and r2 in action_verbs) or \
           (r1 in state_verbs and r2 in state_verbs) or \
           (r1 in relation_verbs and r2 in relation_verbs):
            return 0.5
    
    # 5. ãã‚Œä»¥å¤–ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
        try:
            emb1 = self.relation_embedder.get_text_embedding(r1)
            emb2 = self.relation_embedder.get_text_embedding(r2)
            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)
            return max(0.3, float(sim))
        except Exception:
            return 0.3


    def _update_neo4j_structure(self, kg, graph_store):
        query_template = """
        MERGE (a:Concept {name: $source})
        MERGE (b:Concept {name: $target})
        MERGE (a)-[r:RELATED]->(b)
        ON CREATE SET r.weight = $weight
        ON MATCH SET r.weight = $weight
        """
        collector = ErrorCollector(self.logger)

        for s, o, data in kg.edges(data=True):
            weight = data.get('weight', 0.0)

            if weight <= 0: 
                collector.add_skip()
                continue  # é‡ã¿0ã¯ç„¡è¦–
            
            try:
                graph_store.query(query_template, {
                    'source': s,
                    'target': o,
                    'weight': float(weight)
                })
                collector.add_success()

            except Exception as e:
                collector.add_error(
                    context=f"{s} -> {o}",
                    error=e,
                    weight=weight  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²
                )
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆè‡ªå‹•ã§ãƒ­ã‚°å‡ºåŠ›ï¼‰
        collector.report("Neo4j edge update", threshold=0.3)
    # æˆ»ã‚Šå€¤ã‚‚å–å¾—å¯èƒ½
        return collector.get_summary()        

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Crystal Cluster v1.1')
    parser.add_argument('json_file', help='Clean documents JSON file')
    parser.add_argument('--neo4j-uri', default='bolt://localhost:7687')
    parser.add_argument('--neo4j-user', default='neo4j')
    parser.add_argument('--neo4j-pass', required=True)
    parser.add_argument('--debug', action='store_true')
    
    args = parser.parse_args()
    
    print("ğŸ’¾ Crystal Cluster v1.1")
    print("â”" * 42)
    
    cluster = CrystalCluster(log_level=logging.DEBUG if args.debug else logging.INFO)
    
    documents = cluster.load_documents(args.json_file)
    
    graph_store = Neo4jGraphStore(
        username=args.neo4j_user,
        password=args.neo4j_pass,
        url=args.neo4j_uri
    )
    
    cluster.commit_to_graph(documents, graph_store)
    
    print("âœ¨ Complete!")