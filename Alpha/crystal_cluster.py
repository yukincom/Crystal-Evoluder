"""
Crystal Cluster beta
Knowledge Graph committer for Neo4j

"""

# ============================================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================
import json
import logging
import pickle
import numpy as np
import networkx as nx
import hashlib

from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set

from llama_index.llms.openai import OpenAI  
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.core import Document, KnowledgeGraphIndex, StorageContext
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.core.node_parser import SentenceSplitter

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from shared.logger import setup_logger, HierarchicalLogger
from shared.utils import load_and_validate_paths
from shared.error_handler import ErrorCollector, safe_execute

class CrystalCluster:
    """Crystal Cluster - Neo4jæŠ•å…¥å°‚ç”¨"""
    
    def __init__(self, log_level: int = logging.INFO, use_dual_chunk: bool = False):
        """
        Args:
            use_dual_chunk: Trueãªã‚‰ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        """
        self.logger = setup_logger('CrystalCluster', log_level)
        self.hlogger = HierarchicalLogger(self.logger)
        self.use_dual_chunk = use_dual_chunk    

        self.config = {
            # Entity Linking
            'entity_linking_threshold': 0.88,  # 0.95 â†’ 0.88
            
            # Retrieval chunk
            'retrieval_chunk_size': 512,  # 1024 â†’ 512
            'retrieval_chunk_overlap': 100,  # 200 â†’ 100
            
            # Graph chunk
            'graph_chunk_size': 512,
            'graph_chunk_overlap': 50,
            
            # RAPLæœ€é©åŒ–
            'relation_compat_threshold': 0.08,  # 0.2 â†’ 0.08
            'final_weight_cutoff': 0.02,  # 0.05 â†’ 0.02
            
            # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæŠ½å‡º
            'max_triplets_per_chunk': 15,  # 10 â†’ 15
            
            # LLMé¸æŠ
            'llm_model': 'gpt-4o-mini',  # å¾Œã§UIé¸æŠå¯èƒ½ã«
            'llm_timeout': 120.0
        }

        self.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
            device="mps",
            embed_batch_size=16,
        )

        from llama_index.core import Settings
        Settings.embed_model = self.embed_model

        self.logger.info(f"Crystal Cluster beta initialized")
        self.logger.info(f"Config: {self.config}")

    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg: Optional[nx.Graph] = None) -> List[Document]:
        """
        JSON ã¨ ç”Ÿãƒ†ã‚­ã‚¹ãƒˆä¸¡æ–¹ã‹ã‚‰ Document ã‚’ä½œã‚‹
        
        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            raw_docs: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            path_pickle: ãƒ‘ã‚¹æƒ…å ±ã®Pickleãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ‘ã‚¹æƒ…å ±çµ±åˆæ™‚ã«å¿…è¦ï¼‰
        
        Returns:
            Documentã®ãƒªã‚¹ãƒˆï¼ˆãƒ‘ã‚¹æƒ…å ±ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼‰
        """
        documents = []

        # --- JSON å´ ---
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # json_path ã® documents ã‚’è¿½åŠ 
        for i, doc in enumerate(data.get('documents', [])):
            documents.append(
                Document(
                    text=doc['text'],
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    }
                )
            )

        # --- ç”Ÿãƒ†ã‚­ã‚¹ãƒˆå´ ---
        if raw_docs:
            for i, text in enumerate(raw_docs):
                documents.append(
                    Document(
                        text=text,
                        metadata={
                            "source": "raw",
                            "raw_id": i
                        }
                    )
                )

        json_count = len(data.get('documents', []))
        raw_count = len(raw_docs) if raw_docs else 0
        
        self.logger.info(
            f"ğŸ“‚ Loaded {len(documents)} documents "
            f"({json_count} from JSON, {raw_count} raw texts)"
        )

        # --- ãƒ‘ã‚¹æƒ…å ±ã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰---
        if path_pickle and kg is not None:
            path_dicts = load_and_validate_paths(path_pickle, self.logger)
            if path_dicts:
                self.logger.info("Augmenting documents with path information...")
                documents = self.augment_documents_with_paths(
                    documents, 
                    path_dicts, 
                    kg,
                    entity_embeddings=getattr(self, 'entity_embeddings', None)
                )
                self.logger.info(f"âœ… Path information added to {len(documents)} documents")
            else:
                self.logger.warning("Path information could not be loaded, continuing without it")

        return documents

    def augment_documents_with_paths(
        self,
        documents: List[Document], 
        path_dicts: List[Dict], 
        kg: nx.Graph,
        entity_embeddings: Dict[str, np.ndarray] = None,
        match_key='question') -> List[Document]:
        """
        documents ã«å¯¾å¿œã™ã‚‹ path æƒ…å ±ã‚’æ³¨å…¥
        
        Args:
            documents: Documentã®ãƒªã‚¹ãƒˆ
            path_dicts: load_path_dicts ã®æˆ»ã‚Šå€¤
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            match_key: documents ã¨ path_dicts ã‚’çªãåˆã‚ã›ã‚‹ã‚­ãƒ¼
        
        Returns:
            ãƒ‘ã‚¹æƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸ documents

        """
        # defensive
        if entity_embeddings is None:
            entity_embeddings = {}

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼š path_dicts ã‚’ match_key ã§å¼•ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹
        pd_map = {}
        for p in path_dicts:
            key = p.get(match_key)
            if key is not None:
                pd_map[key] = p

        augmented = []
        matched_count = 0

        for doc in documents:
            meta = dict(getattr(doc, 'metadata', {}) or {})
            doc_key = meta.get(match_key)

            matched = None
            if doc_key is not None and doc_key in pd_map:
                matched = pd_map[doc_key]
                matched_count += 1
            else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆå†…ã« match_key ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ path_dict ã‚’æ¢ã™
                text = getattr(doc, 'text', '') or ''
                for k, p in pd_map.items():
                    if isinstance(k, str) and k in text:
                        matched = p
                        matched_count += 1
                        break

            paths_meta = []
            if matched:
                for path in matched.get('translated_paths', []):
                # path: list of node names (entities)
                    path_len = len(path)
                    edge_weights = []
                    path_node_pairs = list(zip(path[:-1], path[1:])) if path_len >= 2 else []
                    for u, v in path_node_pairs:
                        if kg.has_edge(u, v):
                            edge_weights.append(kg[u][v].get('weight', 0.0))
                        elif kg.has_edge(v, u):
                            edge_weights.append(kg[v][u].get('weight', 0.0))
                        else:
                        # edge ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 0.0 ã‚’å…¥ã‚Œã¦ãŠã
                            edge_weights.append(0.0)

                    avg_edge_weight = float(np.mean(edge_weights)) if edge_weights else 0.0
                    sum_edge_weight = float(np.sum(edge_weights)) if edge_weights else 0.0

                # path å†…ãƒãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°ã€ãƒãƒ¼ãƒ‰é–“é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆå¹³å‡ãƒšã‚¢é¡ä¼¼åº¦ï¼‰
                    pair_sims = []
                    for i in range(len(path) - 1):
                        e1 = entity_embeddings.get(path[i])
                        e2 = entity_embeddings.get(path[i + 1])
                        if e1 is not None and e2 is not None:
                        # safe numpy dot / norms
                            denom = (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-9)
                            pair_sims.append(float(np.dot(e1, e2) / denom))
                    avg_pair_sim = float(np.mean(pair_sims)) if pair_sims else None

                # æœ€çŸ­è·é›¢ï¼ˆkg ä¸Šï¼‰ â€” å­˜åœ¨ã—ãªã‘ã‚Œã° None
                    shortest = None
                    try:
                        if path_len >= 2:
                        # path ã®ç«¯åŒå£«ã®æœ€çŸ­é•·ã‚’è¨ˆç®—ï¼ˆä¾‹ï¼‰
                            s1, s2 = path[0], path[-1]
                            if kg.has_node(s1) and kg.has_node(s2):
                                shortest = int(nx.shortest_path_length(kg, s1, s2))
                    except (nx.NetworkXNoPath, nx.NodeNotFound):
                        pass
                    except Exception:
                        pass

                    paths_meta.append({
                        'path': path,
                        'path_length_nodes': path_len,
                        'avg_edge_weight': avg_edge_weight,
                        'sum_edge_weight': sum_edge_weight,
                        'avg_adjacent_node_sim': avg_pair_sim, 
                        'kg_shortest_between_ends': shortest
                    })

        # attach (æ—¢å­˜ metadata ã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼)
                new_meta = dict(meta)
                new_meta['paths'] = paths_meta
            # create a new Document preserving original text & adding metadata (or mutate in place if ok)
                new_doc = Document(text=getattr(doc, 'text', ''), metadata=new_meta)
                augmented.append(new_doc)
                
            return augmented

        # naive match: by ordering if no explicit key available
        if len(path_dicts) == 0:
            return documents

        if len(path_dicts) == len(documents):
            for i, doc in enumerate(documents):
                doc.metadata['paths'] = path_dicts[i].get('translated_paths', [])
                doc.metadata['path_distances'] = path_dicts[i].get('path_distances', [])
        else:
            # fallback: attach top global paths to every doc (still useful)
            sample_paths = path_dicts[0].get('translated_paths', [])
            for doc in documents:
                doc.metadata.setdefault('paths', sample_paths)
                doc.metadata.setdefault('path_distances', path_dicts[0].get('path_distances', []))
        return documents
    
    def _generate_chunk_id(self, text: str, source_id: str, index: int) -> str:
        """
        ãƒãƒ£ãƒ³ã‚¯ã®ä¸€æ„ãªIDã‚’ç”Ÿæˆ
        
        Args:
            text: ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            source_id: å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ID
            index: ãƒãƒ£ãƒ³ã‚¯ç•ªå·
        
        Returns:
            'doc123_chunk5_a7f3e9b2' ã®ã‚ˆã†ãªä¸€æ„ID
        """
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ï¼ˆæœ€åˆã®100æ–‡å­—ã‹ã‚‰ï¼‰
        text_hash = hashlib.md5(text[:100].encode()).hexdigest()[:8]
        return f"{source_id}_chunk{index}_{text_hash}"
    
    # ============================================================
    # Dual-documents ç”Ÿæˆ
    # ============================================================
    def create_dual_documents(
        self,
        documents: List[Document]) -> Tuple[List[Document], List[Document]]:
        """
        æ—¢å­˜ã®Documentã‹ã‚‰ Graphç”¨ ã¨ Retrievalç”¨ ã®2ç¨®é¡ã‚’ä½œã‚‹
        
        Args:
            documents: load_documents() ã§ä½œæˆã—ãŸDocumentãƒªã‚¹ãƒˆ
        
        Returns:
            (graph_docs, retrieval_docs)
        """
        if not self.use_dual_chunk:
            # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ç„¡åŠ¹æ™‚ã¯å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
            return documents, documents
        
        graph_splitter, retrieval_splitter = self.get_dual_splitters()
        graph_docs = []
        retrieval_docs = []
        
        for doc in documents:
            base_meta = dict(doc.metadata)
            
            # ------------------------------------------------------------
            # Graphç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå°ã•ã‚ï¼‰
            # ------------------------------------------------------------
            try:
                graph_nodes = graph_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(graph_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'structural',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    graph_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Graph splitting failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã†
                md = dict(base_meta)
                md['chunk_type'] = 'structural'
                graph_docs.append(Document(text=doc.text, metadata=md))
            
            # ------------------------------------------------------------
            # Retrievalç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå¤§ãã‚ï¼‰
            # ------------------------------------------------------------
            try:
                retrieval_nodes = retrieval_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(retrieval_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'semantic',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    retrieval_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Retrieval splitting failed: {e}")
                md = dict(base_meta)
                md['chunk_type'] = 'semantic'
                retrieval_docs.append(Document(text=doc.text, metadata=md))
        
        self.logger.info(
            f"ğŸ“„ Created {len(graph_docs)} graph chunks, "
            f"{len(retrieval_docs)} retrieval chunks"
        )
        
        return graph_docs, retrieval_docs

    def _find_overlapping_chunks(
        self,
        start: int,
        end: int,
        graph_docs: List[Document]
    ) -> List[str]:
        """
        æŒ‡å®šç¯„å›²ã¨é‡ãªã‚‹Graphãƒãƒ£ãƒ³ã‚¯ã®IDã‚’è¿”ã™
        
        Args:
            start, end: æ–‡å­—ä½ç½®
            graph_docs: åŒä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®Graphãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        
        Returns:
            é‡ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã®IDãƒªã‚¹ãƒˆ
        """
        overlapping = []
        
        for doc in graph_docs:
            g_start = doc.metadata.get('start_char', 0)
            g_end = doc.metadata.get('end_char', 0)
            
            # ç¯„å›²ã®é‡ãªã‚Šãƒã‚§ãƒƒã‚¯
            if not (end <= g_start or start >= g_end):
                overlapping.append(doc.metadata['chunk_id'])
        
        return overlapping
    

    # ============================================================
    # ä¿®æ­£1: get_dual_splittersï¼ˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª¿æ•´ï¼‰
    # ============================================================
    
    def get_dual_splitters(self) -> Tuple[SentenceSplitter, SentenceSplitter]:
        """
        Graphç”¨ã¨Retrievalç”¨ã®2ç³»çµ±ã‚’è¿”ã™ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç‰ˆï¼‰
        """
        # Graphç”¨ï¼šå°ã•ã‚ãƒãƒ£ãƒ³ã‚¯
        graph_splitter = SentenceSplitter(
            chunk_size=self.config['graph_chunk_size'],
            chunk_overlap=self.config['graph_chunk_overlap'],
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )
        
        # Retrievalç”¨ï¼šä¸­ã‚µã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯ï¼ˆ512ã«å¤‰æ›´ï¼‰
        retrieval_splitter = SentenceSplitter(
            chunk_size=self.config['retrieval_chunk_size'],  # 512
            chunk_overlap=self.config['retrieval_chunk_overlap'],  # 100
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )
        
        self.logger.info(
            f"Splitters: graph={self.config['graph_chunk_size']}, "
            f"retrieval={self.config['retrieval_chunk_size']}"
        )
        
        return graph_splitter, retrieval_splitter


    # ============================================================
    # Retrieveé–¢æ•°ã‚’æ‹¡å¼µï¼ˆGraph nodeæƒ…å ±ã‚’ä»˜ä¸ï¼‰
    # ============================================================
    
    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5,
        chunk_mapping: Dict = None
    ) -> List[Tuple[float, Document, List[str]]]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¤œç´¢
        
        **æ‹¡å¼µç‚¹:**
        - å„çµæœã«å¯¾å¿œã™ã‚‹graph_chunk_idsã‚’ä»˜ä¸
        
        Returns:
            [(score, Document, graph_chunk_ids), ...] ã®ãƒªã‚¹ãƒˆ
        """
        if len(store['docs']) == 0:
            self.logger.warning("âš ï¸  Retrieval store is empty")
            return []
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        qemb = np.array(self.embed_model.get_text_embedding(query))
        qnorm = np.linalg.norm(qemb)
        if qnorm > 1e-9:
            qemb = qemb / qnorm
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        sims = store['embeddings'] @ qemb
        top_indices = np.argsort(-sims)[:top_k]
        
        results = []
        for i in top_indices:
            if i >= len(store['docs']):
                continue
            
            doc = store['docs'][i]
            score = float(sims[i])
            
            # Graph chunk IDsã‚’å–å¾—
            graph_chunk_ids = doc.metadata.get('graph_chunk_ids', [])
            
            # ã¾ãŸã¯ã€chunk_mappingã‹ã‚‰é€†å¼•ã
            if not graph_chunk_ids and chunk_mapping:
                chunk_id = doc.metadata.get('chunk_id')
                graph_chunk_ids = chunk_mapping.get('retrieval_to_graph', {}).get(chunk_id, [])
            
            results.append((score, doc, graph_chunk_ids))
        
        return results
    
    # ============================================================
    # çµ±åˆãƒ“ãƒ«ãƒ‰é–¢æ•°ã‚’ä¿®æ­£
    # ============================================================
    
    def commit_to_graph_with_retrieval(
        self,
        documents: List[Document],
        graph_store: Neo4jGraphStore
    ) -> Dict[str, Any]:
        """
        Graph index ã¨ Retrieval store ã‚’åŒæ™‚ã«æ§‹ç¯‰ï¼ˆåŒæœŸç‰ˆï¼‰
        """
        with self.hlogger.section("Dual-Index Building (Synced)"):
            # 1. Dual-documentsç”Ÿæˆï¼ˆåŒæœŸãƒãƒƒãƒ”ãƒ³ã‚°ä»˜ãï¼‰
            graph_docs, retrieval_docs, chunk_mapping = self.create_dual_documents(documents)
            
            self.logger.info(
                f"ğŸ”— Chunk mapping: "
                f"{len(chunk_mapping['graph_to_retrieval'])} graph -> retrieval links"
            )
            
            # 2. Graphæ§‹ç¯‰
            self.logger.info("ğŸ“Š Building knowledge graph...")
            self.commit_to_graph(graph_docs, graph_store)
            
            # 3. Retrieval storeæ§‹ç¯‰
            self.logger.info("ğŸ” Building retrieval store...")
            retrieval_store = self.build_retrieval_store(retrieval_docs)
            
            # chunk_mappingã‚’storeã«è¿½åŠ 
            retrieval_store['chunk_mapping'] = chunk_mapping
        
        return {
            'retrieval_store': retrieval_store,
            'chunk_mapping': chunk_mapping,
            'stats': {
                'graph_docs': len(graph_docs),
                'retrieval_docs': len(retrieval_docs),
                'sync_links': len(chunk_mapping['retrieval_to_graph'])
            }
        }

  
    def link_entities(
        self,
        kg: nx.Graph,
        similarity_threshold: float = 0.88,
        use_embedding: bool = True
    ) -> Tuple[nx.Graph, Dict[str, str]]:
        """
        åŒä¸€å®Ÿä½“ã‚’çµ±åˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            similarity_threshold: çµ±åˆã™ã‚‹é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.95æ¨å¥¨ï¼‰
            use_embedding: True=åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã€False=æ–‡å­—åˆ—é¡ä¼¼åº¦
        
        Returns:
            (çµ±åˆå¾Œã®ã‚°ãƒ©ãƒ•, ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°)
            
        ä¾‹:
            mapping = {
                'Self-Attention': 'self_attention',
                'the attention mechanism': 'self_attention',
                'it': 'self_attention'  # corefè§£æ±ºãŒå¿…è¦
            }
        """
        self.logger.info(f"ğŸ”— Starting entity linking (threshold={similarity_threshold})")
        
        nodes = list(kg.nodes())
        entity_mapping = {}  # old_name -> canonical_name
        clusters = []  # [[é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒªã‚¹ãƒˆ], ...]
        
        # ============================================================
        # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        # ============================================================
        if use_embedding:
            clusters = self._cluster_entities_by_embedding(
                nodes, similarity_threshold
            )
        else:
            clusters = self._cluster_entities_by_string(nodes)
        
        # ============================================================
        # 2. å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨åã‚’æ±ºå®š
        # ============================================================
        for cluster in clusters:
            if len(cluster) <= 1:
                continue
            
            # ä»£è¡¨åã®é¸æŠæˆ¦ç•¥
            canonical = self._select_canonical_name(cluster, kg)
            
            for entity in cluster:
                if entity != canonical:
                    entity_mapping[entity] = canonical
        
        self.logger.info(f"  â†’ {len(entity_mapping)} entities will be merged")
        
        # ============================================================
        # 3. ã‚°ãƒ©ãƒ•ã®çµ±åˆ
        # ============================================================
        merged_kg = self._merge_graph_entities(kg, entity_mapping)
        
        self.logger.info(
            f"âœ… Entity linking complete: "
            f"{len(kg.nodes)} â†’ {len(merged_kg.nodes)} nodes"
        )
        
        return merged_kg, entity_mapping
    
    def _cluster_entities_by_embedding(
        self,
        entities: List[str],
        threshold: float
    ) -> List[List[str]]:
        """
        åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        Returns:
            [[é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£], [é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£], ...]
        """
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—
        embeddings = []
        valid_entities = []
        
        for entity in entities:
            try:
                emb = self.embed_model.get_text_embedding(entity)
                emb = np.array(emb, dtype=np.float32)
                norm = np.linalg.norm(emb)
                
                if norm > 1e-9:
                    emb = emb / norm
                    embeddings.append(emb)
                    valid_entities.append(entity)
            except Exception as e:
                self.logger.debug(f"Embedding failed for '{entity}': {e}")
        
        if len(embeddings) == 0:
            return []
        
        embeddings = np.vstack(embeddings)
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        sim_matrix = embeddings @ embeddings.T
        
        # Union-Find ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        parent = {i: i for i in range(len(valid_entities))}
        
        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]
        
        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py
        
        # é¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸Šã®ãƒšã‚¢ã‚’çµ±åˆ
        for i in range(len(valid_entities)):
            for j in range(i + 1, len(valid_entities)):
                if sim_matrix[i, j] >= threshold:
                    union(i, j)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ§‹ç¯‰
        clusters_dict = defaultdict(list)
        for i, entity in enumerate(valid_entities):
            root = find(i)
            clusters_dict[root].append(entity)
        
        clusters = list(clusters_dict.values())
        
        self.logger.info(
            f"  â†’ Found {len(clusters)} clusters from {len(valid_entities)} entities"
        )
        
        return clusters
    
    def _cluster_entities_by_string(
        self,
        entities: List[str]
    ) -> List[List[str]]:
        """
        æ–‡å­—åˆ—é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆé«˜é€Ÿã ãŒç²¾åº¦ä½ã„ï¼‰
        
        ä½¿ç”¨ã‚±ãƒ¼ã‚¹ï¼š
        - "Self-Attention" ã¨ "self-attention" ã‚’çµ±åˆ
        - "GPT-3" ã¨ "GPT3" ã‚’çµ±åˆ
        """
        from difflib import SequenceMatcher
        
        clusters_dict = defaultdict(list)
        normalized = {}
        
        for entity in entities:
            # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ï¼‰
            norm = entity.lower().replace('-', '').replace('_', '').replace(' ', '')
            normalized[entity] = norm
            clusters_dict[norm].append(entity)
        
        # 2ã¤ä»¥ä¸Šã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹æ­£è¦åŒ–å½¢ã®ã¿è¿”ã™
        clusters = [v for v in clusters_dict.values() if len(v) > 1]
        
        return clusters
    
    def _select_canonical_name(
        self,
        cluster: List[str],
        kg: nx.Graph
    ) -> str:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨åã‚’é¸æŠ
        
        æˆ¦ç•¥ï¼š
        1. æœ€ã‚‚æ¬¡æ•°ãŒé«˜ã„ï¼ˆå¤šãã®é–¢ä¿‚ã‚’æŒã¤ï¼‰
        2. æœ€ã‚‚é•·ã„åå‰ï¼ˆæƒ…å ±é‡ãŒå¤šã„ï¼‰
        3. ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †
        """
        # æ¬¡æ•°ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scores = {}
        for entity in cluster:
            degree = kg.degree(entity) if kg.has_node(entity) else 0
            length = len(entity)
            
            # ã‚¹ã‚³ã‚¢ = æ¬¡æ•° * 10 + é•·ã•
            scores[entity] = degree * 10 + length
        
        # ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®ã‚‚ã®ã‚’é¸æŠ
        canonical = max(cluster, key=lambda e: scores[e])
        
        self.logger.debug(
            f"  Cluster: {cluster} â†’ Canonical: '{canonical}'"
        )
        
        return canonical
    
    def _merge_graph_entities(
        self,
        kg: nx.Graph,
        entity_mapping: Dict[str, str]
    ) -> nx.Graph:
        """
        ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã£ã¦ã‚°ãƒ©ãƒ•ã‚’çµ±åˆ
        
        Args:
            kg: å…ƒã®ã‚°ãƒ©ãƒ•
            entity_mapping: {old_name: canonical_name}
        
        Returns:
            çµ±åˆå¾Œã®ã‚°ãƒ©ãƒ•
        """
        merged_kg = nx.Graph()
        
        # ãƒãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨ï¼‰
        for node, data in kg.nodes(data=True):
            canonical = entity_mapping.get(node, node)
            
            if merged_kg.has_node(canonical):
                # æ—¢å­˜ãƒãƒ¼ãƒ‰ã®å±æ€§ã‚’ãƒãƒ¼ã‚¸
                for key, value in data.items():
                    if key not in merged_kg.nodes[canonical]:
                        merged_kg.nodes[canonical][key] = value
            else:
                merged_kg.add_node(canonical, **data)
        
        # ã‚¨ãƒƒã‚¸ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨ + é‡ã¿çµ±åˆï¼‰
        edge_weights = defaultdict(lambda: {
            'weight': 0.0,
            'intra_raw': 0.0,
            'inter_raw': 0.0,
            'relations': []
        })
        
        for u, v, data in kg.edges(data=True):
            u_canonical = entity_mapping.get(u, u)
            v_canonical = entity_mapping.get(v, v)
            
            # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¯é™¤å¤–
            if u_canonical == v_canonical:
                continue
            
            # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚­ãƒ¼ï¼ˆæ–¹å‘ãªã—ï¼‰
            edge_key = tuple(sorted([u_canonical, v_canonical]))
            
            # é‡ã¿ã‚’ç´¯ç©
            edge_weights[edge_key]['weight'] += data.get('weight', 0.0)
            edge_weights[edge_key]['intra_raw'] += data.get('intra_raw', 0.0)
            edge_weights[edge_key]['inter_raw'] += data.get('inter_raw', 0.0)
            
            # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã‚’è¨˜éŒ²
            rel = data.get('relation', 'RELATED')
            if rel not in edge_weights[edge_key]['relations']:
                edge_weights[edge_key]['relations'].append(rel)
        
        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for (u, v), weights in edge_weights.items():
            merged_kg.add_edge(
                u, v,
                weight=weights['weight'],
                intra_raw=weights['intra_raw'],
                inter_raw=weights['inter_raw'],
                relation=weights['relations'][0] if weights['relations'] else 'RELATED',
                relation_types=weights['relations']
            )
        
        return merged_kg
    


    # ============================================================
    # Retrieval Store æ§‹ç¯‰
    # ============================================================
    def build_retrieval_store(
        self,
        retrieval_docs: List[Document]
    ) -> Dict[str, Any]:
        """
        Retrievalç”¨ã®åŸ‹ã‚è¾¼ã¿ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰
        
        Returns:
            {
                'docs': [Document, ...],
                'embeddings': np.array,
                'metadata': {...}
            }
        """
        self.logger.info("ğŸ” Building retrieval embeddings...")
        
        docs = []
        embeddings = []
        
        from shared.error_handler import ErrorCollector
        collector = ErrorCollector(self.logger)
        
        for doc in retrieval_docs:
            try:
                emb = self.embed_model.get_text_embedding(doc.text)
                emb = np.array(emb, dtype=np.float32)
                
                # æ­£è¦åŒ–
                norm = np.linalg.norm(emb)
                if norm > 1e-9:
                    emb = emb / norm
                else:
                    self.logger.debug("Zero-norm embedding, skipping")
                    continue
                
                docs.append(doc)
                embeddings.append(emb)
                collector.add_success()
            
            except Exception as e:
                collector.add_error(
                    context=f"doc_{doc.metadata.get('source_id', 'unknown')}",
                    error=e
                )
        
        collector.report("Retrieval embedding generation", threshold=0.3)
        
        embeddings = np.vstack(embeddings) if embeddings else np.zeros((0, 1024))
        
        self.logger.info(f"âœ… Built retrieval store: {len(docs)} docs")
        
        return {
            'docs': docs,
            'embeddings': embeddings,
            'metadata': {
                'total_docs': len(docs),
                'embedding_dim': embeddings.shape[1] if len(embeddings) > 0 else 0
            }
        }
    
    # ============================================================
    # æ¤œç´¢é–¢æ•°
    # ============================================================
    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5
    ) -> List[Tuple[float, Document]]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¤œç´¢
        
        Returns:
            [(score, Document), ...] ã®ãƒªã‚¹ãƒˆï¼ˆé™é †ï¼‰
        """
        if len(store['docs']) == 0:
            self.logger.warning("âš ï¸  Retrieval store is empty")
            return []
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        qemb = np.array(self.embed_model.get_text_embedding(query))
        qnorm = np.linalg.norm(qemb)
        if qnorm > 1e-9:
            qemb = qemb / qnorm
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        sims = store['embeddings'] @ qemb
        
        # Top-kå–å¾—
        top_indices = np.argsort(-sims)[:top_k]
        
        results = [
            (float(sims[i]), store['docs'][i])
            for i in top_indices
            if i < len(store['docs'])
        ]
        
        return results


    # ============================================================
    # çµ±åˆãƒ“ãƒ«ãƒ‰é–¢æ•°ï¼ˆæ—¢å­˜ã®commit_to_graphã‚’æ‹¡å¼µï¼‰
    # ============================================================
    def commit_to_graph_with_retrieval(
        self,
        documents: List[Document],
        graph_store: Neo4jGraphStore
    ) -> Dict[str, Any]:
        """
        Graph index ã¨ Retrieval store ã‚’åŒæ™‚ã«æ§‹ç¯‰
        
        Returns:
            {
                'kg_networkx': networkx.Graph,
                'retrieval_store': dict,
                'stats': {...}
            }
        """
        with self.hlogger.section("Dual-Index Building"):
            # 1. Dual-documentsç”Ÿæˆ
            graph_docs, retrieval_docs = self.create_dual_documents(documents)
            
            # 2. Graphæ§‹ç¯‰ï¼ˆæ—¢å­˜ã®commit_to_graphãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ã†ï¼‰
            self.logger.info("ğŸ“Š Building knowledge graph...")
            self.commit_to_graph(graph_docs, graph_store)
            
            # ã‚°ãƒ©ãƒ•ã‚’å–å¾—ï¼ˆæ—¢ã«æ§‹ç¯‰æ¸ˆã¿ï¼‰
            # â€» commit_to_graphã®ä¸­ã§kgã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£ãŒå¿…è¦
            # ä»Šã¯æš«å®šçš„ã«Noneã‚’è¿”ã™
            kg = None
            
            # 3. Retrieval storeæ§‹ç¯‰
            self.logger.info("ğŸ” Building retrieval store...")
            retrieval_store = self.build_retrieval_store(retrieval_docs)
        
        return {
            'kg_networkx': kg,
            'retrieval_store': retrieval_store,
            'stats': {
                'graph_docs': len(graph_docs),
                'retrieval_docs': len(retrieval_docs),
                'retrieval_embeddings': len(retrieval_store['docs'])
            }
        }

    def commit_to_graph(self, documents: List[Document], graph_store: Neo4jGraphStore):
        """Neo4jã«ã‚°ãƒ©ãƒ•ã‚’æŠ•å…¥"""
        #ã€€æ¥ç¶šç¢ºèªã€€===========================================
        try:
            graph_store.query("RETURN 1")
            self.logger.info("âœ… Neo4j connection verified")
        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j connection failed: {type(e).__name__}")
            raise  # æ¥ç¶šã§ããªã„ãªã‚‰å‡¦ç†ã‚’ä¸­æ–­
        # 2. ã‚°ãƒ©ãƒ•ç”Ÿæˆ ===========================================
        try:
            with self.hlogger.section("Graph Generation"):
                llm = OpenAI(
                    model=self.config['llm_model'],
                    timeout=self.config['llm_timeout']
                )
            #    storage_context = StorageContext.from_defaults(graph_store=graph_store)
            
                local_graph_store = SimpleGraphStore()
                local_storage = StorageContext.from_defaults(graph_store=local_graph_store)

                self.logger.info("Building local knowledge graph...")
                index = KnowledgeGraphIndex.from_documents(
                    documents,
                    storage_context=local_storage, 
                    llm=llm,
                    transformations=[SimpleNodeParser.from_defaults(chunk_size=512)],
                    embed_model=self.embed_model,
                    show_progress=True,
                    max_triplets_per_chunk=self.config['max_triplets_per_chunk']    # 15
                )
            
                kg = index.get_networkx_graph()
                self.logger.info(f"âœ… Initial graph: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
                all_triples = []

                for subj, obj, data in kg.edges(data=True):
                    rel = data.get('relation', 'RELATED')
                    all_triples.append((subj, rel, obj))
                # rel_mapå‡¦ç†
                if hasattr(local_graph_store, 'get_rel_map'):
                    try:
                        rel_map = local_graph_store.get_rel_map()
                        self.logger.debug(f"rel_map structure: {type(rel_map)}")
        
                        for subj, relations in rel_map.items():
                        # relations ãŒè¾æ›¸ã‹ã€ãƒªã‚¹ãƒˆã‹ç¢ºèª
                            if isinstance(relations, dict):
                                # è¾æ›¸ã®å ´åˆ
                                for rel, objs in relations.items():
                                    if isinstance(objs, list):
                                        for obj in objs:
                                            if (subj, rel, obj) not in all_triples:
                                                all_triples.append((subj, rel, obj))
                                    else:
                                        if (subj, rel, objs) not in all_triples:
                                            all_triples.append((subj, rel, objs))
            
                            elif isinstance(relations, list):
                                # ãƒªã‚¹ãƒˆã®å ´åˆ
                                for item in relations:
                                    if isinstance(item, tuple) and len(item) == 2:
                                        rel, obj = item
                                        if (subj, rel, obj) not in all_triples:
                                            all_triples.append((subj, rel, obj))
    
                    except Exception as e:
                        self.logger.warning(f"Could not parse rel_map: {e}")

                self.logger.info(f"Extracted {len(all_triples)} triples total")
                # ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å…¨ãƒˆãƒªãƒ—ãƒ«ã‚’å…±æœ‰
                # ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ¥ã«åˆ†ã‘ã‚‹ã®ã¯é›£ã—ã„ã®ã§ã€å…¨ä½“ã¨ã—ã¦æ‰±ã†ï¼‰
                for doc in documents:
                    doc.metadata['triples'] = all_triples

        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Graph generation failed: {type(e).__name__}"
            )
            raise

        # Entity Linking
        try:
            with self.hlogger.section("Entity Linking"):
                kg, entity_mapping = self.link_entities(
                    kg,
                    similarity_threshold=self.config['entity_linking_threshold'],
                    use_embedding=True
                )
                
                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæ›´æ–°
                updated_triples = []
                for s, r, o in all_triples:
                    s_new = entity_mapping.get(s, s)
                    o_new = entity_mapping.get(o, o)
                    if s_new != o_new:  # è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å¤–
                        updated_triples.append((s_new, r, o_new))
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                for doc in documents:
                    doc.metadata['triples'] = updated_triples
                
                self.logger.info(f"Updated triples: {len(all_triples)} â†’ {len(updated_triples)}")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸  Entity linking failed: {e}")
            # Entity Linkingå¤±æ•—ã§ã‚‚å‡¦ç†ã¯ç¶™ç¶š        

        # ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã«çµ±åˆã€€================================
        try:
            with self.hlogger.section("Merging Path Information"):
                self.merge_paths_into_kg(kg, documents)
                self.logger.info(f"âœ… Path info merged: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

        except Exception as e:
            self.logger.warning(f"âš ï¸  Path merging failed: {e}")

        # RAPLæœ€é©åŒ–
        try:
            with self.hlogger.section("Graph Optimization (RAPL)"):
                optimized_kg = self._optimize_graph_rapl(kg, documents)
                self.logger.info(
                    f"âœ… Optimized graph: {len(optimized_kg.nodes)} nodes, "
                    f"{len(optimized_kg.edges)} edges"
                )
        except Exception as e:
            self.logger.error(  
                f"ğŸš¨ Graph optimization failed: {e}")
            optimized_kg = kg 
            # æœ€é©åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’Neo4jã«åæ˜ 
        try:
            with self.hlogger.section("Updating Neo4j"):
                result = self._update_neo4j_structure(optimized_kg, graph_store)
            
            # result ãŒ None ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if result is None:
                    result = {'updated': 0, 'skipped': 0, 'failed': 0, 'error_details': []}
                    
                    self.logger.warning("âš ï¸  _update_neo4j_structure returned None")

            # çµæœã‚µãƒãƒªãƒ¼
                self.logger.info(
                    f"âœ… Neo4j update complete:\n"
                    f"   - Updated: {result.get('updated', 0)} edges\n"
                    f"   - Skipped: {result.get('skipped', 0)} edges\n"
                    f"   - Failed: {result.get('failed', 0)} edges"
                )
            
            # å¤±æ•—ç‡ãŒé«˜ã„å ´åˆã¯è­¦å‘Š
                total = result.get('updated', 0) + result.get('failed', 0)
                if total > 0 and result.get('failed', 0) / total > 0.3:
                    self.logger.warning(
                        f"âš ï¸  High failure rate ({result.get('failed', 0)/total:.1%}). "
                        f"Check Neo4j constraints and data format."
                    )
    
        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j update failed: {e}")
            raise

    def merge_paths_into_kg(self, kg, documents: List[Document]):
        """
        kg: networkx.Graph (triples turned into nodes/edges)
        documents: the same documents that have metadata['paths'] etc.
        This will:
          - count how many times each entity appears in top-k paths
          - add edge/node attributes: top_path_count, avg_path_length
        """
        from collections import Counter, defaultdict
        path_entity_counts = Counter()
        entity_path_lengths = defaultdict(list)

        for doc in documents:
            paths = doc.metadata.get('paths', [])  # each path is a str like "A -> B -> C" OR list; adapt if needed
            distances = doc.metadata.get('path_distances', [])
            for i, p in enumerate(paths):
                # normalize path representation
                if isinstance(p, str):
                    nodes = [n.strip() for n in p.split('->') if n.strip()]
                elif isinstance(p, (list, tuple)):
                   nodes = list(p)
                else:
                    continue

                dist = distances[i] if i < len(distances) else len(nodes)-1
                for n in nodes:
                    path_entity_counts[n] += 1
                    entity_path_lengths[n].append(dist)

                # if the path describes relations, you could also add edges for consecutive nodes
                for a, b in zip(nodes, nodes[1:]):
                    if kg.has_edge(a, b):
                        # add a path_support counter on existing edge
                        kg[a][b].setdefault('path_support', 0)
                        kg[a][b]['path_support'] += 1
                    else:
                        kg.add_edge(a, b, relation='path_inferred', path_support=1)

        # inject aggregated attrs to nodes
        for n in kg.nodes():
            cnt = path_entity_counts.get(n, 0)
            lens = entity_path_lengths.get(n, [])
            avg_len = sum(lens)/len(lens) if lens else None
            kg.nodes[n]['path_top_count'] = cnt
            if avg_len is not None:
                kg.nodes[n]['path_avg_length'] = avg_len

    
    def _optimize_graph_rapl(self, kg, documents):
        """
        RAPL æœ€é©åŒ–
        """
        from collections import defaultdict
    
    # 1. Triples æŠ½å‡º
        doc_triples = {}
        for idx, doc in enumerate(documents):
            triples = doc.metadata.get("triples", [])
            if triples:  # ç©ºãƒªã‚¹ãƒˆã¯é™¤å¤–
                doc_triples[idx] = triples
        
        all_triples = [t for lst in doc_triples.values() for t in lst]
    
        self.logger.info(f"Total triples: {len(all_triples)}")
    
    # Weight æ ¼ç´é ˜åŸŸã®åˆæœŸåŒ–
        for u, v in kg.edges():
            kg[u][v]["intra_raw"] = 0.0
            kg[u][v]["inter_raw"] = 0.0
    
    # 2. Intra: æ–‡æ›¸å†… triple é–“ç›¸äº’ä½œç”¨
        self.logger.info("Computing intra-interactions...")
        intra_collector = ErrorCollector(self.logger)
        intra_edges = 0
    
        for doc_id, triples in doc_triples.items():
            try:
                entities = set()
                for s, _, o in triples:
                    entities.add(s)
                    entities.add(o)
        
        # Triple é–“ã®ç›¸äº’ä½œç”¨ï¼ˆé–¢ä¿‚ã®ç›¸æ€§ã‚’è€ƒæ…®ï¼‰
                for i in range(len(triples)):
                    s1, r1, o1 = triples[i]
                    for j in range(i + 1, len(triples)):
                        s2, r2, o2 = triples[j]
                
                # é–¢ä¿‚ã®ç›¸æ€§
                        try:
                            rel_compat = self._compute_relation_compatibility(r1, r2)
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…±æœ‰åº¦
                            shared = len({s1, o1} & {s2, o2})
                            shared_score = shared * 0.5
                
                # çµ±åˆé‡ã¿
                            w = rel_compat * 0.6 + shared_score * 0.4
                
                            if w > 0.3:
                                if kg.has_edge(s1, o1):
                                    kg[s1][o1]["intra_raw"] += w
                                if kg.has_edge(s2, o2):
                                    kg[s2][o2]["intra_raw"] += w
                            intra_collector.add_success()
                        except Exception as e:
                            intra_collector.add_error(
                                context=f"doc_{doc_id}_triple_{i}_{j}",
                                error=e,
                                triple1=(s1, r1, o1),
                                triple2=(s2, r2, o2)
                            )
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢é–“ã®ã‚¨ãƒƒã‚¸è¿½åŠ 
                for e1 in entities:
                    for e2 in entities:
                        if e1 != e2:
                            try:
                                w = self._compute_intra_weight(e1, e2, triples, kg)
                                if w > 0.5:
                                    if kg.has_edge(e1, e2):
                                        kg[e1][e2]["weight"] = kg[e1][e2].get("weight", 0) + w
                                    else:
                                        kg.add_edge(e1, e2, relation="intra_doc", weight=w)
                                        intra_edges += 1
                            except Exception as e:
                                intra_collector.add_error(
                                    context=f"entity_pair_{e1}_{e2}",
                                    error=e
                                )
            except Exception as e:
                self.logger.error(f"Failed to process document {doc_id}: {type(e).__name__}")
                continue

        intra_collector.report("Intra-document processing", threshold=0.3)
        self.logger.info(f"Added {intra_edges} intra-document edges")
    
    # 3. Inter: å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é«˜é€ŸåŒ–
        self.logger.info("Computing inter-interactions (optimized)...")
        inter_collector = ErrorCollector(self.logger)
    
    # 3-1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£â†’Triple ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        entity_to_triples = defaultdict(set)
        for idx, (s, r, o) in enumerate(all_triples):
            entity_to_triples[s].add(idx)
            entity_to_triples[o].add(idx)
    
        self.logger.info(f"Built entity index: {len(entity_to_triples)} unique entities")
    
    # 3-2. å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹ Triple ãƒšã‚¢ã®ã¿è¨ˆç®—
        seen_pairs = set()
        inter_count = 0
    
        for _entity, triple_indices in entity_to_triples.items():
            if len(triple_indices) < 2:
                continue  # 1ã¤ã® Triple ã«ã—ã‹å‡ºç¾ã—ãªã„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ã‚¹ã‚­ãƒƒãƒ—
        
            indices = list(triple_indices)
            for i in range(len(indices)):
                for j in range(i + 1, len(indices)):
                    idx1, idx2 = indices[i], indices[j]
                    pair = (min(idx1, idx2), max(idx1, idx2))
                    if pair in seen_pairs:
                        continue
                    seen_pairs.add(pair)
                
                # é‡ã¿è¨ˆç®—
                    try:
                        t1 = all_triples[idx1]
                        t2 = all_triples[idx2]
                        w = self._compute_inter_weight(t1, t2, kg=kg)
                
                        if w > self.config['relation_compat_threshold']: 
                            s1, _, o1 = t1
                            s2, _, o2 = t2
                    
                    # åŒæ–¹å‘ã«é‡ã¿ã‚’åŠ ç®—
                            if kg.has_edge(s1, o1):
                                kg[s1][o1]["inter_raw"] = kg[s1][o1].get("inter_raw", 0.0) + w
                            if kg.has_edge(s2, o2):
                                kg[s2][o2]["inter_raw"] = kg[s2][o2].get("inter_raw", 0.0) + w
                    
                            inter_count += 1
                        inter_collector.add_success()

                    except Exception as e:
                        inter_collector.add_error(
                            context=f"triple_pair_{idx1}_{idx2}",
                            error=e
                        )

        inter_collector.report("Inter-document processing", threshold=0.3)
        self.logger.info(f"Added {inter_count} meaningful inter-interactions")
    
    # 4. Document-level linking
        self.logger.info("Computing document-level connections...")
    
        try:
            entity_docs = {}
            for doc_id, triples in doc_triples.items():
                for s, _, o in triples:
                    entity_docs.setdefault(s, set()).add(doc_id)
                    entity_docs.setdefault(o, set()).add(doc_id)
    
            doc_pairs = {}
            bridge_entities = []
    
            for entity_name, doc_set in entity_docs.items():
                if len(doc_set) > 1:
                    docs = list(doc_set)
                    for i, d1 in enumerate(docs):
                        for d2 in docs[i+1:]:
                            pair = (d1, d2)
                            doc_pairs[pair] = doc_pairs.get(pair, 0) + 1
            
                    if len(doc_set) > 2:
                        bridge_entities.append((entity_name, len(doc_set)))

                # ãƒ–ãƒªãƒƒã‚¸ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ­ã‚°
            if bridge_entities:
                bridge_entities.sort(key=lambda x: x[1], reverse=True)
                self.logger.info("Top bridge entities:")
                for entity_name, count in bridge_entities[:5]:
                    self.logger.info(f"  '{entity_name}': {count} documents")

            inter_doc_count = 0
            for (d1, d2), ct in doc_pairs.items():
                if ct > 2:
                    n1 = f"doc_{d1}"
                    n2 = f"doc_{d2}"
                
                    if not kg.has_node(n1):
                        kg.add_node(n1, type="document")
                    if not kg.has_node(n2):
                        kg.add_node(n2, type="document")
                
                    kg.add_edge(n1, n2, relation="inter_doc", weight=ct)
                    inter_doc_count += 1
            self.logger.info(f"Added {inter_doc_count} inter-document links")

        except Exception as e:
            self.logger.error(f"Document linking failed: {type(e).__name__} - {str(e)[:100]}")
 
    # 5. çµ±åˆé‡ã¿
        self.logger.info("Finalizing edge weights...")
    
        for u, v, d in kg.edges(data=True):
            intra = d.get("intra_raw", 0.0)
            inter = d.get("inter_raw", 0.0)
        
        # RAPLè«–æ–‡: intraé‡è¦– + interè£œå®Œ
            d["weight"] = min(0.7 * intra + 0.3 * inter, 1.0)    
        return kg
    
    def _group_triples_by_document(self, kg, documents):
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’Documentåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        # ç°¡æ˜“å®Ÿè£…: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®š
        doc_triples = {}
        
        for i, doc in enumerate(documents):
            doc_id = f"doc_{i}"
            doc_triples[doc_id] = []
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒDocumentå†…ã«å‡ºç¾ã™ã‚‹ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º
            for s, o, data in kg.edges(data=True):
                if s in doc.text or o in doc.text:
                    doc_triples[doc_id].append((s, data.get('relation', ''), o))
        return doc_triples
    
    def _compute_intra_weight(self, e1: str, e2: str, triples: List, kg=None) -> float:
        """
        åŒä¸€Documentå†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“é‡ã¿è¨ˆç®—
        
        Args:
            e1, e2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å
            triples: (s, r, o) ã®ãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
    # ------------------------------------------------------------
    # 1) å…±èµ·é »åº¦ï¼ˆåŸºæœ¬ï¼‰
    # ------------------------------------------------------------
        cooccur = sum(
            1 for s, _, o in triples
            if (s == e1 and o == e2) or (s == e2 and o == e1)
        )
        co_norm = min(cooccur / 5.0, 1.0)   # æ­£è¦åŒ–

    # ------------------------------------------------------------
    # 2) é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§
    # ------------------------------------------------------------
        rel_pairs = [
            (r, True) for s, r, o in triples
            if (s == e1 and o == e2)
        ] + [
            (r, False) for s, r, o in triples
            if (s == e2 and o == e1)  # é€†å‘ã
        ]
    
        if not rel_pairs:
            rel_bonus = 0.0
        else:
            # é–¢ä¿‚ã®å¤šæ§˜æ€§
            unique_rels = set(r for r, _ in rel_pairs)
            diversity_bonus = min(len(unique_rels) * 0.2, 0.6)

        # æ–¹å‘ã®ä¸€è²«æ€§ï¼ˆåŒã˜å‘ããŒå¤šã„ã»ã©å¼·ã„é–¢ä¿‚ï¼‰
            same_direction_count = sum(1 for _, is_forward in rel_pairs if is_forward)
            opposite_direction_count = len(rel_pairs) - same_direction_count

        # é–¢ä¿‚ã®è³ªï¼ˆåŒã˜å‘ãã‹é€†å‘ãã‹ã§è©•ä¾¡ï¼‰
            if same_direction_count > opposite_direction_count:
                direction_score = same_direction_count / len(rel_pairs)
            else:
            # é€†æ–¹å‘ãŒå¤šã„ = åŒæ–¹å‘ã®é–¢ä¿‚ï¼ˆã“ã‚Œã‚‚æœ‰ç”¨ï¼‰
                direction_score = 0.7  # ã‚„ã‚„é«˜ã‚ã«è©•ä¾¡

            rel_bonus = diversity_bonus * 0.5 + direction_score * 0.5

    # ------------------------------------------------------------
    # 3) ãƒ‘ã‚¹ã‚µãƒãƒ¼ãƒˆï¼ˆkgã« path_support ãŒã‚ã‚Œã°ï¼‰
    # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None and kg.has_edge(e1, e2):
            path_bonus = min(kg[e1][e2].get("path_support", 0) * 0.1, 0.5)

    # ------------------------------------------------------------
    # 4) åˆæˆ
    # ------------------------------------------------------------
        weight = co_norm * 0.5 + rel_bonus * 0.4 + path_bonus * 0.1
        return min(weight, 1.0)


    def _compute_inter_weight(self, t1: tuple, t2: tuple, kg=None, embed_fn=None) -> float:
        """
        Compute inter-triple interaction weight between two triples.
        t1, t2: (s, r, o)
        kg: optional networkx graph
        embed_fn: optional function to compute embeddings of entity names
        """

        s1, r1, o1 = t1
        s2, r2, o2 = t2

        # ------------------------------------------------------------
        # 1) å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆæœ€é‡è¦ï¼‰
        # ------------------------------------------------------------
        shared = len({s1, o1} & {s2, o2})
        shared_bonus = min(shared * 0.5, 1.0)
        # é–¢ä¿‚ã®ç›¸æ€§è¨ˆç®—
        rel_compatibility = safe_execute(
            self._compute_relation_compatibility,
            args=(r1, r2),
            default=0.3,  # å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            logger=self.logger,
            context=f"relation_compatibility({r1}, {r2})"
        )
        # ------------------------------------------------------------
        # 2) ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦ï¼ˆembeddingã‚’æ¸¡ã•ã‚ŒãŸã‚‰ä½¿ã†ï¼‰
        # ------------------------------------------------------------
        sim_bonus = 0.0
        if embed_fn is not None:
            try:        
                e1 = embed_fn(s1)
                e2 = embed_fn(s2)

                # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ãƒã‚§ãƒƒã‚¯
                norm1 = np.linalg.norm(e1)
                norm2 = np.linalg.norm(e2)
            
                if norm1 > 1e-9 and norm2 > 1e-9:
                    sim = (e1 @ e2) / (norm1 * norm2)
                    sim_bonus = max(sim, 0) * 0.3
                else:
                    self.logger.debug(
                        f"Zero embedding detected: {s1} (norm={norm1:.2e}) "
                        f"or {s2} (norm={norm2:.2e})"
                    )
            except Exception as e:
                self.logger.debug(
                    f"Embedding similarity failed for ({s1}, {s2}): "
                    f"{type(e).__name__}"
                )

        # ------------------------------------------------------------
        # 3) graph path-based supportï¼ˆkgãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
        #    2ã€œ3 hop ä»¥å†…ã«ç¹‹ãŒã‚‹ã‹ã‚’è¦‹ã‚‹
        # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None:
            try:
            # 2-hopä»¥å†…ã§ã¤ãªãŒã£ã¦ãŸã‚‰è©•ä¾¡        
                if kg.has_node(s1) and kg.has_node(s2):
                    length = nx.shortest_path_length(kg, s1, s2)
                    if length <= 2:
                        path_bonus = 0.3 * (1.0 - length / 3.0)  # è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            except nx.NetworkXNoPath:
                # ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0.0ã®ã¾ã¾ï¼ˆã“ã‚Œã¯æ­£å¸¸ï¼‰
                pass
            except nx.NodeNotFound as e:
                self.logger.debug(f"Node not found in graph: {e}")
            except Exception as e:
                self.logger.debug(
                    f"Path calculation failed ({s1}->{s2}): {type(e).__name__}"
                )

        # ------------------------------------------------------------
        # 4) ç·åˆ
        # ------------------------------------------------------------
        w = (
            shared_bonus * 0.4 +       # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            rel_compatibility * 0.3 +   # é–¢ä¿‚ã®ç›¸æ€§ï¼ˆã“ã“ã«çµ±åˆæ¸ˆã¿ï¼‰
            sim_bonus * 0.2 +           # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
            path_bonus * 0.1            # ãƒ‘ã‚¹è·é›¢
        )

        return min(w, 1.0)
    
    def _compute_relation_compatibility(self, r1: str, r2: str) -> float:
        """
        é–¢ä¿‚ã®ç›¸æ€§ã‚¹ã‚³ã‚¢
        """
        # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢çµ±ä¸€ï¼‰
        r1 = r1.lower().replace('-', '_')
        r2 = r2.lower().replace('-', '_')
    # 1. å®Œå…¨ä¸€è‡´
        if r1 == r2:
            return 1.0
    
    # 2. é€†é–¢ä¿‚ã®ãƒšã‚¢ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        inverse_pairs = {
            ("cause_of", "caused_by"),
            ("cause_of", "effect_of"), 
            ("part_of", "has_part"),
            ("component_of", "has_component"),
            ("parent_of", "child_of"),
            ("author_of", "written_by"),
            ("owns", "owned_by"),
            ("manages", "managed_by"),
            ("teaches", "taught_by"),
            ("supervises", "supervised_by"),
        }
    
        if (r1, r2) in inverse_pairs or (r2, r1) in inverse_pairs:
            return 0.9
    
    # 3. é–¢é€£ã™ã‚‹é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        related_groups = [
        # å› æœé–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "cause_of", "caused_by", "leads_to", "results_in", 
                "triggers", "produces", "generates", "effect_of"
            },
        
        # æ§‹æˆè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "part_of", "has_part", "component_of", "has_component",
                "contains", "includes", "consists_of", "comprises"
            },
        
        # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "member_of", "has_member", "belongs_to", "works_at", 
                "employed_by", "affiliated_with"
            },
        
        # æ™‚é–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "before", "after", "during", "precedes", "follows",
                "happens_before", "happens_after"
            },
        
        # ç©ºé–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "located_in", "location_of", "near", "adjacent_to",
                "contains", "inside", "outside"
            },
        
        # å±æ€§ãƒ»æ€§è³ªã‚°ãƒ«ãƒ¼ãƒ—
            {
                "is_a", "type_of", "instance_of", "has_property",
                "characterized_by", "defined_by"
            },
        
        # ç›¸äº’ä½œç”¨ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "interacts_with", "collaborates_with", "competes_with",
                "influences", "affected_by"
            },
        ]
    
        for group in related_groups:
            if r1 in group and r2 in group:
                return 0.7
    
    # 4. åŒã˜ã‚«ãƒ†ã‚´ãƒªï¼ˆå‹•è©ã®æ€§è³ªã§åˆ¤å®šï¼‰
    # ä¾‹: action ç³»ã€state ç³»ãªã©
        action_verbs = {
            "creates", "builds", "develops", "produces", "makes",
            "constructs", "designs", "implements", "generates",
            "enables", "powers", "leverages", "accelerates"
        }
    
        state_verbs = {
            "is", "has", "contains", "includes", "comprises",
            "exists", "represents", "defines"
        }
    
        relation_verbs = {
            "relates_to", "associated_with", "connected_to",
            "linked_to", "corresponds_to"
        }
    
        if (r1 in action_verbs and r2 in action_verbs) or \
           (r1 in state_verbs and r2 in state_verbs) or \
           (r1 in relation_verbs and r2 in relation_verbs):
            return 0.5
    
    # 5. åŸ‹ã‚è¾¼ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
        try:
            emb1 = self.relation_embedder.get_text_embedding(r1)
            emb2 = self.relation_embedder.get_text_embedding(r2)
            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)
            return max(0.3, float(sim))
        except Exception:
            return 0.3


    def _update_neo4j_structure(self, kg, graph_store):
        """
        Neo4jæ›´æ–°ï¼ˆã‚«ãƒƒãƒˆã‚ªãƒ•0.02ç‰ˆï¼‰
        """        
        query_template = """
        MERGE (a:Concept {name: $source})
        MERGE (b:Concept {name: $target})
        MERGE (a)-[r:RELATED]->(b)
        ON CREATE SET r.weight = $weight
        ON MATCH SET r.weight = $weight
        """
        collector = ErrorCollector(self.logger)

        for s, o, data in kg.edges(data=True):
            weight = data.get('weight', 0.0)

            if weight <= self.config['final_weight_cutoff']: 
                collector.add_skip()
                continue 
            
            try:
                graph_store.query(query_template, {
                    'source': s,
                    'target': o,
                    'weight': float(weight)
                })
                collector.add_success()

            except Exception as e:
                collector.add_error(
                    context=f"{s} -> {o}",
                    error=e,
                    weight=weight  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²
                )
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆè‡ªå‹•ã§ãƒ­ã‚°å‡ºåŠ›ï¼‰
        collector.report("Neo4j edge update", threshold=0.3)
    # æˆ»ã‚Šå€¤ã‚‚å–å¾—å¯èƒ½
        return collector.get_summary()        

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Crystal Cluster beta')
    parser.add_argument('json_file', help='Clean documents JSON file')
    parser.add_argument('--neo4j-uri', default='bolt://localhost:7687')
    parser.add_argument('--neo4j-user', default='neo4j')
    parser.add_argument('--neo4j-pass', required=True)
    parser.add_argument('--dual-chunk', action='store_true', help='Enable dual-chunk mode')
    parser.add_argument('--test-query', help='Test retrieval with a query')
    parser.add_argument('--debug', action='store_true')
    
    args = parser.parse_args()
    
    print("ğŸ’¾ Crystal Cluster beta")
    if args.dual_chunk:
        print("ğŸ”€ Dual-chunk mode enabled")    
    print("â”" * 42)
    
    cluster = CrystalCluster(
        log_level=logging.DEBUG if args.debug else logging.INFO,
        use_dual_chunk=args.dual_chunk
    )
    
    documents = cluster.load_documents(args.json_file)
    
    graph_store = Neo4jGraphStore(
        username=args.neo4j_user,
        password=args.neo4j_pass,
        url=args.neo4j_uri
    )

    if args.dual_chunk:
        # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰
        result = cluster.commit_to_graph_with_retrieval(documents, graph_store)
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªãŒã‚ã‚Œã°æ¤œç´¢
        if args.test_query:
            print(f"\nğŸ” Testing retrieval: '{args.test_query}'")
            hits = cluster.retrieve(result['retrieval_store'], args.test_query, top_k=3)
            for i, (score, doc) in enumerate(hits, 1):
                print(f"\n{i}. Score: {score:.3f}")
                print(f"   {doc.text[:150]}...")
    else:

        cluster.commit_to_graph(documents, graph_store)
    
    print("âœ¨ Complete!")