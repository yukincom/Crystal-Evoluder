"""
æ¤œç´¢ã‚¹ãƒˆã‚¢æ§‹ç¯‰ã‚¯ãƒ©ã‚¹
"""
import numpy as np
from typing import List, Dict, Any, Tuple

from llama_index.core import Document

from shared import ErrorCollector


class RetrievalBuilder:
    """æ¤œç´¢ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ã¨æ¤œç´¢ã‚’æ‹…å½“"""

    def __init__(self, embed_model, logger):
        self.embed_model = embed_model
        self.logger = logger

    def build_retrieval_store(
        self,
        retrieval_docs: List[Document]
    ) -> Dict[str, Any]:
        """
        Retrievalç”¨ã®åŸ‹ã‚è¾¼ã¿ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰

        Returns:
            {
                'docs': [Document, ...],
                'embeddings': np.array,
                'metadata': {...}
            }
        """
        self.logger.info("ğŸ” Building retrieval embeddings...")

        docs = []
        embeddings = []

        collector = ErrorCollector(self.logger)

        for doc in retrieval_docs:
            try:
                emb = self.embed_model.get_text_embedding(doc.text)
                emb = np.array(emb, dtype=np.float32)

                # æ­£è¦åŒ–
                norm = np.linalg.norm(emb)
                if norm > 1e-9:
                    emb = emb / norm
                else:
                    self.logger.debug("Zero-norm embedding, skipping")
                    continue

                docs.append(doc)
                embeddings.append(emb)
                collector.add_success()

            except Exception as e:
                collector.add_error(
                    context=f"doc_{doc.metadata.get('source_id', 'unknown')}",
                    error=e
                )

        collector.report("Retrieval embedding generation", threshold=0.3)

        embeddings = np.vstack(embeddings) if embeddings else np.zeros((0, 1024))

        self.logger.info(f"âœ… Built retrieval store: {len(docs)} docs")

        return {
            'docs': docs,
            'embeddings': embeddings,
            'metadata': {
                'total_docs': len(docs),
                'embedding_dim': embeddings.shape[1] if len(embeddings) > 0 else 0
            }
        }

    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5,
        chunk_mapping: Dict = None
    ) -> List[Tuple[float, Document, List[str]]]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¤œç´¢

        Returns:
            [(score, Document, graph_chunk_ids), ...] ã®ãƒªã‚¹ãƒˆ
        """
        if len(store['docs']) == 0:
            self.logger.warning("âš ï¸  Retrieval store is empty")
            return []

        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        qemb = np.array(self.embed_model.get_text_embedding(query))
        qnorm = np.linalg.norm(qemb)
        if qnorm > 1e-9:
            qemb = qemb / qnorm

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        sims = store['embeddings'] @ qemb
        top_indices = np.argsort(-sims)[:top_k]

        results = []
        for i in top_indices:
            if i >= len(store['docs']):
                continue

            doc = store['docs'][i]
            score = float(sims[i])

            # Graph chunk IDsã‚’å–å¾—
            graph_chunk_ids = doc.metadata.get('graph_chunk_ids', [])

            # ã¾ãŸã¯ã€chunk_mappingã‹ã‚‰é€†å¼•ã
            if not graph_chunk_ids and chunk_mapping:
                chunk_id = doc.metadata.get('chunk_id')
                graph_chunk_ids = chunk_mapping.get('retrieval_to_graph', {}).get(chunk_id, [])

            # 3. ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ç„¡åŠ¹æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not graph_chunk_ids:
                # åŒä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’æŒã¤ãƒãƒ£ãƒ³ã‚¯ã‚’æ¨å®š
                source_id = doc.metadata.get('source_id') or doc.metadata.get('json_id') or doc.metadata.get('raw_id')
                if source_id is not None:
                    # ç°¡æ˜“çš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’æŒ‡ã™IDã‚’ç”Ÿæˆ
                    graph_chunk_ids = [f"doc_{source_id}_all"]

                # è­¦å‘Šã‚’1å›ã ã‘å‡ºã™ï¼ˆåˆå›ã®ã¿ï¼‰
                if not hasattr(self, '_warned_no_mapping'):
                    self.logger.warning(
                        "âš ï¸  chunk_mapping not available, using fallback document IDs. "
                        "Enable dual-chunk mode for better precision."
                    )
                    self._warned_no_mapping = True

            results.append((score, doc, graph_chunk_ids))

        return results