# bge-m3ã‚’è‡ªå‹•ã§ãƒ­ãƒ¼ãƒ‰

import subprocess
import importlib
import numpy as np
from pathlib import Path

def ensure_bge_m3():
    """
    bge-m3ã‚’è‡ªå‹•ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆãªã‘ã‚Œã°ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
    
    Returns:
        HuggingFaceEmbedding ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    # sentence-transformers ãƒã‚§ãƒƒã‚¯
    try:
        importlib.import_module("sentence_transformers")
    except ImportError:
        print("ğŸ“¦ Installing sentence-transformers...")
        subprocess.run(
            ["pip", "install", "sentence-transformers"],
            check=True
        )
    
    # llama-index-embeddings-huggingface ãƒã‚§ãƒƒã‚¯
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    except ImportError:
        print("ğŸ“¦ Installing llama-index-embeddings-huggingface...")
        subprocess.run(
            ["pip", "install", "llama-index-embeddings-huggingface"],
            check=True
        )
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    
    model_name = "BAAI/bge-m3"
    
    try:
        embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            device="mps",  # Macç”¨ã€ä»–ã¯cuda/cpu
            embed_batch_size=16
        )
        print(f"âœ… Loaded embedding model: {model_name}")
        return embed_model
    
    except Exception as e:
        print(f"âš ï¸  Failed to load {model_name}: {e}")
        print("ğŸ“¦ Installing torch...")
        subprocess.run(["pip", "install", "torch"], check=True)
        
        embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            device="mps",
            embed_batch_size=16
        )
class EmbeddingCache:
    """åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, embed_model=None, cache_size_limit: int = 10000):
        """
        Args:
            embed_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            cache_size_limit: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºä¸Šé™
        """
        self.embed_model = embed_model or ensure_bge_m3()
        self.cache_size_limit = cache_size_limit
        self.cache = {}

    def get_embedding(self, text: str) -> list:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—

        Args:
            text: åŸ‹ã‚è¾¼ã¿å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        if text in self.cache:
            return self.cache[text]

        # æ–°è¦è¨ˆç®—
        embedding = np.array(self.embed_model.get_text_embedding(text))

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if len(self.cache) >= self.cache_size_limit:
            # LRUçš„ã«å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[text] = embedding
        return embedding

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()

    def get_cache_size(self) -> int:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        return len(self.cache)

    def get_cached_embedding(self, text: str) -> list:
        return self.get_embedding(text)
