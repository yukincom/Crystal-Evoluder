"""
Text Parser
ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ãƒ¼ã‚¹
"""
import os
from pathlib import Path
from typing import List
from llama_index.core import Document

from ..text_utils import clean_text, chunk_by_paragraphs

try:
    from langchain_experimental.text_splitter import SemanticChunker
    from langchain_openai import OpenAIEmbeddings
    HAS_SEMANTIC_CHUNKER = True
except ImportError:
    HAS_SEMANTIC_CHUNKER = False

def parse_txt(txt_path: Path, config=None, logger=None) -> List[Document]:
    """ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹"""
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            content = f.read()

        content = clean_text(content)

        # APIã‚­ãƒ¼ã®å–å¾—ï¼ˆå„ªå…ˆé †ä½ï¼‰
        api_key = (
            (config or {}).get('openai_api_key') or  # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«/å¼•æ•°
            os.environ.get('OPENAI_API_KEY')      # 2. ç’°å¢ƒå¤‰æ•°ï¼ˆ.zshrcï¼‰
        )

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        if HAS_SEMANTIC_CHUNKER and api_key:
            try:
                os.environ['OPENAI_API_KEY'] = api_key

                if logger:
                    logger.info("ğŸ¤– Using SemanticChunker")
                embeddings = OpenAIEmbeddings()
                splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
                chunks = splitter.split_text(content)
                if logger:
                    logger.info(f"âœ… {len(chunks)} semantic chunks created")
            except Exception as e:
                if logger:
                    logger.warning(f"âš ï¸  SemanticChunker failed: {e}")
                chunks = chunk_by_paragraphs(content)
        else:
            if HAS_SEMANTIC_CHUNKER and not api_key:
                if logger:
                    logger.info("â„¹ï¸  OpenAI API key not provided, using basic chunking")
            chunks = chunk_by_paragraphs(content)

        documents = []
        for i, chunk in enumerate(chunks):
            documents.append(Document(
                text=chunk,
                metadata={
                    'title': txt_path.stem,
                    'authors': 'Unknown',
                    'section': f"Chunk {i+1}",
                    'section_index': i,
                    'source_format': 'txt'
                }
            ))

        metadata = {'title': txt_path.stem, 'authors': ['Unknown']}
        if logger:
            logger.info(f"Text parsed: {len(documents)} chunks")
        return documents, metadata

    except Exception as e:
        if logger:
            logger.error(f"Text parse failed: {e}")
        return [], {'title': txt_path.stem, 'authors': ['Unknown']}