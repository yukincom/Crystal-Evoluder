"""
Text Utilities
ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
"""
import re
from typing import List

try:
    import ftfy
    HAS_FTFY = True
except ImportError:
    HAS_FTFY = False

def clean_text(text: str, normalize_whitespace: bool = True) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

    Args:
        text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
        normalize_whitespace: ç©ºç™½ã‚’æ­£è¦åŒ–ã™ã‚‹ã‹

    Returns:
        ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

    Examples:
        >>> clean_text("Hello    World\\n\\n\\nTest")
        'Hello World Test'
    """
    # ftfyãŒã‚ã‚Œã°æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
    if HAS_FTFY:
        text = ftfy.fix_text(text)

    if normalize_whitespace:
        # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
        text = re.sub(r'\s+', ' ', text)

    # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    text = text.strip()

    return text

def detect_text_language(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’æ¤œå‡º
    
    Args:
        text: æ¤œå‡ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        è¨€èªã‚³ãƒ¼ãƒ‰ ('en', 'ja', 'zh', 'ko', 'other')
    """
    if not text:
        return "unknown"
    
    sample = text[:300]
    
    hiragana = sum(1 for c in sample if '\u3040' <= c <= '\u309f')
    katakana = sum(1 for c in sample if '\u30a0' <= c <= '\u30ff')
    kanji = sum(1 for c in sample if '\u4e00' <= c <= '\u9faf')
    ascii_chars = sum(1 for c in sample if ord(c) < 128)
    
    total = max(len(sample), 1)
    
    if (hiragana + katakana) / total > 0.15:
        return "ja"
    
    if kanji / total > 0.3 and (hiragana + katakana) / total < 0.05:
        return "zh"
    
    if ascii_chars / total > 0.7:
        return "en"
    
    return "other"

def split_japanese_sentences(text: str) -> List[str]:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã§åˆ†å‰²
    
    Args:
        text: åˆ†å‰²ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        æ–‡ã®ãƒªã‚¹ãƒˆ
    """
    # æ—¥æœ¬èªã®æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.\n', '!\n', '?\n']
    
    sentences = []
    temp_sentence = ""
    
    for char in text:
        temp_sentence += char
        if any(temp_sentence.endswith(end) for end in sentence_endings):
            sentences.append(temp_sentence.strip())
            temp_sentence = ""
    
    if temp_sentence.strip():
        sentences.append(temp_sentence.strip())
    
    return sentences

def chunk_by_paragraphs(
    content: str,
    chunk_size: int = 2000,
    overlap: int = 200,
    language: str = None  # ğŸ‘ˆ è¨€èªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ 
) -> List[str]:
    """
    æ®µè½ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆè¨€èªå¯¾å¿œç‰ˆï¼‰
    
    Args:
        content: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
        overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        language: è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆNone=è‡ªå‹•æ¤œå‡ºï¼‰
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    # è¨€èªè‡ªå‹•æ¤œå‡º
    if language is None:
        language = detect_text_language(content)
    
    # æ—¥æœ¬èªã®å ´åˆã¯å°‚ç”¨å‡¦ç†
    if language == 'ja':
        return _chunk_japanese_text(content, chunk_size, overlap)
    
    # è‹±èªãªã©ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for para in paragraphs:
        para_length = len(para)
        
        if current_length + para_length > chunk_size and current_chunk:
            chunks.append('\n\n'.join(current_chunk))
            
            if overlap > 0 and current_chunk:
                overlap_text = '\n\n'.join(current_chunk)
                if len(overlap_text) > overlap:
                    overlap_paras = []
                    overlap_len = 0
                    for p in reversed(current_chunk):
                        if overlap_len + len(p) <= overlap:
                            overlap_paras.insert(0, p)
                            overlap_len += len(p)
                        else:
                            break
                    current_chunk = overlap_paras
                    current_length = overlap_len
                else:
                    current_chunk = []
                    current_length = 0
            else:
                current_chunk = []
                current_length = 0
        
        current_chunk.append(para)
        current_length += para_length
    
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks

def _chunk_by_sentences(sentences: List[str], chunk_size: int, overlap: int) -> List[str]:
    """
    æ–‡å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆå†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼‰
    """
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sent in sentences:
        sent_length = len(sent)
        
        if current_length + sent_length > chunk_size and current_chunk:
            chunks.append(''.join(current_chunk))
            
            if overlap > 0:
                overlap_sents = []
                overlap_len = 0
                for s in reversed(current_chunk):
                    if overlap_len + len(s) <= overlap:
                        overlap_sents.insert(0, s)
                        overlap_len += len(s)
                    else:
                        break
                current_chunk = overlap_sents
                current_length = overlap_len
            else:
                current_chunk = []
                current_length = 0
        
        current_chunk.append(sent)
        current_length += sent_length
    
    if current_chunk:
        chunks.append(''.join(current_chunk))
    
    return chunks

def chunk_by_paragraphs(
    content: str,
    chunk_size: int = 2000,
    overlap: int = 200,
    language: str = 'en'
) -> List[str]:
    """
    è¨€èªå¯¾å¿œæ®µè½ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    
    Args:
        content: ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        language: è¨€èªã‚³ãƒ¼ãƒ‰ ('en', 'ja', etc.)
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ    
    """
    if language == 'ja':
        language = detect_text_language(content)
    
    # æ—¥æœ¬èªã®å ´åˆã¯å°‚ç”¨å‡¦ç†
    if language == 'ja':
        return _chunk_japanese_text(content, chunk_size, overlap)
    
    # è‹±èªãªã©ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰
    return _chunk_english_text(content, chunk_size, overlap)
        
def _chunk_japanese_text(content: str, chunk_size: int, overlap: int) -> List[str]:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    
    Args:
        content: ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    # æ®µè½åˆ†å‰²
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    
    if not paragraphs:
        # æ®µè½ãŒãªã„å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
        sentences = split_japanese_sentences(content)
        return _chunk_by_sentences(sentences, chunk_size, overlap)
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for para in paragraphs:
        para_length = len(para)
        
        if current_length + para_length > chunk_size and current_chunk:
            chunks.append('\n\n'.join(current_chunk))
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†
            if overlap > 0:
                overlap_paras = []
                overlap_len = 0
                for p in reversed(current_chunk):
                    if overlap_len + len(p) <= overlap:
                        overlap_paras.insert(0, p)
                        overlap_len += len(p)
                    else:
                        break
                current_chunk = overlap_paras
                current_length = overlap_len
            else:
                current_chunk = []
                current_length = 0
        
        current_chunk.append(para)
        current_length += para_length
    
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks

def _chunk_english_text(content: str, chunk_size: int, overlap: int) -> List[str]:
    """
    è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰
    
    Args:
        content: ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for para in paragraphs:
        para_length = len(para)
        
        if current_length + para_length > chunk_size and current_chunk:
            chunks.append('\n\n'.join(current_chunk))
            
            if overlap > 0 and current_chunk:
                overlap_text = '\n\n'.join(current_chunk)
                if len(overlap_text) > overlap:
                    overlap_paras = []
                    overlap_len = 0
                    for p in reversed(current_chunk):
                        if overlap_len + len(p) <= overlap:
                            overlap_paras.insert(0, p)
                            overlap_len += len(p)
                        else:
                            break
                    current_chunk = overlap_paras
                    current_length = overlap_len
                else:
                    current_chunk = []
                    current_length = 0
            else:
                current_chunk = []
                current_length = 0
        
        current_chunk.append(para)
        current_length += para_length
    
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks
