"""
ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚¯ãƒ©ã‚¹
"""
import numpy as np
import networkx as nx
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict

from llama_index.core import Document, KnowledgeGraphIndex, StorageContext
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.llms.openai import OpenAI

from ..shared import HierarchicalLogger, ErrorCollector, safe_execute
from ..filters.triplet_filter import TripletFilter
from ..linkers.entity_linker import EntityLinker
from ..rag.multi_hop import MultiHopExplorer


class GraphBuilder:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ã‚’æ‹…å½“"""

    def __init__(self, config: dict, embed_model, logger):
        self.config = config
        self.embed_model = embed_model
        self.logger = logger
        self.hlogger = HierarchicalLogger(logger)

    def commit_to_graph(self, documents: List[Document], graph_store):
        """Neo4jã«ã‚°ãƒ©ãƒ•ã‚’æŠ•å…¥"""
        # æ¥ç¶šç¢ºèª
        try:
            graph_store.query("RETURN 1")
            self.logger.info("âœ… Neo4j connection verified")
        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j connection failed: {type(e).__name__}")
            raise  # æ¥ç¶šã§ããªã„ãªã‚‰å‡¦ç†ã‚’ä¸­æ–­

        # 2. ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        try:
            with self.hlogger.section("Graph Generation"):
                llm = OpenAI(
                    model=self.config['llm_model'],
                    timeout=self.config['llm_timeout']
                )

                local_graph_store = SimpleGraphStore()
                local_storage = StorageContext.from_defaults(graph_store=local_graph_store)

                self.logger.info("Building local knowledge graph...")
                index = KnowledgeGraphIndex.from_documents(
                    documents,
                    storage_context=local_storage,
                    llm=llm,
                    transformations=[SimpleNodeParser.from_defaults(chunk_size=512)],
                    embed_model=self.embed_model,
                    show_progress=True,
                    max_triplets_per_chunk=self.config['max_triplets_per_chunk']
                )

                kg = index.get_networkx_graph()
                self.logger.info(f"âœ… Initial graph: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
                all_triples = []

                for subj, obj, data in kg.edges(data=True):
                    rel = data.get('relation', 'RELATED')
                    all_triples.append((subj, rel, obj))

                # rel_mapå‡¦ç†
                if hasattr(local_graph_store, 'get_rel_map'):
                    try:
                        rel_map = local_graph_store.get_rel_map()
                        self.logger.debug(f"rel_map structure: {type(rel_map)}")

                        for subj, relations in rel_map.items():
                            # relations ãŒè¾æ›¸ã‹ã€ãƒªã‚¹ãƒˆã‹ç¢ºèª
                            if isinstance(relations, dict):
                                # è¾æ›¸ã®å ´åˆ
                                for rel, objs in relations.items():
                                    if isinstance(objs, list):
                                        for obj in objs:
                                            if (subj, rel, obj) not in all_triples:
                                                all_triples.append((subj, rel, obj))
                                    else:
                                        if (subj, rel, objs) not in all_triples:
                                            all_triples.append((subj, rel, objs))
                            elif isinstance(relations, list):
                                # ãƒªã‚¹ãƒˆã®å ´åˆ
                                for item in relations:
                                    if isinstance(item, tuple) and len(item) == 2:
                                        rel, obj = item
                                        if (subj, rel, obj) not in all_triples:
                                            all_triples.append((subj, rel, obj))
                    except Exception as e:
                        self.logger.warning(f"Could not parse rel_map: {e}")

                self.logger.info(f"Extracted {len(all_triples)} triples (before filtering)")

                # Self-RAGçµ±åˆ
                # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
                if self.config.get('enable_triplet_filter', True):
                    filter_instance = TripletFilter(self.config, self.logger)
                    filtered_triples, rejected_triples, filter_stats = filter_instance.filter_triplets(
                        all_triples,
                        quality_threshold=self.config.get('triplet_quality_threshold', 0.3)
                    )
                    all_triples = filtered_triples

                    self.logger.info(
                        f"After filtering: {len(all_triples)} triples "
                        f"(rejection rate: {filter_stats['rejection_rate']:.1%})"
                    )

                # Self-RAGã‚’é©ç”¨ï¼ˆãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†ï¼‰
                if self.config.get('enable_self_rag', False):
                    with self.hlogger.section("Self-RAG Refinement"):
                        filter_instance = TripletFilter(self.config, self.logger)
                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ
                        refined_all_triples = []
                        total_self_rag_stats = {
                            'attempted': 0,
                            'succeeded': 0,
                            'failed': 0
                        }

                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                        doc_triplet_map = filter_instance._map_triplets_to_documents(all_triples, documents)

                        for doc_idx, (doc, doc_triplets) in enumerate(doc_triplet_map.items()):
                            if not doc_triplets:
                                continue

                            try:
                                refined_triplets, stats = filter_instance.self_rag_triplets(
                                    doc_triplets,
                                    doc.text,
                                    llm=llm  # æ—¢å­˜ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
                                )

                                refined_all_triples.extend(refined_triplets)

                                # çµ±è¨ˆã‚’é›†è¨ˆ
                                if stats.get('self_rag_applied'):
                                    ref_stats = stats['refinement_stats']
                                    total_self_rag_stats['attempted'] += ref_stats['attempted']
                                    total_self_rag_stats['succeeded'] += ref_stats['succeeded']
                                    total_self_rag_stats['failed'] += ref_stats['failed']

                                if (doc_idx + 1) % 10 == 0:
                                    self.logger.info(f"  Processed {doc_idx + 1}/{len(doc_triplet_map)} documents...")

                            except Exception as e:
                                self.logger.warning(f"  Self-RAG failed for doc {doc_idx}: {type(e).__name__}")
                                # å¤±æ•—æ™‚ã¯å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ä¿æŒ
                                refined_all_triples.extend(doc_triplets)

                        # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æ›´æ–°
                        all_triples = refined_all_triples

                        self.logger.info(
                            f"âœ… Self-RAG complete: "
                            f"{total_self_rag_stats['succeeded']} improved, "
                            f"{total_self_rag_stats['attempted']} attempted, "
                            f"final count: {len(all_triples)}"
                        )

                # å†åº¦å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
                if self.config.get('enable_triplet_filter', True):
                    filter_instance = TripletFilter(self.config, self.logger)
                    filtered_triples, rejected_triples, filter_stats = filter_instance.filter_triplets(
                        all_triples,
                        quality_threshold=self.config.get('triplet_quality_threshold', 0.3)
                    )
                    all_triples = filtered_triples

                    # çµ±è¨ˆæƒ…å ±ã‚’æ´»ç”¨
                    self.logger.info(
                        f"After filtering: {len(all_triples)} triples "
                        f"(rejection rate: {filter_stats['rejection_rate']:.1%})"
                    )

                    # å“è³ªãŒä½ã„å ´åˆã¯è­¦å‘Š
                    if filter_stats['avg_quality'] < 0.5:
                        self.logger.warning("âš ï¸  Low average triplet quality!")

                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ãƒªã‚¸ã‚§ã‚¯ãƒˆä¾‹ã‚’è¡¨ç¤º
                    if rejected_triples and self.logger.level <= 10:  # logging.DEBUG
                        self.logger.debug("Sample rejected triplets:")
                        for s, r, o in rejected_triples[:3]:
                            self.logger.debug(f"  ({s}, {r}, {o})")

                # ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å…¨ãƒˆãƒªãƒ—ãƒ«ã‚’å…±æœ‰
                for doc in documents:
                    doc.metadata['triples'] = all_triples

        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Graph generation failed: {type(e).__name__}"
            )
            raise

        # Entity Linking
        try:
            with self.hlogger.section("Entity Linking"):
                linker = EntityLinker(self.config, self.logger)
                kg, entity_mapping = linker.link_entities(
                    kg,
                    similarity_threshold=self.config['entity_linking_threshold'],
                    use_embedding=True
                )

                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæ›´æ–°
                updated_triples = []
                for s, r, o in all_triples:
                    s_new = entity_mapping.get(s, s)
                    o_new = entity_mapping.get(o, o)
                    if s_new != o_new:  # è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å¤–
                        updated_triples.append((s_new, r, o_new))

                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                for doc in documents:
                    doc.metadata['triples'] = updated_triples

                self.logger.info(f"Updated triples: {len(all_triples)} â†’ {len(updated_triples)}")

        except Exception as e:
            self.logger.warning(f"âš ï¸  Entity linking failed: {e}")
            # Entity Linkingå¤±æ•—ã§ã‚‚å‡¦ç†ã¯ç¶™ç¶š

        # ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã«çµ±åˆ
        try:
            with self.hlogger.section("Merging Path Information"):
                self.merge_paths_into_kg(kg, documents)
                self.logger.info(f"âœ… Path info merged: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

        except Exception as e:
            self.logger.warning(f"âš ï¸  Path merging failed: {type(e).__name__} - {str(e)[:100]}")

            self.logger.info("  â†’ Continuing without path information")

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¨˜éŒ²
        if self.logger.level <= 10:  # logging.DEBUG
            import traceback
            self.logger.debug(f"Path merge traceback:\n{traceback.format_exc()}")

        # documentsã‹ã‚‰pathsæƒ…å ±ã‚’å‰Šé™¤ï¼ˆä¸­é€”åŠç«¯ãªãƒ‡ãƒ¼ã‚¿ã‚’æ®‹ã•ãªã„ï¼‰
        for doc in documents:
            doc.metadata.pop('paths', None)
            doc.metadata.pop('path_distances', None)

        # RAPLæœ€é©åŒ–
        try:
            with self.hlogger.section("Graph Optimization (RAPL)"):
                optimized_kg = self._optimize_graph_rapl(kg, documents)
                self.logger.info(
                    f"âœ… Optimized graph: {len(optimized_kg.nodes)} nodes, "
                    f"{len(optimized_kg.edges)} edges"
                )
        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Graph optimization failed: {e}")
            optimized_kg = kg

        # Multi-hop ãƒ‘ã‚¹æ¢ç´¢ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§ä»£è¡¨çš„ãªãƒ‘ã‚¹ã‚’è¨ˆç®—ï¼‰
        try:
            with self.hlogger.section("Multi-hop Path Pre-computation"):
                explorer = MultiHopExplorer(self.config, self.logger)
                explorer._precompute_representative_paths(optimized_kg, documents)
                self.logger.info("âœ… Representative paths computed and stored")

        except Exception as e:
            self.logger.warning(f"âš ï¸  Path pre-computation failed: {type(e).__name__} - {str(e)[:100]}")
            self.logger.info("  â†’ Continuing without pre-computed paths")

            if self.logger.level <= 10:  # logging.DEBUG
                import traceback
                self.logger.debug(f"Path pre-computation traceback:\n{traceback.format_exc()}")

        # æœ€é©åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’Neo4jã«åæ˜ 
        try:
            with self.hlogger.section("Updating Neo4j"):
                result = self._update_neo4j_structure(optimized_kg, graph_store)

                # result ãŒ None ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if result is None:
                    result = {'updated': 0, 'skipped': 0, 'failed': 0, 'error_details': []}

                self.logger.warning("âš ï¸  _update_neo4j_structure returned None")

                # çµæœã‚µãƒãƒªãƒ¼
                self.logger.info(
                    f"âœ… Neo4j update complete:\n"
                    f"   - Updated: {result.get('updated', 0)} edges\n"
                    f"   - Skipped: {result.get('skipped', 0)} edges\n"
                    f"   - Failed: {result.get('failed', 0)} edges"
                )

                # å¤±æ•—ç‡ãŒé«˜ã„å ´åˆã¯è­¦å‘Š
                total = result.get('updated', 0) + result.get('failed', 0)
                if total > 0 and result.get('failed', 0) / total > 0.3:
                    self.logger.warning(
                        f"âš ï¸  High failure rate ({result.get('failed', 0)/total:.1%}). "
                        f"Check Neo4j constraints and data format."
                    )

        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j update failed: {e}")
            raise

    def merge_paths_into_kg(self, kg, documents: List[Document]):
        """
        kg: networkx.Graph (triples turned into nodes/edges)
        documents: the same documents that have metadata['paths'] etc.
        This will:
          - count how many times each entity appears in top-k paths
          - add edge/node attributes: top_path_count, avg_path_length
        """
        path_entity_counts = Counter()
        entity_path_lengths = defaultdict(list)

        for doc in documents:
            paths = doc.metadata.get('paths', [])  # each path is a str like "A -> B -> C" OR list; adapt if needed
            distances = doc.metadata.get('path_distances', [])
            for i, p in enumerate(paths):
                # normalize path representation
                if isinstance(p, str):
                    nodes = [n.strip() for n in p.split('->') if n.strip()]
                elif isinstance(p, (list, tuple)):
                    nodes = list(p)
                else:
                    continue

                dist = distances[i] if i < len(distances) else len(nodes)-1
                for n in nodes:
                    path_entity_counts[n] += 1
                    entity_path_lengths[n].append(dist)

                # if the path describes relations, you could also add edges for consecutive nodes
                for a, b in zip(nodes, nodes[1:]):
                    if kg.has_edge(a, b):
                        # add a path_support counter on existing edge
                        kg[a][b].setdefault('path_support', 0)
                        kg[a][b]['path_support'] += 1
                    else:
                        kg.add_edge(a, b, relation='path_inferred', path_support=1)

        # inject aggregated attrs to nodes
        for n in kg.nodes():
            cnt = path_entity_counts.get(n, 0)
            lens = entity_path_lengths.get(n, [])
            avg_len = sum(lens)/len(lens) if lens else None
            kg.nodes[n]['path_top_count'] = cnt
            if avg_len is not None:
                kg.nodes[n]['path_avg_length'] = avg_len

    def _optimize_graph_rapl(self, kg, documents):
        """
        RAPL æœ€é©åŒ–
        """

        # 1. Triples æŠ½å‡º
        doc_triples = {}
        for idx, doc in enumerate(documents):
            triples = doc.metadata.get("triples", [])
            if triples:  # ç©ºãƒªã‚¹ãƒˆã¯é™¤å¤–
                doc_triples[idx] = triples

        all_triples = [t for lst in doc_triples.values() for t in lst]

        self.logger.info(f"Total triples: {len(all_triples)}")

        # Weight æ ¼ç´é ˜åŸŸã®åˆæœŸåŒ–
        for u, v in kg.edges():
            kg[u][v]["intra_raw"] = 0.0
            kg[u][v]["inter_raw"] = 0.0

        # 2. Intra: æ–‡æ›¸å†… triple é–“ç›¸äº’ä½œç”¨
        self.logger.info("Computing intra-interactions...")
        intra_collector = ErrorCollector(self.logger)
        intra_edges = 0

        for doc_id, triples in doc_triples.items():
            try:
                entities = set()
                for s, _, o in triples:
                    entities.add(s)
                    entities.add(o)

                # Triple é–“ã®ç›¸äº’ä½œç”¨ï¼ˆé–¢ä¿‚ã®ç›¸æ€§ã‚’è€ƒæ…®ï¼‰
                for i in range(len(triples)):
                    s1, r1, o1 = triples[i]
                    for j in range(i + 1, len(triples)):
                        s2, r2, o2 = triples[j]

                        # é–¢ä¿‚ã®ç›¸æ€§
                        try:
                            rel_compat = self._compute_relation_compatibility(r1, r2)

                            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…±æœ‰åº¦
                            shared = len({s1, o1} & {s2, o2})
                            shared_score = shared * 0.5

                            # çµ±åˆé‡ã¿
                            w = rel_compat * 0.6 + shared_score * 0.4

                            if w > 0.3:
                                if kg.has_edge(s1, o1):
                                    kg[s1][o1]["intra_raw"] += w
                                if kg.has_edge(s2, o2):
                                    kg[s2][o2]["intra_raw"] += w
                            intra_collector.add_success()
                        except Exception as e:
                            intra_collector.add_error(
                                context=f"doc_{doc_id}_triple_{i}_{j}",
                                error=e,
                                triple1=(s1, r1, o1),
                                triple2=(s2, r2, o2)
                            )

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢é–“ã®ã‚¨ãƒƒã‚¸è¿½åŠ 
                for e1 in entities:
                    for e2 in entities:
                        if e1 != e2:
                            try:
                                w = self._compute_intra_weight(e1, e2, triples, kg)
                                if w > 0.5:
                                    if kg.has_edge(e1, e2):
                                        kg[e1][e2]["weight"] = kg[e1][e2].get("weight", 0) + w
                                    else:
                                        kg.add_edge(e1, e2, relation="intra_doc", weight=w)
                                        intra_edges += 1
                            except Exception as e:
                                intra_collector.add_error(
                                    context=f"entity_pair_{e1}_{e2}",
                                    error=e
                                )
            except Exception as e:
                self.logger.error(f"Failed to process document {doc_id}: {type(e).__name__}")
                continue

        intra_collector.report("Intra-document processing", threshold=0.3)
        self.logger.info(f"Added {intra_edges} intra-document edges")

        # 3. Inter: å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é«˜é€ŸåŒ–
        self.logger.info("Computing inter-interactions (optimized & sampled)...")
        inter_collector = ErrorCollector(self.logger)

        # 3-1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£â†’Triple ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        entity_to_triples = defaultdict(set)
        for idx, (s, _, o) in enumerate(all_triples):
            entity_to_triples[s].add(idx)
            entity_to_triples[o].add(idx)

        # 3-2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å‡ºç¾é »åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆä¸Šä½ã®ã¿å‡¦ç†ï¼‰
        entity_freq = [(entity, len(triple_indices))
                       for entity, triple_indices in entity_to_triples.items()]
        entity_freq.sort(key=lambda x: x[1], reverse=True)

        # ä¸Šä½100ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿å‡¦ç†ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
        max_entities = min(100, len(entity_freq))
        top_entities = set(entity for entity, _ in entity_freq[:max_entities])

        self.logger.info(
            f"  Sampled {max_entities}/{len(entity_to_triples)} entities "
            f"(covering {sum(freq for _, freq in entity_freq[:max_entities])} triples)"
        )

        # 3-2. å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹ Triple ãƒšã‚¢ã®ã¿è¨ˆç®—
        seen_pairs = set()
        inter_count = 0

        for _entity, triple_indices in entity_to_triples.items():
            if _entity not in top_entities:
                continue  # ä¸Šä½ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(triple_indices) < 3:
                continue

            indices = list(triple_indices)
            for i in range(len(indices)):
                for j in range(i + 1, len(indices)):
                    idx1, idx2 = indices[i], indices[j]
                    pair = (min(idx1, idx2), max(idx1, idx2))
                    if pair in seen_pairs:
                        continue
                    seen_pairs.add(pair)

                    # é‡ã¿è¨ˆç®—
                    try:
                        t1 = all_triples[idx1]
                        t2 = all_triples[idx2]
                        w = self._compute_inter_weight(t1, t2, kg=kg)

                        if w > self.config['relation_compat_threshold']:
                            s1, _, o1 = t1
                            s2, _, o2 = t2

                            # åŒæ–¹å‘ã«é‡ã¿ã‚’åŠ ç®—
                            if kg.has_edge(s1, o1):
                                kg[s1][o1]["inter_raw"] = kg[s1][o1].get("inter_raw", 0.0) + w
                            if kg.has_edge(s2, o2):
                                kg[s2][o2]["inter_raw"] = kg[s2][o2].get("inter_raw", 0.0) + w

                            inter_count += 1
                        inter_collector.add_success()

                    except Exception as e:
                        inter_collector.add_error(
                            context=f"triple_pair_{idx1}_{idx2}",
                            error=e
                        )

        inter_collector.report("Inter-document processing", threshold=0.3)
        self.logger.info(f"Added {inter_count} meaningful inter-interactions")

        # 4. Document-level linking
        self.logger.info("Computing document-level connections...")

        try:
            entity_docs = {}
            for doc_id, triples in doc_triples.items():
                for s, _, o in triples:
                    entity_docs.setdefault(s, set()).add(doc_id)
                    entity_docs.setdefault(o, set()).add(doc_id)

            doc_pairs = {}
            bridge_entities = []

            for entity_name, doc_set in entity_docs.items():
                if len(doc_set) > 1:
                    docs = list(doc_set)
                    for i, d1 in enumerate(docs):
                        for d2 in docs[i+1:]:
                            pair = (d1, d2)
                            doc_pairs[pair] = doc_pairs.get(pair, 0) + 1

                    if len(doc_set) > 2:
                        bridge_entities.append((entity_name, len(doc_set)))

            # ãƒ–ãƒªãƒƒã‚¸ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ­ã‚°
            if bridge_entities:
                bridge_entities.sort(key=lambda x: x[1], reverse=True)
                self.logger.info("Top bridge entities:")
                for entity_name, count in bridge_entities[:5]:
                    self.logger.info(f"  '{entity_name}': {count} documents")

            inter_doc_count = 0
            for (d1, d2), ct in doc_pairs.items():
                if ct > 2:
                    n1 = f"doc_{d1}"
                    n2 = f"doc_{d2}"

                    if not kg.has_node(n1):
                        kg.add_node(n1, type="document")
                    if not kg.has_node(n2):
                        kg.add_node(n2, type="document")

                    kg.add_edge(n1, n2, relation="inter_doc", weight=ct)
                    inter_doc_count += 1
            self.logger.info(f"Added {inter_doc_count} inter-document links")

        except Exception as e:
            self.logger.error(f"Document linking failed: {type(e).__name__} - {str(e)[:100]}")

        # çµ±åˆé‡ã¿è¨ˆç®—
        kg = self._normalize_edge_weights(kg, doc_triples, method='minmax')
        self.logger.info("Finalizing edge weights with normalization...")

        for u, v, d in kg.edges(data=True):
            intra = d.get("intra_normalized", d.get("intra_raw", 0.0))
            inter = d.get("inter_normalized", d.get("inter_raw", 0.0))

            # RAPLè«–æ–‡: intraé‡è¦– + interè£œå®Œ
            d["weight"] = min(0.7 * intra + 0.3 * inter, 1.0)

        self.logger.info(f"Weight calculation complete: {len(kg.edges())} edges")
        return kg

    def _normalize_edge_weights(self, kg: nx.Graph, doc_triples: Dict[int, List[Tuple]], method: str = 'minmax') -> nx.Graph:
        """
        ã‚¨ãƒƒã‚¸é‡ã¿ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«æ­£è¦åŒ–

        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            doc_triples: {doc_id: [(s, r, o), ...]} ã®è¾æ›¸
            method: 'minmax' ã¾ãŸã¯ 'zscore'

        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•
        """
        self.logger.info(f"Normalizing edge weights (method={method})...")

        # ============================================================
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«é‡ã¿ã‚’åé›†
        # ============================================================
        doc_edge_weights = defaultdict(lambda: {'intra': [], 'inter': []})
        edge_to_docs = defaultdict(set)  # ã‚¨ãƒƒã‚¸ãŒã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹

        for doc_id, triples in doc_triples.items():
            doc_entities = set()
            for s, _, o in triples:
                doc_entities.add(s)
                doc_entities.add(o)

            # ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢é€£ã™ã‚‹ã‚¨ãƒƒã‚¸ã‚’æ¢ã™
            for u, v, data in kg.edges(data=True):
                if u in doc_entities or v in doc_entities:
                    edge_key = (u, v)
                    edge_to_docs[edge_key].add(doc_id)

                    intra_raw = data.get('intra_raw', 0.0)
                    inter_raw = data.get('inter_raw', 0.0)

                    if intra_raw > 0:
                        doc_edge_weights[doc_id]['intra'].append(intra_raw)
                    if inter_raw > 0:
                        doc_edge_weights[doc_id]['inter'].append(inter_raw)

        # ============================================================
        # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
        # ============================================================
        norm_params = {}

        for doc_id, weights in doc_edge_weights.items():
            params = {}

            for weight_type in ['intra', 'inter']:
                values = weights[weight_type]

                if not values:
                    params[weight_type] = None
                    continue

                if method == 'minmax':
                    min_val = min(values)
                    max_val = max(values)
                    params[weight_type] = {
                        'min': min_val,
                        'max': max_val,
                        'range': max_val - min_val
                    }

                elif method == 'zscore':
                    mean_val = np.mean(values)
                    std_val = np.std(values)
                    params[weight_type] = {
                        'mean': mean_val,
                        'std': std_val if std_val > 0 else 1.0
                    }

            norm_params[doc_id] = params

        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self._log_normalization_stats(doc_edge_weights, norm_params)

        # ============================================================
        # 3. ã‚¨ãƒƒã‚¸ã”ã¨ã«æ­£è¦åŒ–ã‚’é©ç”¨
        # ============================================================
        normalized_count = 0

        for u, v, data in kg.edges(data=True):
            edge_key = (u, v)
            related_docs = edge_to_docs.get(edge_key, set())

            if not related_docs:
                continue

            # ã“ã®ã‚¨ãƒƒã‚¸ã«é–¢é€£ã™ã‚‹å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ­£è¦åŒ–å€¤ã‚’å¹³å‡
            intra_normalized = []
            inter_normalized = []

            for doc_id in related_docs:
                if doc_id not in norm_params:
                    continue

                params = norm_params[doc_id]
                intra_raw = data.get('intra_raw', 0.0)
                inter_raw = data.get('inter_raw', 0.0)

                # Intraæ­£è¦åŒ–
                if params['intra'] and intra_raw > 0:
                    norm_val = self._normalize_value(
                        intra_raw,
                        params['intra'],
                        method
                    )
                    intra_normalized.append(norm_val)

                # Interæ­£è¦åŒ–
                if params['inter'] and inter_raw > 0:
                    norm_val = self._normalize_value(
                        inter_raw,
                        params['inter'],
                        method
                    )
                    inter_normalized.append(norm_val)

            # æ­£è¦åŒ–å¾Œã®å€¤ã‚’å¹³å‡
            if intra_normalized:
                data['intra_normalized'] = np.mean(intra_normalized)
                normalized_count += 1
            else:
                data['intra_normalized'] = data.get('intra_raw', 0.0)

            if inter_normalized:
                data['inter_normalized'] = np.mean(inter_normalized)
            else:
                data['inter_normalized'] = data.get('inter_raw', 0.0)

        self.logger.info(f"  â†’ Normalized {normalized_count} edges")

        return kg

    def _normalize_value(self, value: float, params: dict, method: str) -> float:
        """
        å˜ä¸€ã®å€¤ã‚’æ­£è¦åŒ–

        Args:
            value: æ­£è¦åŒ–ã™ã‚‹å€¤
            params: æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            method: 'minmax' ã¾ãŸã¯ 'zscore'

        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸå€¤
        """
        if method == 'minmax':
            min_val = params['min']
            max_val = params['max']
            range_val = params['range']

            if range_val < 1e-9:
                return 0.5  # å…¨ã¦åŒã˜å€¤ã®å ´åˆã¯ä¸­é–“å€¤

            # [0, 1] ã«æ­£è¦åŒ–
            normalized = (value - min_val) / range_val
            return max(0.0, min(1.0, normalized))

        elif method == 'zscore':
            mean_val = params['mean']
            std_val = params['std']

            # z-scoreã‚’è¨ˆç®—å¾Œã€sigmoidã§ [0, 1] ã«å¤‰æ›
            z = (value - mean_val) / std_val
            sigmoid = 1 / (1 + np.exp(-z))
            return sigmoid

        return value

    def _log_normalization_stats(self, doc_edge_weights: dict, norm_params: dict):
        """æ­£è¦åŒ–çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›"""
        self.logger.info("  Normalization statistics:")

        for doc_id in list(norm_params.keys())[:3]:  # æœ€åˆã®3ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            params = norm_params[doc_id]

            intra_weights = doc_edge_weights[doc_id]['intra']
            inter_weights = doc_edge_weights[doc_id]['inter']

            if intra_weights:
                self.logger.info(
                    f"    Doc {doc_id} intra: "
                    f"min={min(intra_weights):.3f}, "
                    f"max={max(intra_weights):.3f}, "
                    f"mean={np.mean(intra_weights):.3f}"
                )

            if inter_weights:
                self.logger.info(
                    f"    Doc {doc_id} inter: "
                    f"min={min(inter_weights):.3f}, "
                    f"max={max(inter_weights):.3f}, "
                    f"mean={np.mean(inter_weights):.3f}"
                )

    def _compute_intra_weight(self, e1: str, e2: str, triples: List, kg=None) -> float:
        """
        åŒä¸€Documentå†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“é‡ã¿è¨ˆç®—

        Args:
            e1, e2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å
            triples: (s, r, o) ã®ãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        # ------------------------------------------------------------
        # 1) å…±èµ·é »åº¦ï¼ˆåŸºæœ¬ï¼‰
        # ------------------------------------------------------------
        cooccur = sum(
            1 for s, _, o in triples
            if (s == e1 and o == e2) or (s == e2 and o == e1)
        )
        co_norm = min(cooccur / 5.0, 1.0)   # æ­£è¦åŒ–

        # ------------------------------------------------------------
        # 2) é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§
        # ------------------------------------------------------------
        rel_pairs = [
            (r, True) for s, r, o in triples
            if (s == e1 and o == e2)
        ] + [
            (r, False) for s, r, o in triples
            if (s == e2 and o == e1)  # é€†å‘ã
        ]

        if not rel_pairs:
            rel_bonus = 0.0
        else:
            # é–¢ä¿‚ã®å¤šæ§˜æ€§
            unique_rels = set(r for r, _ in rel_pairs)
            diversity_bonus = min(len(unique_rels) * 0.2, 0.6)

            # æ–¹å‘ã®ä¸€è²«æ€§ï¼ˆåŒã˜å‘ããŒå¤šã„ã»ã©å¼·ã„é–¢ä¿‚ï¼‰
            same_direction_count = sum(1 for _, is_forward in rel_pairs if is_forward)
            opposite_direction_count = len(rel_pairs) - same_direction_count

            # é–¢ä¿‚ã®è³ªï¼ˆåŒã˜å‘ãã‹é€†å‘ãã‹ã§è©•ä¾¡ï¼‰
            if same_direction_count > opposite_direction_count:
                direction_score = same_direction_count / len(rel_pairs)
            else:
                # é€†æ–¹å‘ãŒå¤šã„ = åŒæ–¹å‘ã®é–¢ä¿‚ï¼ˆã“ã‚Œã‚‚æœ‰ç”¨ï¼‰
                direction_score = 0.7  # ã‚„ã‚„é«˜ã‚ã«è©•ä¾¡

            rel_bonus = diversity_bonus * 0.5 + direction_score * 0.5

        # ------------------------------------------------------------
        # 3) ãƒ‘ã‚¹ã‚µãƒãƒ¼ãƒˆï¼ˆkgã« path_support ãŒã‚ã‚Œã°ï¼‰
        # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None and kg.has_edge(e1, e2):
            path_bonus = min(kg[e1][e2].get("path_support", 0) * 0.1, 0.5)

        # ------------------------------------------------------------
        # 4) åˆæˆ
        # ------------------------------------------------------------
        weight = co_norm * 0.5 + rel_bonus * 0.4 + path_bonus * 0.1
        return min(weight, 1.0)

    def _compute_inter_weight(self, t1: tuple, t2: tuple, kg=None):
        """inter-triple interaction weightè¨ˆç®—"""

        s1, r1, o1 = t1
        s2, r2, o2 = t2

        # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆæœ€é‡è¦ï¼‰
        shared = len({s1, o1} & {s2, o2})
        shared_bonus = min(shared * 0.5, 1.0)
        # é–¢ä¿‚ã®ç›¸æ€§è¨ˆç®—
        rel_compatibility = safe_execute(
            self._compute_relation_compatibility,
            args=(r1, r2),
            default=0.3,
            logger=self.logger,
            context=f"relation_compatibility({r1}, {r2})"
        )
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
        sim_bonus = 0.0
        try:
            e1 = self.embed_model.get_text_embedding(s1)
            e2 = self.embed_model.get_text_embedding(s2)

            # æ­£è¦åŒ–æ¸ˆã¿ãªã®ã§ç›´æ¥å†…ç©ã‚’è¨ˆç®—
            sim = float(np.dot(e1, e2))
            sim_bonus = max(sim, 0) * 0.3

        except Exception as e:
            if not hasattr(self, '_embedding_error_warned'):
                self.logger.warning(f"âš ï¸  Embedding similarity errors detected")
                self._embedding_error_warned = True

        # 3) graph path-based supportï¼ˆkgãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
        path_bonus = 0.0
        if kg is not None:
            try:
                # 2-hopä»¥å†…ã§ã¤ãªãŒã£ã¦ãŸã‚‰è©•ä¾¡
                if kg.has_node(s1) and kg.has_node(s2):
                    length = nx.shortest_path_length(kg, s1, s2)
                    if length <= 2:
                        path_bonus = 0.3 * (1.0 - length / 3.0)  # è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            except nx.NetworkXNoPath:
                pass
            except nx.NodeNotFound:
                if self.logger.level <= 10:  # logging.DEBUG
                    self.logger.debug(f"Node not found in graph: {s1} or {s2}")
            except Exception as e:
                if self.logger.level <= 10:  # logging.DEBUG
                    self.logger.debug(f"Path calc failed ({s1}->{s2}): {type(e).__name__}")

        # 4) ç·åˆ
        w = (
            shared_bonus * 0.4 +       # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            rel_compatibility * 0.3 +   # é–¢ä¿‚ã®ç›¸æ€§ï¼ˆã“ã“ã«çµ±åˆæ¸ˆã¿ï¼‰
            sim_bonus * 0.2 +           # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
            path_bonus * 0.1            # ãƒ‘ã‚¹è·é›¢
        )

        return min(w, 1.0)

    def _compute_relation_compatibility(self, r1: str, r2: str) -> float:
        """
        é–¢ä¿‚ã®ç›¸æ€§ã‚¹ã‚³ã‚¢
        """
        # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢çµ±ä¸€ï¼‰
        r1 = r1.lower().replace('-', '_')
        r2 = r2.lower().replace('-', '_')
        # 1. å®Œå…¨ä¸€è‡´
        if r1 == r2:
            return 1.0

        # 2. é€†é–¢ä¿‚ã®ãƒšã‚¢ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        inverse_pairs = {
            ("cause_of", "caused_by"),
            ("cause_of", "effect_of"),
            ("part_of", "has_part"),
            ("component_of", "has_component"),
            ("parent_of", "child_of"),
            ("author_of", "written_by"),
            ("owns", "owned_by"),
            ("manages", "managed_by"),
            ("teaches", "taught_by"),
            ("supervises", "supervised_by"),
        }

        if (r1, r2) in inverse_pairs or (r2, r1) in inverse_pairs:
            return 0.9

        # 3. é–¢é€£ã™ã‚‹é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        related_groups = [
            # å› æœé–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "cause_of", "caused_by", "leads_to", "results_in",
                "triggers", "produces", "generates", "effect_of"
            },

            # æ§‹æˆè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "part_of", "has_part", "component_of", "has_component",
                "contains", "includes", "consists_of", "comprises"
            },

            # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "member_of", "has_member", "belongs_to", "works_at",
                "employed_by", "affiliated_with"
            },

            # æ™‚é–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "before", "after", "during", "precedes", "follows",
                "happens_before", "happens_after"
            },

            # ç©ºé–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "located_in", "location_of", "near", "adjacent_to",
                "contains", "inside", "outside"
            },

            # å±æ€§ãƒ»æ€§è³ªã‚°ãƒ«ãƒ¼ãƒ—
            {
                "is_a", "type_of", "instance_of", "has_property",
                "characterized_by", "defined_by"
            },

            # ç›¸äº’ä½œç”¨ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "interacts_with", "collaborates_with", "competes_with",
                "influences", "affected_by"
            },
        ]

        for group in related_groups:
            if r1 in group and r2 in group:
                return 0.7

        # 4. åŒã˜ã‚«ãƒ†ã‚´ãƒªï¼ˆå‹•è©ã®æ€§è³ªã§åˆ¤å®šï¼‰
        # ä¾‹: action ç³»ã€state ç³»ãªã©
        action_verbs = {
            "creates", "builds", "develops", "produces", "makes",
            "constructs", "designs", "implements", "generates",
            "enables", "powers", "leverages", "accelerates"
            # ï¼ˆML/AIå°‚é–€ï¼‰
            "utilizes", "parameterizes", "fine_tunes", "approximates",
            "encodes", "regularizes", "iterates", "optimizes",
            "traverses", "samples", "augments", "normalizes",
            "quantizes", "distills", "ensembles", "prunes",
            "compresses", "aggregates", "fuses", "aligns",
            "projects", "embeds", "transforms", "adapts",

            # CVç³»
            "detects", "segments", "classifies", "recognizes",
            "extracts", "filters", "convolves", "pools",

            # NLPç³»
            "tokenizes", "parses", "generates_text", "translates",
            "attends_to", "masks", "predicts",

            # Graphç³»
            "propagates", "aggregates_neighbors", "diffuses",
            "clusters", "partitions", "samples_neighbors"
        }

        state_verbs = {
            "is", "has", "contains", "includes", "comprises",
            "exists", "represents", "defines", "consists_of",
            "maintains", "preserves", "exhibits", "displays"
        }

        relation_verbs = {
            "relates_to", "associated_with", "connected_to",
            "linked_to", "corresponds_to", "depends_on",
            "derived_from", "based_on", "inspired_by"
        }

        # --- 3-4. è¨ˆç®—å‹•è© ---
        computational_verbs = {
            "computes", "calculates", "evaluates", "measures",
            "estimates", "infers", "learns", "trains",
            "updates", "backpropagates", "forward_passes"
        }

        # --- 3-5. æ¯”è¼ƒå‹•è© ---
        comparison_verbs = {
            "outperforms", "surpasses", "exceeds", "improves_upon",
            "compares_to", "contrasts_with", "benchmarks_against"
        }

        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°
        verb_categories = [
            action_verbs,
            state_verbs,
            relation_verbs,
            computational_verbs,
            comparison_verbs
        ]

        for category in verb_categories:
            if r1 in category and r2 in category:
                return 0.5

        # 5. åŸ‹ã‚è¾¼ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
        try:
            emb1 = self.embed_model.get_text_embedding(r1)
            emb2 = self.embed_model.get_text_embedding(r2)
            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)
            return max(0.3, float(sim))
        except Exception:
            return 0.3

    def _update_neo4j_structure(self, kg, graph_store):
        """
        Neo4jæ›´æ–°
        """
        batch_query  = """
        UNWIND $batch AS row
        MERGE (a:Concept {name: row.source})
        MERGE (b:Concept {name: row.target})
        MERGE (a)-[r:RELATED]->(b)
        ON CREATE SET r.weight = row.weight
        ON MATCH SET r.weight = row.weight
        """
        collector = ErrorCollector(self.logger)

        batch = []
        batch_size = 1000  # 1000ä»¶ã”ã¨ã«é€ä¿¡

        for s, o, data in kg.edges(data=True):
            weight = data.get('weight', 0.0)

            if weight <= self.config['final_weight_cutoff']:
                collector.add_skip()
                continue

            # ãƒãƒƒãƒã«è¿½åŠ 
            batch.append({
                'source': s,
                'target': o,
                'weight': float(weight)
            })

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰é€ä¿¡
        if len(batch) >= batch_size:

            try:
                graph_store.query(batch_query, {'batch': batch})
                collector.add_success(count=len(batch))

                self.logger.debug(f"  Sent batch of {len(batch)} edges")
                batch = []  # ãƒãƒƒãƒã‚’ã‚¯ãƒªã‚¢

            except Exception as e:
                collector.add_error(
                    context=f"batch_{len(batch)}_edges",
                    error=e
                )
                # å¤±æ•—ã—ãŸãƒãƒƒãƒã¯ç ´æ£„ï¼ˆã¾ãŸã¯å€‹åˆ¥å‡¦ç†ï¼‰
                batch = []

        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’é€ä¿¡
        if batch:
            try:
                graph_store.query(batch_query, {'batch': batch})
                collector.add_success(count=len(batch))
                self.logger.debug(f"  Sent final batch of {len(batch)} edges")

            except Exception as e:
                collector.add_error(
                    context=f"final_batch_{len(batch)}_edges",
                    error=e
                )
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆè‡ªå‹•ã§ãƒ­ã‚°å‡ºåŠ›ï¼‰
        collector.report("Neo4j edge update", threshold=0.3)
        # æˆ»ã‚Šå€¤ã‚‚å–å¾—å¯èƒ½
        return collector.get_summary()