"""
ãƒ¡ã‚¤ãƒ³çµ±åˆã‚¯ãƒ©ã‚¹ - Crystal Cluster
"""
import numpy as np
import networkx as nx
from typing import List, Dict, Any, Optional, Tuple, Iterable
from collections import defaultdict
import itertools

from llama_index.core import Document
from llama_index.graph_stores.neo4j import Neo4jGraphStore


from config.config_manager import _load_config
from shared import setup_logger, HierarchicalLogger
from shared.ai_router import AIRouter
from processors import DocumentProcessor, ChunkProcessor
from builders import GraphBuilder, RetrievalBuilder
from linkers import EntityLinker
from filters import TripletFilter
from rag import MultiHopExplorer


class CrystalCluster:
    """Crystal Cluster - Neo4jæŠ•å…¥å°‚ç”¨"""

    def __init__(self, log_level: int = 20, use_dual_chunk: bool = False, custom_config: dict = None):
        """
        Args:
            use_dual_chunk: Trueãªã‚‰ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        """
        self.logger = setup_logger('CrystalCluster', log_level)
        self.hlogger = HierarchicalLogger(self.logger)
        self.use_dual_chunk = use_dual_chunk

        default_config = {
            'entity_linking_threshold': 0.88,
            'retrieval_chunk_size': 320,
            'retrieval_chunk_overlap': 120,
            'graph_chunk_size': 512,
            'graph_chunk_overlap': 50,
            'relation_compat_threshold': 0.11,
            'final_weight_cutoff': 0.035,
            'max_triplets_per_chunk': 15,
            'llm_model': 'gpt-4o-mini',
            'llm_timeout': 120.0,

            # Self-RAGè¨­å®š
            'enable_self_rag': True,                    # Self-RAGã‚’æœ‰åŠ¹åŒ–
            'self_rag_confidence_threshold': 0.75,       # å†ç”Ÿæˆã®é–¾å€¤
            'self_rag_critic_model': 'gpt-4o-mini',     # è©•ä¾¡ç”¨LLM
            'self_rag_refiner_model': 'gpt-5-mini',         # å†ç”Ÿæˆç”¨LLMï¼ˆã‚ˆã‚Šé«˜æ€§èƒ½ï¼‰
            'self_rag_max_retries': 1,                  # æœ€å¤§å†è©¦è¡Œå›æ•°
            'self_rag_token_budget': 100000,            # ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—
            'self_rag_validation_checks': [             # æ¤œè¨¼é …ç›®
                'entity_quality',
                'relation_clarity',
                'grammar',
                'redundancy'
            ],
            # Multi-hopè¨­å®šï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            'multihop_beam_width': 2,                   # ãƒ“ãƒ¼ãƒ å¹…ã‚’ç‹­ã
            'multihop_max_paths': 50,                   # ãƒ‘ã‚¹æ•°ä¸Šé™ã‚’è¿½åŠ 
            # RAPLæœ€é©åŒ–è¨­å®š
            'rapl_max_entities': 100,           # Interè¨ˆç®—ã§å‡¦ç†ã™ã‚‹æœ€å¤§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°
            'rapl_min_shared_triples': 3,       # å…±æœ‰ãƒˆãƒªãƒ—ãƒ«æ•°ã®æœ€å°å€¤ï¼ˆ2â†’3ï¼‰
            'neo4j_batch_size': 1000,           # Neo4jãƒãƒƒãƒã‚µã‚¤ã‚º

            # AIãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®š
            'ai_routing': {
                'mode': 'api',  # 'api' or 'ollama'
                'ollama_url': 'http://localhost:11434',
                'api_key': None  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚‚å¯
            }
        }

        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãŒã‚ã‚Œã°ä¸Šæ›¸ã
        if custom_config:
            default_config.update(custom_config)

        self.config = default_config
        self.config.setdefault('enable_triplet_filter', True)
        self.config.setdefault('triplet_quality_threshold', 0.3)

        # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        self.relation_blacklist = {
            'is', 'has', 'are', 'was', 'were',
            'the', 'a', 'an',
            'of', 'in', 'on', 'at',
        }

        from ..model import ensure_bge_m3
        self.embed_model = ensure_bge_m3()

        # AI RouteråˆæœŸåŒ–
        self.ai_router = AIRouter(config=self.config, logger=self.logger)

        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.document_processor = DocumentProcessor(self.logger)
        self.chunk_processor = ChunkProcessor(self.config, self.logger)
        self.graph_builder = GraphBuilder(self.config, self.embed_model, self.logger)
        self.retrieval_builder = RetrievalBuilder(self.embed_model, self.logger)
        self.entity_linker = EntityLinker(self.config, self.logger)
        self.triplet_filter = TripletFilter(self.config, self.logger)
        self.multi_hop_explorer = MultiHopExplorer(self.config, self.logger)

        self.entity_emb_cache = {}          # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.relation_emb_cache = {}        # é–¢ä¿‚åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.total_self_rag_tokens = 0      # Self-RAGãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿
        self.visited_paths = set()          # Multi-hopãƒ‘ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        self.logger.info(f"Crystal Cluster beta initialized")
        self.logger.info(f"Self-RAG: {'enabled' if self.config['enable_self_rag'] else 'disabled'}")
        self.logger.info(f"Config: {self.config}")

    def get_cached_embedding(
        self,
        text: str,
        cache_type: str = 'entity'
    ) -> np.ndarray:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—

        Args:
            text: ãƒ†ã‚­ã‚¹ãƒˆ
            cache_type: 'entity' ã¾ãŸã¯ 'relation'

        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é¸æŠ
        cache = self.entity_emb_cache if cache_type == 'entity' else self.relation_emb_cache

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
        if text in cache:
            return cache[text]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: è¨ˆç®—ã—ã¦ä¿å­˜
        try:
            emb = self.embed_model.get_text_embedding(text)
            emb = np.array(emb, dtype=np.float32)

            # æ­£è¦åŒ–
            norm = np.linalg.norm(emb)
            if norm > 1e-9:
                emb = emb / norm
            else:
                emb = np.zeros_like(emb)

            cache[text] = emb

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Šã™ããŸã‚‰è­¦å‘Š
            if len(cache) % 1000 == 0:
                self.logger.debug(f"  {cache_type} cache size: {len(cache)}")

            return emb

        except Exception as e:
            self.logger.debug(f"Embedding failed for '{text[:30]}': {type(e).__name__}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(1024, dtype=np.float32)

    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg=None,
        neo4j_store=None,
        enable_duplicate_check: bool = True
    ) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†"""
        self.document_processor.neo4j_store = neo4j_store
        return self.document_processor.load_documents(
            json_path=json_path,
            raw_docs=raw_docs,
            path_pickle=path_pickle,
            kg=kg,
            enable_duplicate_check=enable_duplicate_check
        )

    def create_dual_documents(
        self,
        documents: List[Document]
    ) -> Tuple[List[Document], List[Document]]:
        """ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        return self.chunk_processor.create_dual_documents(documents)

    def build_retrieval_store(
        self,
        retrieval_docs: List[Document]
    ) -> Dict[str, Any]:
        """æ¤œç´¢ã‚¹ãƒˆã‚¢æ§‹ç¯‰"""
        return self.retrieval_builder.build_retrieval_store(retrieval_docs)

    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5,
        chunk_mapping: Dict = None
    ) -> List[Tuple[float, Document, List[str]]]:
        """æ¤œç´¢å®Ÿè¡Œ"""
        return self.retrieval_builder.retrieve(
            store=store,
            query=query,
            top_k=top_k,
            chunk_mapping=chunk_mapping
        )

    def commit_to_graph(self, documents: List[Document], graph_store: Neo4jGraphStore):
        """ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã¨Neo4jæŠ•å…¥"""
        self.graph_builder.commit_to_graph(documents, graph_store)

    def commit_to_graph_with_retrieval(
        self,
        documents: List[Document],
        graph_store: Neo4jGraphStore
    ) -> Dict[str, Any]:
        """
        Graph index ã¨ Retrieval store ã‚’åŒæ™‚ã«æ§‹ç¯‰ï¼ˆåŒæœŸç‰ˆï¼‰
        """
        with self.hlogger.section("Dual-Index Building (Synced)"):
            # 1. Dual-documentsç”Ÿæˆï¼ˆåŒæœŸãƒãƒƒãƒ”ãƒ³ã‚°ä»˜ãï¼‰
            graph_docs, retrieval_docs = self.create_dual_documents(documents)

            # ç°¡æ˜“çš„ãªchunk_mappingä½œæˆ
            chunk_mapping = {'graph_to_retrieval': {}, 'retrieval_to_graph': {}}

            self.logger.info(
                f"ğŸ”— Chunk mapping: "
                f"{len(chunk_mapping['graph_to_retrieval'])} graph -> retrieval links"
            )

            # 2. Graphæ§‹ç¯‰
            self.logger.info("ğŸ“Š Building knowledge graph...")
            self.commit_to_graph(graph_docs, graph_store)

            # 3. Retrieval storeæ§‹ç¯‰
            self.logger.info("ğŸ” Building retrieval store...")
            retrieval_store = self.build_retrieval_store(retrieval_docs)

            # chunk_mappingã‚’storeã«è¿½åŠ 
            retrieval_store['chunk_mapping'] = chunk_mapping

        return {
            'retrieval_store': retrieval_store,
            'chunk_mapping': chunk_mapping,
            'stats': {
                'graph_docs': len(graph_docs),
                'retrieval_docs': len(retrieval_docs),
                'sync_links': len(chunk_mapping['retrieval_to_graph'])
            }
        }

    def query_with_multihop(
        self,
        query: str,
        kg,
        retrieval_store: Dict = None,
        max_steps: int = 5,
        top_k_retrieval: int = 5,
        top_k_paths: int = 10
    ) -> Dict[str, Any]:
        """Multi-hopæ¢ç´¢ã‚’ä½¿ã£ãŸã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        return self.multi_hop_explorer.query_with_multihop(
            query=query,
            kg=kg,
            retrieval_store=retrieval_store,
            max_steps=max_steps,
            top_k_retrieval=top_k_retrieval,
            top_k_paths=top_k_paths
        )

    def filter_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        quality_threshold: float = 0.3
    ) -> Tuple[List[Tuple], List[Tuple], Dict]:
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        return self.triplet_filter.filter_triplets(triplets, quality_threshold)

    def link_entities(
        self,
        kg,
        similarity_threshold: float = 0.88,
        use_embedding: bool = True
    ) -> Tuple[Any, Dict[str, str]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±åˆ"""
        return self.entity_linker.link_entities(
            kg=kg,
            similarity_threshold=similarity_threshold,
            use_embedding=use_embedding
        )
    
    def _extract_entity_contexts(
        self, 
        documents: List[Document], 
        window_sentences: int = 1
    ) -> Dict[str, List[str]]:
        """
        documentsã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡ã‚’æŠ½å‡º
        
        Args:
            documents: Documentãƒªã‚¹ãƒˆ
            window_sentences: entity ãŒç™»å ´ã™ã‚‹æ–‡ã®å‰å¾Œã«ä½•æ–‡å–ã‚‹ã‹
        
        Returns:
            {entity_name: [context_str1, context_str2, ...]}
        """
        import re
        
        entity_contexts = defaultdict(list)
        
        for doc in documents:
            text = getattr(doc, "text", "") or ""
            # æ–‡åˆ†å‰²ï¼ˆæ—¥æœ¬èªãƒ»è‹±èªå¯¾å¿œï¼‰
            pieces = [s.strip() for s in re.split(r'(?<=[ã€‚ï¼.!?])\s+', text) if s.strip()]

            # triples ãŒ metadata ã«ã‚ã‚Œã°ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ã†
            triples = doc.metadata.get("triples", [])
            candidate_entities = set()
            for s, r, o in triples:
                candidate_entities.add(str(s))
                candidate_entities.add(str(o))

            # metadata ã® 'keywords' ã‚‚å€™è£œã«è¿½åŠ 
            for kw in doc.metadata.get("keywords", []) if doc.metadata.get("keywords") else []:
                candidate_entities.add(str(kw))

            if not candidate_entities:
                continue

            # å„ entity ã®ç™»å ´æ–‡ã‚’æ¢ã—ã¦å‰å¾Œ window_sentences æ–‡ã‚’å–ã‚‹
            for ent in candidate_entities:
                ent_norm = str(ent).strip()
                if not ent_norm:
                    continue
                
                for i, sent in enumerate(pieces):
                    if ent_norm.lower() in sent.lower():
                        start = max(0, i - window_sentences)
                        end = min(len(pieces), i + window_sentences + 1)
                        ctx = " ".join(pieces[start:end])
                        entity_contexts[ent_norm].append(ctx)
                
                # å‡ºç¾ãŒãªã‘ã‚Œã°æ–‡æ›¸å†’é ­ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if len(entity_contexts[ent_norm]) == 0:
                    entity_contexts[ent_norm].append(pieces[0] if pieces else text[:200])

        return entity_contexts

    def _batch_embed_texts(
        self, 
        texts: Iterable[str], 
        batch_size: int = 32
    ) -> List[np.ndarray]:
        """
        self.embed_modelã‚’ç”¨ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿
        
        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        embeddings = []
        batch = []
        
        for t in texts:
            batch.append(t)
            if len(batch) >= batch_size:
                # llama_indexã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œ
                if hasattr(self.embed_model, 'get_text_embedding_batch'):
                    embs = self.embed_model.get_text_embedding_batch(batch)
                elif hasattr(self.embed_model, 'get_text_embedding'):
                    embs = [self.embed_model.get_text_embedding(t) for t in batch]
                else:
                    embs = self.embed_model.embed_batch(batch)
                
                for e in embs:
                    embeddings.append(np.array(e, dtype=np.float32))
                batch = []
        
        # æ®‹ã‚Šã®ãƒãƒƒãƒå‡¦ç†
        if batch:
            if hasattr(self.embed_model, 'get_text_embedding_batch'):
                embs = self.embed_model.get_text_embedding_batch(batch)
            elif hasattr(self.embed_model, 'get_text_embedding'):
                embs = [self.embed_model.get_text_embedding(t) for t in batch]
            else:
                embs = self.embed_model.embed_batch(batch)
            
            for e in embs:
                embeddings.append(np.array(e, dtype=np.float32))
        
        return embeddings

    def _detect_language_simple(self, text: str) -> str:
        """
        ç°¡æ˜“è¨€èªåˆ¤å®šï¼ˆæ—¥æœ¬èªãƒ»ä¸­å›½èªãƒ»éŸ“å›½èªãƒ»è‹±èªå¯¾å¿œï¼‰
        
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€‚langdetect ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚Œã°
        _detect_language_accurate() ãŒå„ªå…ˆã•ã‚Œã‚‹ã€‚
        
        Args:
            text: åˆ¤å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            è¨€èªã‚³ãƒ¼ãƒ‰ ('en', 'ja', 'zh', 'ko', 'other', 'unknown')
        """
        if not text:
            return "unknown"
        
        sample = text[:300]
        
        # æ–‡å­—ç¨®åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        hiragana = sum(1 for c in sample if '\u3040' <= c <= '\u309f')
        katakana = sum(1 for c in sample if '\u30a0' <= c <= '\u30ff')
        kanji = sum(1 for c in sample if '\u4e00' <= c <= '\u9faf')
        hangul = sum(1 for c in sample if '\uac00' <= c <= '\ud7af')
        ascii_chars = sum(1 for c in sample if ord(c) < 128)
        
        total = max(len(sample), 1)
        
        # æ—¥æœ¬èªåˆ¤å®šï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒå¤šã„ï¼‰
        if (hiragana + katakana) / total > 0.15:
            return "ja"
        
        # éŸ“å›½èªåˆ¤å®š
        if hangul / total > 0.3:
            return "ko"
        
        # ä¸­å›½èªåˆ¤å®šï¼ˆæ¼¢å­—ã®ã¿ã§æ—¥æœ¬èªçš„ãªæ–‡å­—ãŒãªã„ï¼‰
        if kanji / total > 0.3 and (hiragana + katakana) / total < 0.05:
            return "zh"
        
        # è‹±èªåˆ¤å®š
        if ascii_chars / total > 0.7:
            return "en"
        
        return "other"

    def _detect_language_accurate(self, text: str) -> str:
        """
        é«˜ç²¾åº¦è¨€èªåˆ¤å®šï¼ˆlangdetectä½¿ç”¨ï¼‰
        
        ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install langdetect
        
        Args:
            text: åˆ¤å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            è¨€èªã‚³ãƒ¼ãƒ‰
        """
        try:
            from langdetect import detect
            return detect(text[:500])
        except ImportError:
            self.logger.debug("langdetect not installed, using simple detection")
            return self._detect_language_simple(text)
        except Exception as e:
            self.logger.debug(f"Language detection failed: {e}, falling back to simple")
            return self._detect_language_simple(text)

    def multilingual_entity_linking(
        self,
        kg: nx.Graph,
        documents: List[Document],
        *,
        window_sentences: int = 1,
        batch_size: int = 32,
        same_lang_threshold: float = 0.90,
        cross_lang_threshold: float = 0.85,
        preserve_original_triples: bool = True,
        use_accurate_detection: bool = True
    ) -> Dict[str, Any]:
        """
        å¤šè¨€èªå¯¾å¿œã®Entity Linkingã‚’å®Ÿè¡Œ
        
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            documents: Documentã®ãƒªã‚¹ãƒˆ
            window_sentences: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã®å‰å¾Œæ–‡æ•°
            batch_size: åŸ‹ã‚è¾¼ã¿ã®ãƒãƒƒãƒã‚µã‚¤ã‚º
            same_lang_threshold: åŒè¨€èªæ¯”è¼ƒã®é–¾å€¤
            cross_lang_threshold: ç•°è¨€èªæ¯”è¼ƒã®é–¾å€¤
            preserve_original_triples: å…ƒã®ãƒˆãƒªãƒ—ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            use_accurate_detection: langdetectã‚’ä½¿ã†ã‹
        
        Returns:
            ã‚µãƒãƒªãƒ¼è¾æ›¸
        """
        self.logger.info("ğŸŒ Starting multilingual entity linking...")
        
        # 1) entity -> contexts ã‚’é›†ã‚ã‚‹
        entity_contexts = self._extract_entity_contexts(documents, window_sentences)
        
        # 2) å„ entity ã«ä»£è¡¨æ–‡è„ˆã‚’ä½œã‚‹
        entity_representations = {}
        for ent, ctxs in entity_contexts.items():
            rep = " ".join(ctxs[:2])  # æœ€å¤§2æ–‡ã‚’ã¤ãªã’ã‚‹
            entity_representations[ent] = rep
        
        # 3) åŸ‹ã‚è¾¼ã¿ã‚’ä½œã‚‹ï¼ˆãƒãƒƒãƒï¼‰
        ents = list(entity_representations.keys())
        reps = [entity_representations[e] for e in ents]
        
        self.logger.info(f"  Embedding {len(reps)} entity contexts (batch_size={batch_size})")
        emb_list = self._batch_embed_texts(reps, batch_size=batch_size)
        
        # 4) è¨€èªåˆ¤å®š
        detect_func = self._detect_language_accurate if use_accurate_detection else self._detect_language_simple
        lang_map = {ent: detect_func(entity_representations[ent]) for ent in ents}
        
        # 5) ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã¨ãƒãƒ¼ã‚¸
        def cosine(a: np.ndarray, b: np.ndarray) -> float:
            na = np.linalg.norm(a)
            nb = np.linalg.norm(b)
            if na < 1e-9 or nb < 1e-9:
                return 0.0
            return float(np.dot(a, b) / (na * nb))
        
        n = len(ents)
        merged_groups = []
        visited = set()
        
        for i in range(n):
            if ents[i] in visited:
                continue
            group = [ents[i]]
            visited.add(ents[i])
            
            for j in range(i + 1, n):
                if ents[j] in visited:
                    continue
                
                lang_i = lang_map[ents[i]]
                lang_j = lang_map[ents[j]]
                score = cosine(emb_list[i], emb_list[j])
                thresh = same_lang_threshold if lang_i == lang_j else cross_lang_threshold
                
                if score >= thresh:
                    group.append(ents[j])
                    visited.add(ents[j])
            
            merged_groups.append(group)
        
        # 6) canonical name ã‚’æ±ºå®š
        name_to_canonical = {}
        canonical_stats = []
        
        for group in merged_groups:
            if len(group) == 1:
                name_to_canonical[group[0]] = group[0]
                canonical_stats.append((group[0], 1))
                continue
            
            # å‡ºç¾é »åº¦ãƒ™ãƒ¼ã‚¹ã§ä»£è¡¨ã‚’é¸ã¶
            counts = {g: 0 for g in group}
            for doc in documents:
                txt = (getattr(doc, "text", "") or "").lower()
                for g in group:
                    if g.lower() in txt:
                        counts[g] += 1
            
            canonical = max(group, key=lambda x: (counts.get(x, 0), -len(x)))
            for g in group:
                name_to_canonical[g] = canonical
            canonical_stats.append((canonical, len(group)))
        
        # 7) NetworkX ãƒãƒ¼ãƒ‰ã®ãƒãƒ¼ã‚¸
        for old, canon in name_to_canonical.items():
            if old == canon:
                continue
            
            if not kg.has_node(canon):
                if kg.has_node(old):
                    kg.add_node(canon, **kg.nodes[old])
                else:
                    kg.add_node(canon)
            
            if kg.has_node(old):
                # ã‚¨ãƒƒã‚¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
                for u, v, data in list(kg.in_edges(old, data=True)):
                    if not kg.has_edge(u, canon):
                        kg.add_edge(u, canon, **data)
                    else:
                        kg[u][canon]['weight'] = max(
                            kg[u][canon].get('weight', 0.0), 
                            data.get('weight', 0.0)
                        )
                
                for u, v, data in list(kg.out_edges(old, data=True)):
                    if not kg.has_edge(canon, v):
                        kg.add_edge(canon, v, **data)
                    else:
                        kg[canon][v]['weight'] = max(
                            kg[canon][v].get('weight', 0.0), 
                            data.get('weight', 0.0)
                        )
                
                try:
                    kg.remove_node(old)
                except Exception:
                    pass
        
        # 8) Document metadata (triples) ã®æ›´æ–°
        for doc in documents:
            triples = doc.metadata.get("triples", [])
            new_triples = []
            for s, r, o in triples:
                s2 = name_to_canonical.get(s, s)
                o2 = name_to_canonical.get(o, o)
                new_triples.append((s2, r, o2))
            
            if preserve_original_triples:
                doc.metadata.setdefault("_original_triples", doc.metadata.get("triples", []).copy())
            doc.metadata["triples"] = new_triples
        
        summary = {
            "num_entities_before": n,
            "num_groups": len(merged_groups),
            "merged_count": sum(1 for g in merged_groups if len(g) > 1),
            "canonical_stats": canonical_stats[:10],  # æœ€åˆã®10å€‹ã ã‘
        }
        
        self.logger.info(
            f"âœ… Multilingual EL completed: "
            f"{summary['num_groups']} groups, "
            f"{summary['merged_count']} merges"
        )
        
        return summary