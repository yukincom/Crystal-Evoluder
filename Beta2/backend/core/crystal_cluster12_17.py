"""
ãƒ¡ã‚¤ãƒ³çµ±åˆã‚¯ãƒ©ã‚¹ - Crystal Cluster
"""
import numpy as np
from typing import List, Dict, Any, Optional, Tuple

from llama_index.core import Document
from llama_index.graph_stores.neo4j import Neo4jGraphStore

from config import load_config
from shared import setup_logger, HierarchicalLogger
from processors import DocumentProcessor, ChunkProcessor
from builders import GraphBuilder, RetrievalBuilder
from linkers import EntityLinker
from filters import TripletFilter
from rag import MultiHopExplorer


class CrystalCluster:
    """Crystal Cluster - Neo4jæŠ•å…¥å°‚ç”¨"""

    def __init__(self, log_level: int = 20, use_dual_chunk: bool = False, custom_config: dict = None):
        """
        Args:
            use_dual_chunk: Trueãªã‚‰ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        """
        self.logger = setup_logger('CrystalCluster', log_level)
        self.hlogger = HierarchicalLogger(self.logger)
        self.use_dual_chunk = use_dual_chunk

        default_config = {
            'entity_linking_threshold': 0.88,
            'retrieval_chunk_size': 320,
            'retrieval_chunk_overlap': 120,
            'graph_chunk_size': 512,
            'graph_chunk_overlap': 50,
            'relation_compat_threshold': 0.11,
            'final_weight_cutoff': 0.035,
            'max_triplets_per_chunk': 15,
            'llm_model': 'gpt-4o-mini',
            'llm_timeout': 120.0,

            # Self-RAGè¨­å®š
            'enable_self_rag': True,                    # Self-RAGã‚’æœ‰åŠ¹åŒ–
            'self_rag_confidence_threshold': 0.75,       # å†ç”Ÿæˆã®é–¾å€¤
            'self_rag_critic_model': 'gpt-4o-mini',     # è©•ä¾¡ç”¨LLM
            'self_rag_refiner_model': 'gpt-5-mini',         # å†ç”Ÿæˆç”¨LLMï¼ˆã‚ˆã‚Šé«˜æ€§èƒ½ï¼‰
            'self_rag_max_retries': 1,                  # æœ€å¤§å†è©¦è¡Œå›æ•°
            'self_rag_token_budget': 100000,            # ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—
            'self_rag_validation_checks': [             # æ¤œè¨¼é …ç›®
                'entity_quality',
                'relation_clarity',
                'grammar',
                'redundancy'
            ],
            # Multi-hopè¨­å®šï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            'multihop_beam_width': 2,                   # ãƒ“ãƒ¼ãƒ å¹…ã‚’ç‹­ã
            'multihop_max_paths': 50,                   # ãƒ‘ã‚¹æ•°ä¸Šé™ã‚’è¿½åŠ 
            # RAPLæœ€é©åŒ–è¨­å®š
            'rapl_max_entities': 100,           # Interè¨ˆç®—ã§å‡¦ç†ã™ã‚‹æœ€å¤§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°
            'rapl_min_shared_triples': 3,       # å…±æœ‰ãƒˆãƒªãƒ—ãƒ«æ•°ã®æœ€å°å€¤ï¼ˆ2â†’3ï¼‰
            'neo4j_batch_size': 1000,           # Neo4jãƒãƒƒãƒã‚µã‚¤ã‚º
            
        }
        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãŒã‚ã‚Œã°ä¸Šæ›¸ã
        if custom_config:
            default_config.update(custom_config)

        self.config = default_config
        self.config.setdefault('enable_triplet_filter', True)
        self.config.setdefault('triplet_quality_threshold', 0.3)


        # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        self.relation_blacklist = {
            'is', 'has', 'are', 'was', 'were',
            'the', 'a', 'an',
            'of', 'in', 'on', 'at',
        }

        from model import ensure_bge_m3
        self.embed_model = ensure_bge_m3()

        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.document_processor = DocumentProcessor(self.logger)
        self.chunk_processor = ChunkProcessor(self.config, self.logger)
        self.graph_builder = GraphBuilder(self.config, self.embed_model, self.logger)
        self.retrieval_builder = RetrievalBuilder(self.embed_model, self.logger)
        self.entity_linker = EntityLinker(self.config, self.logger)
        self.triplet_filter = TripletFilter(self.config, self.logger)
        self.multi_hop_explorer = MultiHopExplorer(self.config, self.logger)

        self.entity_emb_cache = {}          # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.relation_emb_cache = {}        # é–¢ä¿‚åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.total_self_rag_tokens = 0      # Self-RAGãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿
        self.visited_paths = set()          # Multi-hopãƒ‘ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        self.logger.info(f"Crystal Cluster beta initialized")
        self.logger.info(f"Self-RAG: {'enabled' if self.config['enable_self_rag'] else 'disabled'}")
        self.logger.info(f"Config: {self.config}")

    def get_cached_embedding(
        self,
        text: str,
        cache_type: str = 'entity'
    ) -> np.ndarray:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—

        Args:
            text: ãƒ†ã‚­ã‚¹ãƒˆ
            cache_type: 'entity' ã¾ãŸã¯ 'relation'

        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é¸æŠ
        cache = self.entity_emb_cache if cache_type == 'entity' else self.relation_emb_cache

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
        if text in cache:
            return cache[text]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: è¨ˆç®—ã—ã¦ä¿å­˜
        try:
            emb = self.embed_model.get_text_embedding(text)
            emb = np.array(emb, dtype=np.float32)

            # æ­£è¦åŒ–
            norm = np.linalg.norm(emb)
            if norm > 1e-9:
                emb = emb / norm
            else:
                emb = np.zeros_like(emb)

            cache[text] = emb

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Šã™ããŸã‚‰è­¦å‘Š
            if len(cache) % 1000 == 0:
                self.logger.debug(f"  {cache_type} cache size: {len(cache)}")

            return emb

        except Exception as e:
            self.logger.debug(f"Embedding failed for '{text[:30]}': {type(e).__name__}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(1024, dtype=np.float32)

    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg=None,
        neo4j_store=None,
        enable_duplicate_check: bool = True
    ) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†"""
        self.document_processor.neo4j_store = neo4j_store
        return self.document_processor.load_documents(
            json_path=json_path,
            raw_docs=raw_docs,
            path_pickle=path_pickle,
            kg=kg,
            enable_duplicate_check=enable_duplicate_check
        )



    def create_dual_documents(
        self,
        documents: List[Document]
    ) -> Tuple[List[Document], List[Document]]:
        """ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        return self.chunk_processor.create_dual_documents(documents)

    def build_retrieval_store(
        self,
        retrieval_docs: List[Document]
    ) -> Dict[str, Any]:
        """æ¤œç´¢ã‚¹ãƒˆã‚¢æ§‹ç¯‰"""
        return self.retrieval_builder.build_retrieval_store(retrieval_docs)

    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5,
        chunk_mapping: Dict = None
    ) -> List[Tuple[float, Document, List[str]]]:
        """æ¤œç´¢å®Ÿè¡Œ"""
        return self.retrieval_builder.retrieve(
            store=store,
            query=query,
            top_k=top_k,
            chunk_mapping=chunk_mapping
        )

    def commit_to_graph(self, documents: List[Document], graph_store: Neo4jGraphStore):
        """ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã¨Neo4jæŠ•å…¥"""
        self.graph_builder.commit_to_graph(documents, graph_store)

    def commit_to_graph_with_retrieval(
        self,
        documents: List[Document],
        graph_store: Neo4jGraphStore
    ) -> Dict[str, Any]:
        """
        Graph index ã¨ Retrieval store ã‚’åŒæ™‚ã«æ§‹ç¯‰ï¼ˆåŒæœŸç‰ˆï¼‰
        """
        with self.hlogger.section("Dual-Index Building (Synced)"):
            # 1. Dual-documentsç”Ÿæˆï¼ˆåŒæœŸãƒãƒƒãƒ”ãƒ³ã‚°ä»˜ãï¼‰
            graph_docs, retrieval_docs = self.create_dual_documents(documents)

            # ç°¡æ˜“çš„ãªchunk_mappingä½œæˆ
            chunk_mapping = {'graph_to_retrieval': {}, 'retrieval_to_graph': {}}

            self.logger.info(
                f"ğŸ”— Chunk mapping: "
                f"{len(chunk_mapping['graph_to_retrieval'])} graph -> retrieval links"
            )

            # 2. Graphæ§‹ç¯‰
            self.logger.info("ğŸ“Š Building knowledge graph...")
            self.commit_to_graph(graph_docs, graph_store)

            # 3. Retrieval storeæ§‹ç¯‰
            self.logger.info("ğŸ” Building retrieval store...")
            retrieval_store = self.build_retrieval_store(retrieval_docs)

            # chunk_mappingã‚’storeã«è¿½åŠ 
            retrieval_store['chunk_mapping'] = chunk_mapping

        return {
            'retrieval_store': retrieval_store,
            'chunk_mapping': chunk_mapping,
            'stats': {
                'graph_docs': len(graph_docs),
                'retrieval_docs': len(retrieval_docs),
                'sync_links': len(chunk_mapping['retrieval_to_graph'])
            }
        }

    def query_with_multihop(
        self,
        query: str,
        kg,
        retrieval_store: Dict = None,
        max_steps: int = 5,
        top_k_retrieval: int = 5,
        top_k_paths: int = 10
    ) -> Dict[str, Any]:
        """Multi-hopæ¢ç´¢ã‚’ä½¿ã£ãŸã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        return self.multi_hop_explorer.query_with_multihop(
            query=query,
            kg=kg,
            retrieval_store=retrieval_store,
            max_steps=max_steps,
            top_k_retrieval=top_k_retrieval,
            top_k_paths=top_k_paths
        )

    def filter_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        quality_threshold: float = 0.3
    ) -> Tuple[List[Tuple], List[Tuple], Dict]:
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        return self.triplet_filter.filter_triplets(triplets, quality_threshold)

    def link_entities(
        self,
        kg,
        similarity_threshold: float = 0.88,
        use_embedding: bool = True
    ) -> Tuple[Any, Dict[str, str]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±åˆ"""
        return self.entity_linker.link_entities(
            kg=kg,
            similarity_threshold=similarity_threshold,
            use_embedding=use_embedding
        )