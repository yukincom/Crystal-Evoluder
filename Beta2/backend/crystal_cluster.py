"""
Crystal Cluster beta
Knowledge Graph committer for Neo4j

"""

# ============================================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================
import json
import logging
import pickle
import numpy as np
import networkx as nx
import traceback
import hashlib
import re
import argparse

from difflib import SequenceMatcher
from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set

from llama_index.core import Settings
from llama_index.llms.openai import OpenAI  
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.core import Document, KnowledgeGraphIndex, StorageContext
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.core.node_parser import SentenceSplitter

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from shared.logger import setup_logger, HierarchicalLogger
from shared.utils import load_and_validate_paths
from shared.error_handler import ErrorCollector, safe_execute
from shared.duplicate_checker import ContentLevelDuplicateChecker


class CrystalCluster:
    """Crystal Cluster - Neo4jæŠ•å…¥å°‚ç”¨"""
    
    def __init__(self, log_level: int = logging.INFO, use_dual_chunk: bool = False, custom_config: dict = None):
        """
        Args:
            use_dual_chunk: Trueãªã‚‰ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        """
        self.logger = setup_logger('CrystalCluster', log_level)
        self.hlogger = HierarchicalLogger(self.logger)
        self.use_dual_chunk = use_dual_chunk    

        default_config = {
            'entity_linking_threshold': 0.88,
            'retrieval_chunk_size': 320,
            'retrieval_chunk_overlap': 120,
            'graph_chunk_size': 512,
            'graph_chunk_overlap': 50,
            'relation_compat_threshold': 0.11,
            'final_weight_cutoff': 0.035,
            'max_triplets_per_chunk': 15,
            'llm_model': 'gpt-4o-mini',
            'llm_timeout': 120.0,
        
        # Self-RAGè¨­å®š
            'enable_self_rag': True,                    # Self-RAGã‚’æœ‰åŠ¹åŒ–
            'self_rag_confidence_threshold': 0.75,       # å†ç”Ÿæˆã®é–¾å€¤
            'self_rag_critic_model': 'gpt-4o-mini',     # è©•ä¾¡ç”¨LLM
            'self_rag_refiner_model': 'gpt-4o',         # å†ç”Ÿæˆç”¨LLMï¼ˆã‚ˆã‚Šé«˜æ€§èƒ½ï¼‰
            'self_rag_max_retries': 1,                  # æœ€å¤§å†è©¦è¡Œå›æ•°
            'self_rag_token_budget': 100000,            # ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—
            'self_rag_validation_checks': [             # æ¤œè¨¼é …ç›®
                'entity_quality',
                'relation_clarity',
                'grammar',
                'redundancy'
            ],
                   # Multi-hopè¨­å®šï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            'multihop_beam_width': 2,                   # ãƒ“ãƒ¼ãƒ å¹…ã‚’ç‹­ã
            'multihop_max_paths': 50,                   # ãƒ‘ã‚¹æ•°ä¸Šé™ã‚’è¿½åŠ  
            # RAPLæœ€é©åŒ–è¨­å®š
            'rapl_max_entities': 100,           # Interè¨ˆç®—ã§å‡¦ç†ã™ã‚‹æœ€å¤§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°
            'rapl_min_shared_triples': 3,       # å…±æœ‰ãƒˆãƒªãƒ—ãƒ«æ•°ã®æœ€å°å€¤ï¼ˆ2â†’3ï¼‰
            'neo4j_batch_size': 1000,           # Neo4jãƒãƒƒãƒã‚µã‚¤ã‚º
        }

        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãŒã‚ã‚Œã°ä¸Šæ›¸ã
        if custom_config:
            default_config.update(custom_config)
        
        self.config = default_config
        self.config.setdefault('enable_triplet_filter', True)
        self.config.setdefault('triplet_quality_threshold', 0.3)

    # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        self.relation_blacklist = {
            'is', 'has', 'are', 'was', 'were',
            'the', 'a', 'an',
            'of', 'in', 'on', 'at',
        }        
        
        self.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
            device="mps",
            embed_batch_size=16,
        )
        
        Settings.embed_model = self.embed_model

        self.entity_emb_cache = {}          # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.relation_emb_cache = {}        # é–¢ä¿‚åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.total_self_rag_tokens = 0      # Self-RAGãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿
        self.visited_paths = set()          # Multi-hopãƒ‘ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥    

        self.logger.info(f"Crystal Cluster beta initialized")
        self.logger.info(f"Self-RAG: {'enabled' if self.config['enable_self_rag'] else 'disabled'}")
        self.logger.info(f"Config: {self.config}")

    def get_cached_embedding(
        self,
        text: str,
        cache_type: str = 'entity'
    ) -> np.ndarray:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
    
        Args:
            text: ãƒ†ã‚­ã‚¹ãƒˆ
            cache_type: 'entity' ã¾ãŸã¯ 'relation'
    
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é¸æŠ
        cache = self.entity_emb_cache if cache_type == 'entity' else self.relation_emb_cache
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
        if text in cache:
            return cache[text]
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: è¨ˆç®—ã—ã¦ä¿å­˜
        try:
            emb = self.embed_model.get_text_embedding(text)
            emb = np.array(emb, dtype=np.float32)
        
        # æ­£è¦åŒ–
            norm = np.linalg.norm(emb)
            if norm > 1e-9:
                emb = emb / norm
            else:
                emb = np.zeros_like(emb)
        
            cache[text] = emb
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Šã™ããŸã‚‰è­¦å‘Š
            if len(cache) % 1000 == 0:
                self.logger.debug(f"  {cache_type} cache size: {len(cache)}")
        
            return emb
    
        except Exception as e:
            self.logger.debug(f"Embedding failed for '{text[:30]}': {type(e).__name__}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(1024, dtype=np.float32)

    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg: Optional[nx.Graph] = None) -> List[Document]:
        enable_duplicate_check: bool = True  # â† è¿½åŠ     
        """
        JSON ã¨ ç”Ÿãƒ†ã‚­ã‚¹ãƒˆä¸¡æ–¹ã‹ã‚‰ Document ã‚’ä½œã‚‹
        
        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            raw_docs: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            path_pickle: ãƒ‘ã‚¹æƒ…å ±ã®Pickleãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ‘ã‚¹æƒ…å ±çµ±åˆæ™‚ã«å¿…è¦ï¼‰
        
        Returns:
            Documentã®ãƒªã‚¹ãƒˆï¼ˆãƒ‘ã‚¹æƒ…å ±ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼‰
        """

    
        if enable_duplicate_check:
        
            content_checker = ContentLevelDuplicateChecker(
                similarity_threshold=0.85,
                neo4j_store=getattr(self, 'graph_store', None),
                logger=self.logger
            )
        
            self.logger.info("ğŸ” Checking for content duplicates...")
            
        documents = []

        # --- JSON å´ ---
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # json_path ã® documents ã‚’è¿½åŠ 
        for i, doc in enumerate(data.get('documents', [])):
            doc_text = doc['text']
            if enable_duplicate_check:
                is_duplicate, similar_docs = content_checker.check_duplicate(
                    doc_text,
                    check_fuzzy=True,
                    check_neo4j=True
                )
                if is_duplicate:
                    self.logger.info(
                        f"  âŠ— Skipping duplicate document {i} "
                        f"(similar to: {similar_docs[0].get('doc_id')})"
                    )
                    continue  # é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                # é‡è¤‡ã§ãªã„å ´åˆã¯ç™»éŒ²
                content_checker.add_content(
                    text=doc_text,
                    doc_id=f"json_{i}",
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    },
                    save_to_neo4j=True,
                    store_full_text=False  # å…¨æ–‡ã¯ä¿å­˜ã—ãªã„ï¼ˆã‚µã‚¤ã‚ºå‰Šæ¸›ï¼‰
                )

            documents.append(
                Document(
                    text=doc['text'],
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    }
                )
            )

        # --- ç”Ÿãƒ†ã‚­ã‚¹ãƒˆå´ ---
        if raw_docs:
            for i, text in enumerate(raw_docs):
                if enable_duplicate_check:
                    is_duplicate, similar_docs = content_checker.check_duplicate(
                        text,
                        check_fuzzy=True,
                        check_neo4j=True
                    )
                
                    if is_duplicate:
                        self.logger.info(
                            f"  âŠ— Skipping duplicate raw document {i}"
                        )
                        continue
                
                    content_checker.add_content(
                        text=text,
                        doc_id=f"raw_{i}",
                        metadata={"source": "raw", "raw_id": i},
                        save_to_neo4j=True,
                        store_full_text=False
                    )                

                documents.append(
                    Document(
                        text=text,
                        metadata={
                            "source": "raw",
                            "raw_id": i
                        }
                    )
                )

        json_count = len(data.get('documents', []))
        raw_count = len(raw_docs) if raw_docs else 0
        
        self.logger.info(
            f"ğŸ“‚ Loaded {len(documents)} documents "
            f"({json_count} from JSON, {raw_count} raw texts)"
        )

        # --- ãƒ‘ã‚¹æƒ…å ±ã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰---
        if path_pickle and kg is not None:
            path_dicts = load_and_validate_paths(path_pickle, self.logger)
            if path_dicts:
                self.logger.info("Augmenting documents with path information...")
                documents = self.augment_documents_with_paths(
                    documents, 
                    path_dicts, 
                    kg,
                    entity_embeddings=getattr(self, 'entity_embeddings', None)
                )
                self.logger.info(f"âœ… Path information added to {len(documents)} documents")
            else:
                self.logger.warning("Path information could not be loaded, continuing without it")

        return documents

    def augment_documents_with_paths(
        self,
        documents: List[Document], 
        path_dicts: List[Dict], 
        kg: nx.Graph,
        entity_embeddings: Dict[str, np.ndarray] = None,
        match_key='question') -> List[Document]:
        """
        documents ã«å¯¾å¿œã™ã‚‹ path æƒ…å ±ã‚’æ³¨å…¥
        
        Args:
            documents: Documentã®ãƒªã‚¹ãƒˆ
            path_dicts: load_path_dicts ã®æˆ»ã‚Šå€¤
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            match_key: documents ã¨ path_dicts ã‚’çªãåˆã‚ã›ã‚‹ã‚­ãƒ¼
        
        Returns:
            ãƒ‘ã‚¹æƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸ documents

        """
        # defensive
        if entity_embeddings is None:
            entity_embeddings = {}
        
        # path_dictsãŒç©ºãªã‚‰å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        if len(path_dicts) == 0:
            self.logger.info("  â†’ No path information available, returning original documents")
            return documents

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼š path_dicts ã‚’ match_key ã§å¼•ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹
        pd_map = {}
        for p in path_dicts:
            key = p.get(match_key)
            if key is not None:
                pd_map[key] = p

        augmented = []
        matched_count = 0

        for doc in documents:
            meta = dict(getattr(doc, 'metadata', {}) or {})
            doc_key = meta.get(match_key)

            matched = None
            if doc_key is not None and doc_key in pd_map:
                matched = pd_map[doc_key]
                matched_count += 1
            else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆå†…ã« match_key ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ path_dict ã‚’æ¢ã™
                text = getattr(doc, 'text', '') or ''
                for k, p in pd_map.items():
                    if isinstance(k, str) and k in text:
                        matched = p
                        matched_count += 1
                        break

            paths_meta = []
            if matched:
                for path in matched.get('translated_paths', []):
                # path: list of node names (entities)
                    path_len = len(path)
                    edge_weights = []
                    path_node_pairs = list(zip(path[:-1], path[1:])) if path_len >= 2 else []
                    for u, v in path_node_pairs:
                        if kg.has_edge(u, v):
                            edge_weights.append(kg[u][v].get('weight', 0.0))
                        elif kg.has_edge(v, u):
                            edge_weights.append(kg[v][u].get('weight', 0.0))
                        else:
                        # edge ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 0.0 ã‚’å…¥ã‚Œã¦ãŠã
                            edge_weights.append(0.0)

                    avg_edge_weight = float(np.mean(edge_weights)) if edge_weights else 0.0
                    sum_edge_weight = float(np.sum(edge_weights)) if edge_weights else 0.0

                # path å†…ãƒãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°ã€ãƒãƒ¼ãƒ‰é–“é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆå¹³å‡ãƒšã‚¢é¡ä¼¼åº¦ï¼‰
                    pair_sims = []
                    for i in range(len(path) - 1):
                        e1 = entity_embeddings.get(path[i])
                        e2 = entity_embeddings.get(path[i + 1])
                        if e1 is not None and e2 is not None:
                        # safe numpy dot / norms
                            denom = (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-9)
                            pair_sims.append(float(np.dot(e1, e2) / denom))
                    avg_pair_sim = float(np.mean(pair_sims)) if pair_sims else None

                # æœ€çŸ­è·é›¢ï¼ˆkg ä¸Šï¼‰ â€” å­˜åœ¨ã—ãªã‘ã‚Œã° None
                    shortest = None
                    try:
                        if path_len >= 2:
                        # path ã®ç«¯åŒå£«ã®æœ€çŸ­é•·ã‚’è¨ˆç®—ï¼ˆä¾‹ï¼‰
                            s1, s2 = path[0], path[-1]
                            if kg.has_node(s1) and kg.has_node(s2):
                                shortest = int(nx.shortest_path_length(kg, s1, s2))
                    except (nx.NetworkXNoPath, nx.NodeNotFound):
                        pass
                    except Exception:
                        pass

                    paths_meta.append({
                        'path': path,
                        'path_length_nodes': path_len,
                        'avg_edge_weight': avg_edge_weight,
                        'sum_edge_weight': sum_edge_weight,
                        'avg_adjacent_node_sim': avg_pair_sim, 
                        'kg_shortest_between_ends': shortest
                    })

        # attach (æ—¢å­˜ metadata ã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼)
                new_meta = dict(meta)
                new_meta['paths'] = paths_meta
            # create a new Document preserving original text & adding metadata (or mutate in place if ok)
                new_doc = Document(text=getattr(doc, 'text', ''), metadata=new_meta)
                augmented.append(new_doc)
            else:
            # ãƒãƒƒãƒã—ãªã‹ã£ãŸå ´åˆã‚‚å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿æŒ
                augmented.append(doc)
    
            self.logger.info(f"  â†’ Matched {matched_count}/{len(documents)} documents with path information")
                  
            return augmented

        # naive match: by ordering if no explicit key available
        if len(path_dicts) == 0:
            return documents

        if len(path_dicts) == len(documents):
            for i, doc in enumerate(documents):
                doc.metadata['paths'] = path_dicts[i].get('translated_paths', [])
                doc.metadata['path_distances'] = path_dicts[i].get('path_distances', [])
        else:
            # fallback: attach top global paths to every doc (still useful)
            sample_paths = path_dicts[0].get('translated_paths', [])
            for doc in documents:
                doc.metadata.setdefault('paths', sample_paths)
                doc.metadata.setdefault('path_distances', path_dicts[0].get('path_distances', []))
        return documents
    
    def _generate_chunk_id(self, text: str, source_id: str, index: int) -> str:
        """
        ãƒãƒ£ãƒ³ã‚¯ã®ä¸€æ„ãªIDã‚’ç”Ÿæˆ
        
        Args:
            text: ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            source_id: å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ID
            index: ãƒãƒ£ãƒ³ã‚¯ç•ªå·
        
        Returns:
            'doc123_chunk5_a7f3e9b2' ã®ã‚ˆã†ãªä¸€æ„ID
        """
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ï¼ˆæœ€åˆã®100æ–‡å­—ã‹ã‚‰ï¼‰
        text_hash = hashlib.md5(text[:100].encode()).hexdigest()[:8]
        return f"{source_id}_chunk{index}_{text_hash}"
    
    # ============================================================
    # Dual-documents ç”Ÿæˆ
    # ============================================================
    def create_dual_documents(
        self,
        documents: List[Document]) -> Tuple[List[Document], List[Document]]:
        """
        æ—¢å­˜ã®Documentã‹ã‚‰ Graphç”¨ ã¨ Retrievalç”¨ ã®2ç¨®é¡ã‚’ä½œã‚‹
        
        Args:
            documents: load_documents() ã§ä½œæˆã—ãŸDocumentãƒªã‚¹ãƒˆ
        
        Returns:
            (graph_docs, retrieval_docs)
        """
        if not self.use_dual_chunk:
            # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ç„¡åŠ¹æ™‚ã¯å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
            return documents, documents
        
        graph_splitter, retrieval_splitter = self.get_dual_splitters()
        graph_docs = []
        retrieval_docs = []
        
        for doc in documents:
            base_meta = dict(doc.metadata)
            
            # ------------------------------------------------------------
            # Graphç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå°ã•ã‚ï¼‰
            # ------------------------------------------------------------
            try:
                graph_nodes = graph_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(graph_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'structural',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    graph_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Graph splitting failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã†
                md = dict(base_meta)
                md['chunk_type'] = 'structural'
                graph_docs.append(Document(text=doc.text, metadata=md))
            
            # ------------------------------------------------------------
            # Retrievalç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå¤§ãã‚ï¼‰
            # ------------------------------------------------------------
            try:
                retrieval_nodes = retrieval_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(retrieval_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'semantic',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    retrieval_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Retrieval splitting failed: {e}")
                md = dict(base_meta)
                md['chunk_type'] = 'semantic'
                retrieval_docs.append(Document(text=doc.text, metadata=md))
        
        self.logger.info(
            f"ğŸ“„ Created {len(graph_docs)} graph chunks, "
            f"{len(retrieval_docs)} retrieval chunks"
        )
        
        return graph_docs, retrieval_docs

    def _find_overlapping_chunks(
        self,
        start: int,
        end: int,
        graph_docs: List[Document]
    ) -> List[str]:
        """
        æŒ‡å®šç¯„å›²ã¨é‡ãªã‚‹Graphãƒãƒ£ãƒ³ã‚¯ã®IDã‚’è¿”ã™
        
        Args:
            start, end: æ–‡å­—ä½ç½®
            graph_docs: åŒä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®Graphãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        
        Returns:
            é‡ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã®IDãƒªã‚¹ãƒˆ
        """
        overlapping = []
        
        for doc in graph_docs:
            g_start = doc.metadata.get('start_char', 0)
            g_end = doc.metadata.get('end_char', 0)
            
            # ç¯„å›²ã®é‡ãªã‚Šãƒã‚§ãƒƒã‚¯
            if not (end <= g_start or start >= g_end):
                overlapping.append(doc.metadata['chunk_id'])
        
        return overlapping
    

    # ============================================================
    # ä¿®æ­£1: get_dual_splittersï¼ˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª¿æ•´ï¼‰
    # ============================================================
    
    def get_dual_splitters(self) -> Tuple[SentenceSplitter, SentenceSplitter]:
        """
        Graphç”¨ã¨Retrievalç”¨ã®2ç³»çµ±ã‚’è¿”ã™ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç‰ˆï¼‰
        """
        # Graphç”¨ï¼šå°ã•ã‚ãƒãƒ£ãƒ³ã‚¯
        graph_splitter = SentenceSplitter(
            chunk_size=self.config['graph_chunk_size'],
            chunk_overlap=self.config['graph_chunk_overlap'],
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )
        
        # Retrievalç”¨ï¼šä¸­ã‚µã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯ï¼ˆ512ã«å¤‰æ›´ï¼‰
        retrieval_splitter = SentenceSplitter(
            chunk_size=self.config['retrieval_chunk_size'],  # 512
            chunk_overlap=self.config['retrieval_chunk_overlap'],  # 100
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )
        
        self.logger.info(
            f"Splitters: graph={self.config['graph_chunk_size']}, "
            f"retrieval={self.config['retrieval_chunk_size']}"
        )
        
        return graph_splitter, retrieval_splitter

    # ============================================================
    # Retrieval Store æ§‹ç¯‰
    # ============================================================
    def build_retrieval_store(
        self,
        retrieval_docs: List[Document]
    ) -> Dict[str, Any]:
        """
        Retrievalç”¨ã®åŸ‹ã‚è¾¼ã¿ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰
        
        Returns:
            {
                'docs': [Document, ...],
                'embeddings': np.array,
                'metadata': {...}
            }
        """
        self.logger.info("ğŸ” Building retrieval embeddings...")
        
        docs = []
        embeddings = []
        
        collector = ErrorCollector(self.logger)
        
        for doc in retrieval_docs:
            try:
                emb = self.embed_model.get_text_embedding(doc.text)
                emb = np.array(emb, dtype=np.float32)
                
                # æ­£è¦åŒ–
                norm = np.linalg.norm(emb)
                if norm > 1e-9:
                    emb = emb / norm
                else:
                    self.logger.debug("Zero-norm embedding, skipping")
                    continue
                
                docs.append(doc)
                embeddings.append(emb)
                collector.add_success()
            
            except Exception as e:
                collector.add_error(
                    context=f"doc_{doc.metadata.get('source_id', 'unknown')}",
                    error=e
                )
        
        collector.report("Retrieval embedding generation", threshold=0.3)
        
        embeddings = np.vstack(embeddings) if embeddings else np.zeros((0, 1024))
        
        self.logger.info(f"âœ… Built retrieval store: {len(docs)} docs")
        
        return {
            'docs': docs,
            'embeddings': embeddings,
            'metadata': {
                'total_docs': len(docs),
                'embedding_dim': embeddings.shape[1] if len(embeddings) > 0 else 0
            }
        }
    
    def retrieve(
        self,
        store: Dict,
        query: str,
        top_k: int = 5,
        chunk_mapping: Dict = None
    ) -> List[Tuple[float, Document, List[str]]]:
        """
        ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¤œç´¢
        
        Returns:
            [(score, Document, graph_chunk_ids), ...] ã®ãƒªã‚¹ãƒˆ
        """
        if len(store['docs']) == 0:
            self.logger.warning("âš ï¸  Retrieval store is empty")
            return []
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        qemb = np.array(self.embed_model.get_text_embedding(query))
        qnorm = np.linalg.norm(qemb)
        if qnorm > 1e-9:
            qemb = qemb / qnorm
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        sims = store['embeddings'] @ qemb
        top_indices = np.argsort(-sims)[:top_k]
        
        results = []
        for i in top_indices:
            if i >= len(store['docs']):
                continue
            
            doc = store['docs'][i]
            score = float(sims[i])
            
            # Graph chunk IDsã‚’å–å¾—
            graph_chunk_ids = doc.metadata.get('graph_chunk_ids', [])
            
            # ã¾ãŸã¯ã€chunk_mappingã‹ã‚‰é€†å¼•ã
            if not graph_chunk_ids and chunk_mapping:
                chunk_id = doc.metadata.get('chunk_id')
                graph_chunk_ids = chunk_mapping.get('retrieval_to_graph', {}).get(chunk_id, [])
            
            # 3. ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ç„¡åŠ¹æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not graph_chunk_ids:
            # åŒä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’æŒã¤ãƒãƒ£ãƒ³ã‚¯ã‚’æ¨å®š
                source_id = doc.metadata.get('source_id') or doc.metadata.get('json_id') or doc.metadata.get('raw_id')
                if source_id is not None:
                    # ç°¡æ˜“çš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’æŒ‡ã™IDã‚’ç”Ÿæˆ
                    graph_chunk_ids = [f"doc_{source_id}_all"]
                
                # è­¦å‘Šã‚’1å›ã ã‘å‡ºã™ï¼ˆåˆå›ã®ã¿ï¼‰
                    if not hasattr(self, '_warned_no_mapping'):
                        self.logger.warning(
                           "âš ï¸  chunk_mapping not available, using fallback document IDs. "
                            "Enable dual-chunk mode for better precision."
                        )
                        self._warned_no_mapping = True
        
                results.append((score, doc, graph_chunk_ids))
    
            return results
        
    def explore_multi_hop_paths(
        self,
        kg: nx.Graph,
        query: str,
        retrieval_chunks: List[str] = None,
        max_steps: int = 5,
        top_k_per_hop: int = 3,
        confidence_threshold: float = 0.7,
        extend_on_low_confidence: bool = True
    ) -> Dict[str, Any]:
        """
        Multi-hopæ¢ç´¢ã‚’å®Ÿè¡Œ
    
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            retrieval_chunks: é–‹å§‹ç‚¹ã¨ãªã‚‹ãƒãƒ£ãƒ³ã‚¯IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_steps: æœ€å¤§ãƒ›ãƒƒãƒ—æ•°
            top_k_per_hop: å„ãƒ›ãƒƒãƒ—ã§æ¢ç´¢ã™ã‚‹ä¸Šä½Kå€‹
            confidence_threshold: ä¿¡é ¼åº¦ã®é–¾å€¤
            extend_on_low_confidence: ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã«æ¢ç´¢ã‚’æ‹¡å¼µã™ã‚‹ã‹
    
        Returns:
            {
                'paths': ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚ŒãŸãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ,
                'entities': è¨ªå•ã—ãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£,
                'confidence': ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢,
                'steps_used': å®Ÿéš›ã«ä½¿ç”¨ã—ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°,
                'evidence': ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
            }
        """
        self.logger.info(f"ğŸ” Starting multi-hop exploration (max_steps={max_steps})")
    
    # ============================================================
    # 1. é–‹å§‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ±ºå®š
    # ============================================================
        start_entities = set()
    
        if retrieval_chunks:
        # Retrievalã§å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é–‹å§‹
            start_entities = self._resolve_entities_from_chunks(retrieval_chunks, kg)
    
        if not start_entities:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
            start_entities = self._extract_query_entities(query, kg, top_k=5)
    
        if not start_entities:
            self.logger.warning("âš ï¸  No starting entities found")
            return {
                'paths': [],
                'entities': [],
                'confidence': 0.0,
                'steps_used': 0,
                'evidence': []
            }
    
        self.logger.info(f"  â†’ Starting from {len(start_entities)} entities: {list(start_entities)[:3]}...")
    
    # ============================================================
    # 2. å„é–‹å§‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æ¢ç´¢
    # ============================================================
        all_paths = []
        visited_entities = set()
        evidence_texts = []
    
        for start_entity in list(start_entities)[:top_k_per_hop]:
            if start_entity not in kg.nodes():
                self.logger.debug(f"  Entity '{start_entity}' not in graph, skipping")
                continue
        
            path_result = self._explore_from_entity(
                kg,
                start_entity,
                query,
                max_steps=max_steps,
                visited=set()
            )
        
            all_paths.extend(path_result['paths'])
            visited_entities.update(path_result['visited'])
        
            # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
            for path_info in path_result['paths'][:5]:  # Top 5ã®ã¿
                path = path_info['path']
                evidence_texts.append(' â†’ '.join(path))
    
        # å…¨ä½“ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—
        if all_paths:
            confidence = np.mean([p['score'] for p in all_paths])
        else:
            confidence = 0.0
    
        current_step = max_steps
    
        self.logger.info(
            f"  â†’ Found {len(all_paths)} paths with confidence {confidence:.2f}"
        )    

        # ============================================================
        # 3. ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯æ‹¡å¼µ
        # ============================================================
        if extend_on_low_confidence and confidence < confidence_threshold:
            extended_steps = max_steps + 2
            self.logger.info(
                f"  â†’ Low confidence ({confidence:.2f} < {confidence_threshold}), "
                f"extending to {extended_steps} steps"
            )
            
            # å†æ¢ç´¢
            extended_paths = []
            for start_entity in list(start_entities)[:top_k_per_hop]:
                if start_entity not in kg.nodes():
                    continue
                
                path_result = self._explore_from_entity(
                    kg,
                    start_entity,
                    query,
                    max_steps=extended_steps,
                    visited=set()  # ãƒªã‚»ãƒƒãƒˆ
                )
                
                extended_paths.extend(path_result['paths'])
                confidence = max(confidence, path_result['confidence'])
            
            if len(extended_paths) > len(paths):
                paths = extended_paths
                current_step = extended_steps
                self.logger.info(f"  â†’ Extended search found {len(paths)} paths")
        
        # ============================================================
        # 4. ãƒ‘ã‚¹ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        # ============================================================
        ranked_paths = self._rank_paths(paths, query, kg)
        
        return {
            'paths': ranked_paths[:10],  # Top 10
            'entities': list(visited_entities),
            'confidence': confidence,
            'steps_used': current_step,
            'evidence': evidence_texts
        }
    
    def _precompute_representative_paths(
        self,
        kg: nx.Graph,
        documents: List[Document],
        num_sample_queries: int = 10
    ) -> None:
        """
        ä»£è¡¨çš„ãªã‚¯ã‚¨ãƒªã§ãƒ‘ã‚¹ã‚’äº‹å‰è¨ˆç®—ã—ã€ã‚°ãƒ©ãƒ•ã«ä¿å­˜
    
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            num_sample_queries: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªæ•°
        """
        self.logger.info(f"Computing representative paths for {num_sample_queries} sample queries...")
    
    # ============================================================
    # 1. ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ
    # ============================================================
        sample_queries = self._generate_sample_queries(documents, kg, num_sample_queries)
    
        if not sample_queries:
            self.logger.warning("  â†’ No sample queries generated, skipping path pre-computation")
            return
    
        self.logger.info(f"  Generated {len(sample_queries)} sample queries")
    
    # ============================================================
    # 2. å„ã‚¯ã‚¨ãƒªã§Multi-hopæ¢ç´¢ã‚’å®Ÿè¡Œ
    # ============================================================
        all_paths = []
        path_count = 0
    
        for i, query in enumerate(sample_queries):
            try:
                result = self.explore_multi_hop_paths(
                    kg=kg,
                    query=query,
                    max_steps=5,
                    top_k_per_hop=3,
                    extend_on_low_confidence=False  # äº‹å‰è¨ˆç®—ã§ã¯æ‹¡å¼µã—ãªã„
                )
            
            # é«˜å“è³ªãªãƒ‘ã‚¹ã®ã¿ä¿å­˜ï¼ˆconfidence > 0.5ï¼‰
                for path_info in result['paths']:
                    if path_info.get('final_score', 0) > 0.5:
                        all_paths.append(path_info)
                        path_count += 1
            
                if (i + 1) % 5 == 0:
                    self.logger.info(f"  Processed {i+1}/{len(sample_queries)} queries...")
        
            except Exception as e:
                self.logger.debug(f"  Query '{query[:30]}...' failed: {type(e).__name__}")
                continue
    
        self.logger.info(f"  â†’ Computed {path_count} high-quality paths")
    
    # ============================================================
    # 3. ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸ã«ä¿å­˜
    # ============================================================
        self._store_paths_in_graph(kg, all_paths)

    def _generate_sample_queries(
        self,
        documents: List[Document],
        kg: nx.Graph,
        num_queries: int = 10
    ) -> List[str]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ä»£è¡¨çš„ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
    
        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•
            num_queries: ç”Ÿæˆã™ã‚‹ã‚¯ã‚¨ãƒªæ•°
    
        Returns:
            ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
        """
        queries = []
    
    # ============================================================
    # æˆ¦ç•¥1: ä¸­å¿ƒæ€§ã®é«˜ã„ãƒãƒ¼ãƒ‰ã‚’ã‚¯ã‚¨ãƒªã«ã™ã‚‹
    # ============================================================
        try:
        # æ¬¡æ•°ä¸­å¿ƒæ€§ã‚’è¨ˆç®—
            degree_centrality = nx.degree_centrality(kg)
        
        # ä¸Šä½ãƒãƒ¼ãƒ‰ã‚’å–å¾—
            top_nodes = sorted(
                degree_centrality.items(),
                key=lambda x: x[1],
                reverse=True
            )[:num_queries // 2]
        
        # ãƒãƒ¼ãƒ‰åã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨
            for node, _ in top_nodes:
                queries.append(f"What is {node}?")
                queries.append(f"How does {node} work?")
    
        except Exception as e:
            self.logger.debug(f"Centrality-based query generation failed: {e}")
    
    # ============================================================
    # æˆ¦ç•¥2: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
    # ============================================================
        for doc in documents[:num_queries // 2]:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«'question'ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
            question = doc.metadata.get('question')
            if question:
                queries.append(question)
            else:
            # ãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®æ–‡ã‚’ä½¿ç”¨
                text = doc.text.strip()
                if text:
                    first_sentence = text.split('.')[0][:100]
                    if len(first_sentence) > 10:
                        queries.append(first_sentence)
    
    # ============================================================
    # æˆ¦ç•¥3: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢ã®é–¢ä¿‚ã‚’å•ã†ã‚¯ã‚¨ãƒª
    # ============================================================
        try:
        # é‡ã¿ã®é«˜ã„ã‚¨ãƒƒã‚¸ã‚’å–å¾—
            high_weight_edges = sorted(
                kg.edges(data=True),
                key=lambda x: x[2].get('weight', 0),
                reverse=True
            )[:num_queries // 3]
        
            for u, v, data in high_weight_edges:
                relation = data.get('relation', 'related to')
                queries.append(f"How is {u} {relation} {v}?")
    
        except Exception as e:
            self.logger.debug(f"Edge-based query generation failed: {e}")
    
    # é‡è¤‡ã‚’é™¤å»ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        queries = list(set(queries))
        import random
        random.shuffle(queries)
    
        return queries[:num_queries]
    
    def _store_paths_in_graph(
        self,
        kg: nx.Graph,
        paths: List[Dict]
    ) -> None:
        """
        è¨ˆç®—ã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸å±æ€§ã«ä¿å­˜
    
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            paths: ãƒ‘ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        self.logger.info("  Storing path information in graph...")
    
    # ============================================================
    # 1. å„ãƒãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ãƒ‘ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    # ============================================================
        node_path_counts = defaultdict(int)
        node_avg_scores = defaultdict(list)
    
        for path_info in paths:
            path = path_info.get('path', [])
            score = path_info.get('final_score', 0)
        
            for node in path:
                if kg.has_node(node):
                    node_path_counts[node] += 1
                    node_avg_scores[node].append(score)
    
    # ãƒãƒ¼ãƒ‰ã«å±æ€§ã‚’è¿½åŠ 
        for node in kg.nodes():
            kg.nodes[node]['path_frequency'] = node_path_counts.get(node, 0)
        
            scores = node_avg_scores.get(node, [])
            if scores:
                kg.nodes[node]['avg_path_score'] = float(np.mean(scores))
            else:
                kg.nodes[node]['avg_path_score'] = 0.0
    
    # ============================================================
    # 2. å„ã‚¨ãƒƒã‚¸ãŒå«ã¾ã‚Œã‚‹ãƒ‘ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    # ============================================================
        edge_path_counts = defaultdict(int)
        edge_avg_scores = defaultdict(list)
    
        for path_info in paths:
            path = path_info.get('path', [])
            score = path_info.get('final_score', 0)
        
        # ãƒ‘ã‚¹å†…ã®é€£ç¶šã™ã‚‹ãƒãƒ¼ãƒ‰ãƒšã‚¢ã‚’ã‚¨ãƒƒã‚¸ã¨ã—ã¦è¨˜éŒ²
            for i in range(len(path) - 1):
                u, v = path[i], path[i + 1]
            
            # ç„¡å‘ã‚°ãƒ©ãƒ•ã¨ã—ã¦æ‰±ã†
                edge_key = tuple(sorted([u, v]))
                edge_path_counts[edge_key] += 1
                edge_avg_scores[edge_key].append(score)
    
    # ã‚¨ãƒƒã‚¸ã«å±æ€§ã‚’è¿½åŠ 
        for u, v in kg.edges():
            edge_key = tuple(sorted([u, v]))
        
            kg[u][v]['path_frequency'] = edge_path_counts.get(edge_key, 0)
        
            scores = edge_avg_scores.get(edge_key, [])
            if scores:
                kg[u][v]['avg_path_score'] = float(np.mean(scores))
            else:
                kg[u][v]['avg_path_score'] = 0.0
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        total_nodes_with_paths = sum(1 for n in kg.nodes() if kg.nodes[n]['path_frequency'] > 0)
        total_edges_with_paths = sum(1 for u, v in kg.edges() if kg[u][v]['path_frequency'] > 0)
    
        self.logger.info(
            f"  â†’ {total_nodes_with_paths}/{len(kg.nodes())} nodes and "
            f"{total_edges_with_paths}/{len(kg.edges())} edges have path information"
        )

    def _extract_query_entities(
        self,
        query: str,
        kg: nx.Graph,
        top_k: int = 5
    ) -> Set[str]:
        """
        ã‚¯ã‚¨ãƒªã‹ã‚‰é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
    
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            kg: NetworkXã‚°ãƒ©ãƒ•
            top_k: ä¸Šä½Kå€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¿”ã™
    
        Returns:
            ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®ã‚»ãƒƒãƒˆ
        """
    # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_emb = self.get_cached_embedding(query, cache_type='entity')
    
    # å…¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
        entity_scores = []
    
        for entity in kg.nodes():
            try:
                entity_emb = self.get_cached_embedding(entity, cache_type='entity')
            
                similarity = float(np.dot(query_emb, entity_emb))
                entity_scores.append((entity, similarity))
        
            except Exception:
                continue
    
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        entity_scores.sort(key=lambda x: x[1], reverse=True)
    
        # Top K ã‚’è¿”ã™
        top_entities = {entity for entity, _ in entity_scores[:top_k]}
    
        return top_entities

    def _resolve_entities_from_chunks(
        self,
        chunk_ids: Set[str],
        kg: nx.Graph
    ) -> Set[str]:
        """
        ãƒãƒ£ãƒ³ã‚¯IDã‹ã‚‰å®Ÿéš›ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã«å¤‰æ›
        
        Args:
            chunk_ids: ãƒãƒ£ãƒ³ã‚¯IDã®ã‚»ãƒƒãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•
        
        Returns:
            ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®ã‚»ãƒƒãƒˆ
        """
        entities = set()
        
        for chunk_id in chunk_ids:
            # chunk_idãŒã™ã§ã«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®å ´åˆ
            if chunk_id in kg.nodes():
                entities.add(chunk_id)
                continue

        # ============================================================
        # 2. ãƒãƒ£ãƒ³ã‚¯IDã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¨å®š
        # ============================================================
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "doc_X_chunkY_hash" å½¢å¼
        # â†’ ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰å±æ€§ 'chunk_id' ã‚’æŒã¤ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
            for node, data in kg.nodes(data=True):
                node_chunk_ids = data.get('chunk_ids', [])
            
                # chunk_ids ãŒæ–‡å­—åˆ—ã®å ´åˆã‚‚ã‚ã‚‹ã®ã§å¯¾å¿œ
                if isinstance(node_chunk_ids, str):
                    node_chunk_ids = [node_chunk_ids]
            
                if chunk_id in node_chunk_ids:
                    entities.add(node)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒ£ãƒ³ã‚¯IDå†…ã«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åãŒå«ã¾ã‚Œã‚‹
        # ï¼ˆä¾‹: chunk_id = "attention_mechanism_chunk3"ï¼‰
        # â†’ ã‚°ãƒ©ãƒ•å†…ã®ãƒãƒ¼ãƒ‰åãŒchunk_idã«éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            chunk_id_lower = chunk_id.lower()
            for node in kg.nodes():
                node_lower = node.lower()
            
            # éƒ¨åˆ†ä¸€è‡´ï¼ˆå°‘ãªãã¨ã‚‚5æ–‡å­—ä»¥ä¸Šï¼‰
                if len(node_lower) >= 5 and node_lower in chunk_id_lower:
                    entities.add(node)
                elif len(chunk_id_lower) >= 5 and chunk_id_lower in node_lower:
                    entities.add(node)
    
        if not entities:
            self.logger.debug(
                f"  Could not resolve entities from {len(chunk_ids)} chunk IDs"
            )
    
        return entities
    
    def _explore_from_entity(
        self,
        kg: nx.Graph,
        start_entity: str,
        query: str,
        max_steps: int,
        visited: Set[str]
    ) -> Dict[str, Any]:
        """
        ç‰¹å®šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æ·±ã•å„ªå…ˆæ¢ç´¢
        
        Returns:
            {
                'paths': [ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ],
                'visited': è¨ªå•ãƒãƒ¼ãƒ‰,
                'steps': æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°,
                'confidence': ä¿¡é ¼åº¦
            }
        """
        paths = []
        visited.add(start_entity)
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_emb = self.get_cached_embedding(query, cache_type='entity')
        
        # BFS
        queue = [(start_entity, [start_entity], 0)]  
        # ãƒ‘ã‚¹æ•°åˆ¶é™
        max_paths = self.config.get('multihop_max_paths', 50)
   
        while queue and len(paths) < max_paths: 
            current, path, depth = queue.pop(0)
            
            if depth >= max_steps:
                continue
            
            # éš£æ¥ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢
            neighbors = list(kg.neighbors(current))
            
            # å„éš£æ¥ãƒãƒ¼ãƒ‰ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            neighbor_scores = []
            for neighbor in neighbors:
                if neighbor in visited:
                    continue
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®åŸ‹ã‚è¾¼ã¿
                entity_emb = self.get_cached_embedding(neighbor, cache_type='entity')
                    
                    # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦
                similarity = float(np.dot(query_emb, entity_emb))
                    
                    # ã‚¨ãƒƒã‚¸ã®é‡ã¿
                edge_weight = kg[current][neighbor].get('weight', 0.5)
                    
                    # ç·åˆã‚¹ã‚³ã‚¢
                score = similarity * 0.6 + edge_weight * 0.4
                    
                neighbor_scores.append((neighbor, score))
                
            
            # ã‚¹ã‚³ã‚¢ä¸Šä½ã‚’é¸æŠ
            neighbor_scores.sort(key=lambda x: x[1], reverse=True)
            beam_width = self.config.get('multihop_beam_width', 2)
            top_neighbors = neighbor_scores[:beam_width]  # 3 â†’ 2
            
            for neighbor, score in top_neighbors:
                new_path = path + [neighbor]
                
            #ãƒ‘ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯ 
                path_tuple = tuple(new_path)
                if path_tuple in self.visited_paths:
                    continue
                self.visited_paths.add(path_tuple)
            
                visited.add(neighbor)

                # ãƒ‘ã‚¹ã‚’ä¿å­˜
                paths.append({
                    'path': new_path,
                    'score': score,
                    'depth': depth + 1
                })
                
                # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                queue.append((neighbor, new_path, depth + 1))
        
        # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆãƒ‘ã‚¹ã®å¹³å‡ã‚¹ã‚³ã‚¢ï¼‰
        confidence = np.mean([p['score'] for p in paths]) if paths else 0.0
        
        return {
            'paths': paths,
            'visited': visited,
            'steps': max_steps,
            'confidence': float(confidence)
        }
    
    def _rank_paths(
        self,
        paths: List[Dict],
        query: str,
        kg: nx.Graph
    ) -> List[Dict]:
        """
        ãƒ‘ã‚¹ã‚’ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        """
        if not paths:
            return []
        
        query_emb = self.get_cached_embedding(query, cache_type='entity')

        # å„ãƒ‘ã‚¹ã«æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for path_info in paths:
            path = path_info['path']
            
            # ãƒ‘ã‚¹ã®é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé•·ã™ãã‚‹ã¨ä¿¡é ¼åº¦ä½ä¸‹ï¼‰
            length_penalty = 1.0 / (1.0 + 0.1 * len(path))
            
            # ã‚¨ãƒƒã‚¸é‡ã¿ã®å¹³å‡
            edge_weights = []
            for i in range(len(path) - 1):
                if kg.has_edge(path[i], path[i+1]):
                    edge_weights.append(kg[path[i]][path[i+1]].get('weight', 0.5))
            
            avg_edge_weight = np.mean(edge_weights) if edge_weights else 0.5
            #  ãƒ‘ã‚¹å…¨ä½“ã¨ã‚¯ã‚¨ãƒªã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢
            path_query_relevance = 0.0     
            entity_similarities = []

            for entity in path:    
                entity_emb = self.get_cached_embedding(entity, cache_type='entity')    
                similarity = float(np.dot(query_emb, entity_emb))
                entity_similarities.append(similarity)
        
            if entity_similarities:
                # ãƒ‘ã‚¹å†…ã®æœ€å¤§é¡ä¼¼åº¦ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚é–¢é€£ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é‡è¦–ï¼‰
                path_query_relevance = max(entity_similarities)
   
            # æœ€çµ‚ã‚¹ã‚³ã‚¢
            final_score = (
                path_info['score'] * 0.4 +
                avg_edge_weight * 0.25 +
                length_penalty * 0.15 +
                path_query_relevance * 0.2
            )
            
            path_info['final_score'] = final_score
            path_info['query_relevance'] = path_query_relevance  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        paths.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        
        return paths

    # ============================================================
    # çµ±åˆãƒ“ãƒ«ãƒ‰é–¢æ•°
    # ============================================================
    
    def commit_to_graph_with_retrieval(
        self,
        documents: List[Document],
        graph_store: Neo4jGraphStore
    ) -> Dict[str, Any]:
        """
        Graph index ã¨ Retrieval store ã‚’åŒæ™‚ã«æ§‹ç¯‰ï¼ˆåŒæœŸç‰ˆï¼‰
        """
        with self.hlogger.section("Dual-Index Building (Synced)"):
            # 1. Dual-documentsç”Ÿæˆï¼ˆåŒæœŸãƒãƒƒãƒ”ãƒ³ã‚°ä»˜ãï¼‰
            graph_docs, retrieval_docs, chunk_mapping = self.create_dual_documents(documents)
            
            self.logger.info(
                f"ğŸ”— Chunk mapping: "
                f"{len(chunk_mapping['graph_to_retrieval'])} graph -> retrieval links"
            )
            
            # 2. Graphæ§‹ç¯‰
            self.logger.info("ğŸ“Š Building knowledge graph...")
            self.commit_to_graph(graph_docs, graph_store)
            
            # 3. Retrieval storeæ§‹ç¯‰
            self.logger.info("ğŸ” Building retrieval store...")
            retrieval_store = self.build_retrieval_store(retrieval_docs)
            
            # chunk_mappingã‚’storeã«è¿½åŠ 
            retrieval_store['chunk_mapping'] = chunk_mapping
        
        return {
            'retrieval_store': retrieval_store,
            'chunk_mapping': chunk_mapping,
            'stats': {
                'graph_docs': len(graph_docs),
                'retrieval_docs': len(retrieval_docs),
                'sync_links': len(chunk_mapping['retrieval_to_graph'])
            }
        }

    def query_with_multihop(
        self,
        query: str,
        kg: nx.Graph,
        retrieval_store: Dict = None,
        max_steps: int = 5,
        top_k_retrieval: int = 5,
        top_k_paths: int = 10
    ) -> Dict[str, Any]:
        """
        Multi-hopæ¢ç´¢ã‚’ä½¿ã£ãŸã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            kg: NetworkXã‚°ãƒ©ãƒ•
            retrieval_store: Retrievalã‚¹ãƒˆã‚¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_steps: æœ€å¤§ãƒ›ãƒƒãƒ—æ•°
            top_k_retrieval: Retrievalçµæœã®ä¸Šä½Kä»¶
            top_k_paths: è¿”ã™ãƒ‘ã‚¹ã®ä¸Šä½Kä»¶
    
        Returns:
            {
                'paths': ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¹,
                'retrieval_docs': Retrievalã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ,
                'confidence': ä¿¡é ¼åº¦,
                'answer': çµ±åˆã•ã‚ŒãŸå›ç­”ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            }
        """
        self.logger.info(f"ğŸ” Query: '{query}'")
    
        results = {
            'paths': [],
            'retrieval_docs': [],
            'confidence': 0.0,
            'answer': None
        }
    
    # ============================================================
    # 1. Retrievalï¼ˆæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    # ============================================================
        retrieval_chunks = []
    
        if retrieval_store:
            try:
                retrieval_results = self.retrieve(
                    store=retrieval_store,
                    query=query,
                    top_k=top_k_retrieval
                )
            
                for score, doc, graph_chunk_ids in retrieval_results:
                    results['retrieval_docs'].append({
                        'text': doc.text,
                        'score': score,
                        'metadata': doc.metadata
                    })
                    retrieval_chunks.extend(graph_chunk_ids)
            
                self.logger.info(
                    f"  â†’ Retrieval: {len(results['retrieval_docs'])} docs, "
                    f"{len(retrieval_chunks)} graph chunks"
                )
        
            except Exception as e:
                self.logger.warning(f"âš ï¸  Retrieval failed: {type(e).__name__}")
    
    # ============================================================
    # 2. Multi-hopæ¢ç´¢
    # ============================================================
        try:
            path_result = self.explore_multi_hop_paths(
                kg=kg,
                query=query,
                retrieval_chunks=retrieval_chunks if retrieval_chunks else None,
                max_steps=max_steps,
                top_k_per_hop=3,
                confidence_threshold=0.7,
                extend_on_low_confidence=True
            )
        
            results['paths'] = path_result['paths'][:top_k_paths]
            results['confidence'] = path_result['confidence']
        
            self.logger.info(
                f"  â†’ Multi-hop: {len(results['paths'])} paths, "
                f"confidence={results['confidence']:.2f}"
            )
    
        except Exception as e:
            self.logger.error(f"ğŸš¨ Multi-hop exploration failed: {type(e).__name__}")
            self.logger.error(f"   {str(e)[:200]}")
        
            if self.logger.level <= logging.DEBUG:
                self.logger.debug(traceback.format_exc())
    
    # ============================================================
    # 3. çµæœã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ============================================================
        if results['paths'] and results['retrieval_docs']:
            results['answer'] = self._synthesize_answer(
                query=query,
                paths=results['paths'],
                retrieval_docs=results['retrieval_docs']
            )
    
        return results

    def _synthesize_answer(
        self,
        query: str,
        paths: List[Dict],
        retrieval_docs: List[Dict]
    ) -> str:
        """
        ãƒ‘ã‚¹ã¨Retrievalãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’çµ±åˆ
    
        Args:
            query: ã‚¯ã‚¨ãƒª
            paths: Multi-hopã§ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¹
            retrieval_docs: Retrievalã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    
        Returns:
            çµ±åˆã•ã‚ŒãŸå›ç­”æ–‡å­—åˆ—
        """
    # ç°¡æ˜“å®Ÿè£…ï¼ˆLLMã‚’ä½¿ã£ãŸçµ±åˆã¯åˆ¥é€”å®Ÿè£…å¯èƒ½ï¼‰
    
        answer_parts = []
    
    # ãƒ‘ã‚¹ã‹ã‚‰ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
        answer_parts.append("**From Knowledge Graph:**")
        for i, path_info in enumerate(paths[:3], 1):
            path = path_info['path']
            score = path_info.get('final_score', 0)
            path_str = ' â†’ '.join(path)
            answer_parts.append(f"{i}. {path_str} (score: {score:.2f})")
    
    # Retrievalãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
        answer_parts.append("\n**From Documents:**")
        for i, doc_info in enumerate(retrieval_docs[:3], 1):
            text_preview = doc_info['text'][:150] + "..."
            score = doc_info['score']
            answer_parts.append(f"{i}. {text_preview} (score: {score:.2f})")
    
        return '\n'.join(answer_parts)

    def link_entities(
        self,
        kg: nx.Graph,
        similarity_threshold: float = 0.88,
        use_embedding: bool = True
    ) -> Tuple[nx.Graph, Dict[str, str]]:
        """
        åŒä¸€å®Ÿä½“ã‚’çµ±åˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            similarity_threshold: çµ±åˆã™ã‚‹é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.95æ¨å¥¨ï¼‰
            use_embedding: True=åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã€False=æ–‡å­—åˆ—é¡ä¼¼åº¦
        
        Returns:
            (çµ±åˆå¾Œã®ã‚°ãƒ©ãƒ•, ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°)
            
        ä¾‹:
            mapping = {
                'Self-Attention': 'self_attention',
                'the attention mechanism': 'self_attention',
                'it': 'self_attention'  # corefè§£æ±ºãŒå¿…è¦
            }
        """
        self.logger.info(f"ğŸ”— Starting entity linking (threshold={similarity_threshold})")
        
        nodes = list(kg.nodes())
        entity_mapping = {}  # old_name -> canonical_name
        clusters = []  # [[é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒªã‚¹ãƒˆ], ...]
        
        # ============================================================
        # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        # ============================================================
        if use_embedding:
            clusters = self._cluster_entities_by_embedding(
                nodes, similarity_threshold
            )
        else:
            clusters = self._cluster_entities_by_string(nodes)
        
        # ============================================================
        # 2. å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨åã‚’æ±ºå®š
        # ============================================================
        for cluster in clusters:
            if len(cluster) <= 1:
                continue
            
            # ä»£è¡¨åã®é¸æŠæˆ¦ç•¥
            canonical = self._select_canonical_name(cluster, kg)
            
            for entity in cluster:
                if entity != canonical:
                    entity_mapping[entity] = canonical
        
        self.logger.info(f"  â†’ {len(entity_mapping)} entities will be merged")
        
        # ============================================================
        # 3. ã‚°ãƒ©ãƒ•ã®çµ±åˆ
        # ============================================================
        merged_kg = self._merge_graph_entities(kg, entity_mapping)
        
        self.logger.info(
            f"âœ… Entity linking complete: "
            f"{len(kg.nodes)} â†’ {len(merged_kg.nodes)} nodes"
        )
        
        return merged_kg, entity_mapping
    
    def _cluster_entities_by_embedding(
        self,
        entities: List[str],
        threshold: float
    ) -> List[List[str]]:
        """
        åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        Returns:
            [[é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£], [é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£], ...]
        """
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—
        embeddings = []
        valid_entities = []
        
        for entity in entities:
            try:
                emb = self.embed_model.get_text_embedding(entity)
                emb = np.array(emb, dtype=np.float32)
                norm = np.linalg.norm(emb)
                
                if norm > 1e-9:
                    emb = emb / norm
                    embeddings.append(emb)
                    valid_entities.append(entity)
            except Exception as e:
                self.logger.debug(f"Embedding failed for '{entity}': {e}")
        
        if len(embeddings) == 0:
            return []
        
        embeddings = np.vstack(embeddings)
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        sim_matrix = embeddings @ embeddings.T
        
        # Union-Find ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        parent = {i: i for i in range(len(valid_entities))}
        
        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]
        
        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py
        
        # é¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸Šã®ãƒšã‚¢ã‚’çµ±åˆ
        for i in range(len(valid_entities)):
            for j in range(i + 1, len(valid_entities)):
                if sim_matrix[i, j] >= threshold:
                    union(i, j)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ§‹ç¯‰
        clusters_dict = defaultdict(list)
        for i, entity in enumerate(valid_entities):
            root = find(i)
            clusters_dict[root].append(entity)
        
        clusters = list(clusters_dict.values())
        
        self.logger.info(
            f"  â†’ Found {len(clusters)} clusters from {len(valid_entities)} entities"
        )
        
        return clusters
    
    def _cluster_entities_by_string(
        self,
        entities: List[str]
    ) -> List[List[str]]:
        """
        æ–‡å­—åˆ—é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆé«˜é€Ÿã ãŒç²¾åº¦ä½ã„ï¼‰
        
        ä½¿ç”¨ã‚±ãƒ¼ã‚¹ï¼š
        - "Self-Attention" ã¨ "self-attention" ã‚’çµ±åˆ
        - "GPT-3" ã¨ "GPT3" ã‚’çµ±åˆ
        """
        
        clusters_dict = defaultdict(list)
        normalized = {}
        
        for entity in entities:
            # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ï¼‰
            norm = entity.lower().replace('-', '').replace('_', '').replace(' ', '')
            normalized[entity] = norm
            clusters_dict[norm].append(entity)
        
        # 2ã¤ä»¥ä¸Šã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹æ­£è¦åŒ–å½¢ã®ã¿è¿”ã™
        clusters = [v for v in clusters_dict.values() if len(v) > 1]
        
        return clusters
    
    def _select_canonical_name(
        self,
        cluster: List[str],
        kg: nx.Graph
    ) -> str:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨åã‚’é¸æŠ
        
        æˆ¦ç•¥ï¼š
        1. æœ€ã‚‚æ¬¡æ•°ãŒé«˜ã„ï¼ˆå¤šãã®é–¢ä¿‚ã‚’æŒã¤ï¼‰
        2. æœ€ã‚‚é•·ã„åå‰ï¼ˆæƒ…å ±é‡ãŒå¤šã„ï¼‰
        3. ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †
        """
        # æ¬¡æ•°ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scores = {}
        for entity in cluster:
            degree = kg.degree(entity) if kg.has_node(entity) else 0
            length = len(entity)
            
            # ã‚¹ã‚³ã‚¢ = æ¬¡æ•° * 10 + é•·ã•
            scores[entity] = degree * 10 + length
        
        # ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®ã‚‚ã®ã‚’é¸æŠ
        canonical = max(cluster, key=lambda e: scores[e])
        
        self.logger.debug(
            f"  Cluster: {cluster} â†’ Canonical: '{canonical}'"
        )
        
        return canonical
    
    def _merge_graph_entities(
        self,
        kg: nx.Graph,
        entity_mapping: Dict[str, str]
    ) -> nx.Graph:
        """
        ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã£ã¦ã‚°ãƒ©ãƒ•ã‚’çµ±åˆ
        
        Args:
            kg: å…ƒã®ã‚°ãƒ©ãƒ•
            entity_mapping: {old_name: canonical_name}
        
        Returns:
            çµ±åˆå¾Œã®ã‚°ãƒ©ãƒ•
        """
        merged_kg = nx.Graph()
        
        # ãƒãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨ï¼‰
        for node, data in kg.nodes(data=True):
            canonical = entity_mapping.get(node, node)
            
            if merged_kg.has_node(canonical):
                # æ—¢å­˜ãƒãƒ¼ãƒ‰ã®å±æ€§ã‚’ãƒãƒ¼ã‚¸
                for key, value in data.items():
                    if key not in merged_kg.nodes[canonical]:
                        merged_kg.nodes[canonical][key] = value
            else:
                merged_kg.add_node(canonical, **data)
        
        # ã‚¨ãƒƒã‚¸ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨ + é‡ã¿çµ±åˆï¼‰
        edge_weights = defaultdict(lambda: {
            'weight': 0.0,
            'intra_raw': 0.0,
            'inter_raw': 0.0,
            'relations': []
        })
        
        for u, v, data in kg.edges(data=True):
            u_canonical = entity_mapping.get(u, u)
            v_canonical = entity_mapping.get(v, v)
            
            # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¯é™¤å¤–
            if u_canonical == v_canonical:
                continue
            
            # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚­ãƒ¼ï¼ˆæ–¹å‘ãªã—ï¼‰
            edge_key = tuple(sorted([u_canonical, v_canonical]))
            
            # é‡ã¿ã‚’ç´¯ç©
            edge_weights[edge_key]['weight'] += data.get('weight', 0.0)
            edge_weights[edge_key]['intra_raw'] += data.get('intra_raw', 0.0)
            edge_weights[edge_key]['inter_raw'] += data.get('inter_raw', 0.0)
            
            # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã‚’è¨˜éŒ²
            rel = data.get('relation', 'RELATED')
            if rel not in edge_weights[edge_key]['relations']:
                edge_weights[edge_key]['relations'].append(rel)
        
        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for (u, v), weights in edge_weights.items():
            merged_kg.add_edge(
                u, v,
                weight=weights['weight'],
                intra_raw=weights['intra_raw'],
                inter_raw=weights['inter_raw'],
                relation=weights['relations'][0] if weights['relations'] else 'RELATED',
                relation_types=weights['relations']
            )
        
        return merged_kg    

    def filter_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        quality_threshold: float = 0.3
    ) -> Tuple[List[Tuple], List[Tuple], Dict]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å“è³ªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            triplets: [(subject, relation, object), ...] ã®ãƒªã‚¹ãƒˆ
            quality_threshold: å“è³ªã‚¹ã‚³ã‚¢ã®é–¾å€¤ï¼ˆ0.0~1.0ï¼‰
        
        Returns:
            (filtered_triplets, rejected_triplets, stats)
        """
        self.logger.info(f"ğŸ” Filtering {len(triplets)} triplets...")
        
        filtered = []
        rejected = []
        quality_scores = []
        
        for s, r, o in triplets:
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            score = self._compute_triplet_quality(s, r, o)
            quality_scores.append(score)
            
            if score >= quality_threshold:
                filtered.append((s, r, o))
            else:
                rejected.append((s, r, o))
                self.logger.debug(
                    f"  Rejected: ({s}, {r}, {o}) [score={score:.2f}]"
                )
        
        # çµ±è¨ˆæƒ…å ±
        stats = {
            'original': len(triplets),
            'filtered': len(filtered),
            'rejected': len(rejected),
            'avg_quality': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'rejection_rate': len(rejected) / len(triplets) if triplets else 0
        }
        
        self.logger.info(
            f"  â†’ Kept {len(filtered)} triplets, "
            f"rejected {len(rejected)} ({stats['rejection_rate']:.1%})"
        )
        self.logger.info(f"  â†’ Avg quality: {stats['avg_quality']:.2f}")
        
        return filtered, rejected, stats
    
    def self_rag_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        chunk_text: str,
        llm: Any = None
    ) -> Tuple[List[Tuple], Dict]:
        """
        Self-RAG: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’è©•ä¾¡ã—ã€ä½å“è³ªãªã‚‚ã®ã‚’å†ç”Ÿæˆ

        """
        if not self.config.get('enable_self_rag', False):
            return triplets, {'self_rag_applied': False}
    
        # ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—ãƒã‚§ãƒƒã‚¯
        token_budget = self.config.get('self_rag_token_budget', 100000)
    
        if self.total_self_rag_tokens >= token_budget:
            self.logger.warning(
                f"âš ï¸  Self-RAG token budget exhausted "
                f"({self.total_self_rag_tokens}/{token_budget}), skipping"
            )
            return triplets, {
                'self_rag_applied': False,
                'budget_exhausted': True
            }

        self.logger.info(f"ğŸ”„ Self-RAG: Evaluating {len(triplets)} triplets...")
    
    # Critic: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’è©•ä¾¡
        evaluated_triplets = []
    
        for s, r, o in triplets:
            confidence = self._critic_evaluate_triplet(s, r, o, chunk_text)
            evaluated_triplets.append({
                'triplet': (s, r, o),
                'confidence': confidence,
                'needs_refinement': confidence < self.config['self_rag_confidence_threshold']
            })
    
    # çµ±è¨ˆ
        low_confidence_count = sum(1 for t in evaluated_triplets if t['needs_refinement'])
        avg_confidence = np.mean([t['confidence'] for t in evaluated_triplets])
    
        self.logger.info(
            f"  â†’ Avg confidence: {avg_confidence:.2f}, "
            f"Low confidence: {low_confidence_count}/{len(triplets)}"
        )
    
    # Refiner: ä½å“è³ªãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ
        refined_triplets = []
        refinement_stats = {
            'attempted': 0,
            'succeeded': 0,
            'failed': 0,
            'tokens_used': 0 
        }
    
        for triplet_info in evaluated_triplets:
            # äºˆç®—ãƒã‚§ãƒƒã‚¯
            if self.total_self_rag_tokens >= token_budget:
                self.logger.info("  â†’ Budget limit reached, stopping refinement")
                # æ®‹ã‚Šã¯å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ä¿æŒ
                refined_triplets.append(triplet_info['triplet'])
                continue

            if triplet_info['needs_refinement']:
                # å†ç”Ÿæˆã‚’è©¦ã¿ã‚‹
                refined, tokens_used = self._refiner_regenerate_triplet(
                    triplet_info['triplet'],
                    chunk_text,
                    llm
                )
            
                refinement_stats['attempted'] += 1
                refinement_stats['tokens_used'] += tokens_used
                self.total_self_rag_tokens += tokens_used
            
                if refined:
                # å†è©•ä¾¡
                    s, r, o = refined
                    new_confidence = self._critic_evaluate_triplet(s, r, o, chunk_text)
                
                    if new_confidence > triplet_info['confidence']:
                    # æ”¹å–„ã•ã‚ŒãŸå ´åˆã¯ç½®ãæ›ãˆ
                        refined_triplets.append(refined)
                        refinement_stats['succeeded'] += 1
                    
                        self.logger.debug(
                            f"  âœ“ Refined: {triplet_info['triplet']} â†’ {refined} "
                            f"(confidence: {triplet_info['confidence']:.2f} â†’ {new_confidence:.2f})"
                        )
                    else:
                    # æ”¹å–„ã•ã‚Œãªã‹ã£ãŸå ´åˆã¯å…ƒã‚’ä¿æŒ
                        refined_triplets.append(triplet_info['triplet'])
                        refinement_stats['failed'] += 1
                else:
                # å†ç”Ÿæˆå¤±æ•—æ™‚ã¯å…ƒã‚’ä¿æŒ
                    refined_triplets.append(triplet_info['triplet'])
                    refinement_stats['failed'] += 1
            else:
            # é«˜å“è³ªãªã‚‚ã®ã¯ãã®ã¾ã¾
                refined_triplets.append(triplet_info['triplet'])
    
    # ============================================================
    # 3. Validator: æœ€çµ‚æ¤œè¨¼
    # ============================================================
        validated_triplets = self._validator_check_consistency(
            refined_triplets,
            chunk_text
        )
    
    # çµ±è¨ˆæƒ…å ±
        stats = {
            'self_rag_applied': True,
            'original_count': len(triplets),
            'refined_count': len(validated_triplets),
            'avg_confidence': float(avg_confidence),
            'low_confidence_count': low_confidence_count,
            'refinement_stats': refinement_stats,
            'total_tokens_used': self.total_self_rag_tokens
        }
    
        self.logger.info(
            f"  â†’ Self-RAG complete: "
            f"{refinement_stats['succeeded']} improved, "
            f"{refinement_stats['failed']} kept original"
            f"tokens: {refinement_stats['tokens_used']}"        
        )
    
        return validated_triplets, stats

    def _critic_evaluate_triplet(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str
    ) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®å“è³ªã‚’è©•ä¾¡ï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: 0.0ï½1.0ï¼‰
    
        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
    
        Returns:
            ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©é«˜å“è³ªï¼‰
        """
        scores = []
    
    # ============================================================
    # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å“è³ªï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
    # ============================================================
        entity_score = self._score_entities(subject, object_)
        scores.append(('entity', entity_score, 0.3))
    
    # ============================================================
    # 2. é–¢ä¿‚ã®æ˜ç¢ºæ€§ï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
    # ============================================================
        relation_score = self._score_relation(relation)
        scores.append(('relation', relation_score, 0.3))
    
    # ============================================================
    # 3. æ–‡æ³•çš„æ­£ã—ã•ï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
    # ============================================================
        grammar_score = self._score_grammar(subject, relation, object_)
        scores.append(('grammar', grammar_score, 0.2))
    
    # ============================================================
    # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®æ•´åˆæ€§ï¼ˆæ–°è¦ï¼‰
    # ============================================================
        context_score = self._score_context_alignment(
            subject, relation, object_, context
        )
        scores.append(('context', context_score, 0.2))
    
    # ============================================================
    # 5. é‡ã¿ä»˜ãå¹³å‡
    # ============================================================
        total_score = sum(score * weight for _, score, weight in scores)
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆDEBUGæ™‚ã®ã¿ï¼‰
        if self.logger.level <= logging.DEBUG:
            score_details = ', '.join(f"{name}={score:.2f}" for name, score, _ in scores)
            self.logger.debug(
                f"  Triplet: ({subject[:20]}, {relation}, {object_[:20]}) "
                f"â†’ {score_details} = {total_score:.2f}"
            )
    
        return total_score

    def _score_context_alignment(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str
    ) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ•´åˆæ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    
        Returns:
            0.0ï¼ˆæ•´åˆæ€§ãªã—ï¼‰ï½ 1.0ï¼ˆå®Œå…¨ã«æ•´åˆï¼‰
        """
        score = 0.0
        context_lower = context.lower()
    
    # ============================================================
    # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹ã‹
    # ============================================================
        subject_in_context = subject.lower() in context_lower
        object_in_context = object_.lower() in context_lower
    
        if subject_in_context and object_in_context:
            score += 0.5
        elif subject_in_context or object_in_context:
            score += 0.3
        else:
            # ã©ã¡ã‚‰ã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„å ´åˆã¯ä½ã‚¹ã‚³ã‚¢
            score += 0.1
    
    # ============================================================
    # 2. é–¢ä¿‚ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆã¨åˆè‡´ã™ã‚‹ã‹
    # ============================================================
        relation_lower = relation.lower().replace('_', ' ')
    
    # é–¢ä¿‚ã®å‹•è©å½¢ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹ã‹
        if relation_lower in context_lower:
            score += 0.3
        else:
        # é¡ä¼¼è¡¨ç¾ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            relation_synonyms = self._get_relation_synonyms(relation)
            if any(syn in context_lower for syn in relation_synonyms):
                score += 0.2
    
    # ============================================================
    # 3. ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå…¨ä½“ã®è¿‘æ¥æ€§
    # ============================================================
    # ä¸»èªã¨ç›®çš„èªãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§è¿‘ã„ä½ç½®ã«ã‚ã‚‹ã‹
        if subject_in_context and object_in_context:
            try:
                subject_pos = context_lower.find(subject.lower())
                object_pos = context_lower.find(object_.lower())
            
                distance = abs(object_pos - subject_pos)
            
            # è·é›¢ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´ï¼ˆè¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
                if distance < 50:
                    score += 0.2
                elif distance < 100:
                    score += 0.1
            except Exception:
                pass
    
        return min(score, 1.0)

    def _get_relation_synonyms(self, relation: str) -> List[str]:
        """
        é–¢ä¿‚ã®åŒç¾©èªãƒ»é¡ä¼¼è¡¨ç¾ã‚’è¿”ã™
    
        Args:
            relation: é–¢ä¿‚å
    
        Returns:
            åŒç¾©èªã®ãƒªã‚¹ãƒˆ
        """
    # ä¸»è¦ãªé–¢ä¿‚ã®åŒç¾©èªãƒãƒƒãƒ—
        synonym_map = {
            'uses': ['use', 'utilizes', 'employs', 'applies'],
            'causes': ['cause', 'leads to', 'results in', 'triggers'],
            'part_of': ['part of', 'component of', 'belongs to'],
            'is_a': ['is a', 'type of', 'kind of', 'instance of'],
            'has': ['have', 'contains', 'includes', 'comprises'],
            'improves': ['improve', 'enhances', 'optimizes', 'boosts'],
            'based_on': ['based on', 'derived from', 'built on', 'relies on'],
            'enables': ['enable', 'allows', 'permits', 'facilitates'],
            'requires': ['require', 'needs', 'depends on', 'necessitates'],
        }
    
        relation_lower = relation.lower().replace('_', ' ')
    
    # å®Œå…¨ä¸€è‡´ã‚’æ¢ã™
        for key, synonyms in synonym_map.items():
            if relation_lower == key.replace('_', ' ') or relation_lower in synonyms:
                return synonyms
    
    # ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®é–¢ä¿‚ã®ã¿
        return [relation_lower]

    def _refiner_regenerate_triplet(
        self,
        original_triplet: Tuple[str, str, str],
        chunk_text: str,
        llm: Any = None
    ) -> Tuple[Optional[Tuple[str, str, str]], int]: 
        """
        ä½å“è³ªãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ
    
        Args:
            original_triplet: å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ
            chunk_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            llm: LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    
        Returns:
            æ”¹å–„ã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        s, r, o = original_triplet
    
    # LLMãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        if llm is None:
            llm = OpenAI(
                model=self.config['self_rag_refiner_model'],
                timeout=self.config['llm_timeout']
            )
    
    # ============================================================
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    # ============================================================
        prompt = f"""Given the following text, improve the quality of this knowledge triplet.

    Original triplet:
    - Subject: {s}
    - Relation: {r}
    - Object: {o}

    Text context:
    {chunk_text[:500]}

    Please provide an improved triplet that:
    1. Uses more specific and descriptive entities
    2. Uses a clear and meaningful relation
    3. Accurately reflects the text content
    4. Avoids vague terms like "it", "this", "that"

    Return ONLY the improved triplet in this exact format:
    Subject: [improved subject]
    Relation: [improved relation]
    Object: [improved object]

    If the original triplet cannot be improved, return "NO_IMPROVEMENT".
    """
    # ç°¡æ˜“å®Ÿè£…: æ–‡å­—æ•° / 4 â‰ˆ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆè‹±èªï¼‰
        prompt_tokens = len(prompt) // 4
        
    # ============================================================
    # LLMã§å†ç”Ÿæˆ
    # ============================================================
        try:
            response = llm.complete(prompt)
            response_text = response.text.strip()
        
            response_tokens = len(response_text) // 4
            total_tokens = prompt_tokens + response_tokens

            # "NO_IMPROVEMENT"ãƒã‚§ãƒƒã‚¯
            if "NO_IMPROVEMENT" in response_text.upper():
                return None, total_tokens
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            refined = self._parse_triplet_response(response_text)
        
            if refined:
                return refined, total_tokens
            else:
                self.logger.debug(f"  Failed to parse refinement response")
                return None, total_tokens
    
        except Exception as e:
            self.logger.debug(f"  Refinement failed: {type(e).__name__}")
            return None, prompt_tokens
        
    def _parse_triplet_response(self, response: str) -> Optional[Tuple[str, str, str]]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º
    
        Args:
            response: LLMã®å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    
        Returns:
            (subject, relation, object) ã¾ãŸã¯ None
        """
        try:
            lines = response.strip().split('\n')
        
            subject = None
            relation = None
            object_ = None
        
            for line in lines:
                line = line.strip()
            
                if line.startswith('Subject:'):
                    subject = line.replace('Subject:', '').strip()
                elif line.startswith('Relation:'):
                    relation = line.replace('Relation:', '').strip()
                elif line.startswith('Object:'):
                    object_ = line.replace('Object:', '').strip()
        
        # ã™ã¹ã¦ãŒæŠ½å‡ºã§ããŸã‹ç¢ºèª
            if subject and relation and object_:
            # ç©ºç™½ã‚„è¨˜å·ã®ã¿ã§ãªã„ã‹ç¢ºèª
                if (len(subject.strip()) > 1 and 
                    len(relation.strip()) > 1 and 
                    len(object_.strip()) > 1):
                    return (subject, relation, object_)
        
            return None
    
        except Exception as e:
            self.logger.debug(f"  Parse error: {e}")
            return None
        
    def _validator_check_consistency(
        self,
        triplets: List[Tuple[str, str, str]],
        context: str
    ) -> List[Tuple[str, str, str]]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ä¸€è²«æ€§ã¨çŸ›ç›¾ã‚’ãƒã‚§ãƒƒã‚¯
    
        Args:
            triplets: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
    
        Returns:
            æ¤œè¨¼æ¸ˆã¿ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆï¼ˆçŸ›ç›¾ãŒã‚ã‚‹ã‚‚ã®ã¯é™¤å¤–ï¼‰
        """
        validated = []
        seen_triplets = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    
        for s, r, o in triplets:
        # ============================================================
        # 1. é‡è¤‡ãƒã‚§ãƒƒã‚¯
        # ============================================================
            triplet_key = (s.lower(), r.lower(), o.lower())
            if triplet_key in seen_triplets:
                self.logger.debug(f"  âŠ— Duplicate: ({s}, {r}, {o})")
                continue
        
        # ============================================================
        # 2. è‡ªå·±å‚ç…§ãƒã‚§ãƒƒã‚¯ï¼ˆä¸»èªã¨ç›®çš„èªãŒåŒã˜ï¼‰
        # ============================================================
            if s.lower().strip() == o.lower().strip():
                self.logger.debug(f"  âŠ— Self-reference: ({s}, {r}, {o})")
                continue
        
        # ============================================================
        # 3. é€†é–¢ä¿‚ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
        # ============================================================
            if self._has_contradictory_relation(s, r, o, validated):
                self.logger.debug(f"  âŠ— Contradictory: ({s}, {r}, {o})")
                continue
        
        # ============================================================
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¦¥å½“æ€§ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        # ============================================================
            if not self._is_contextually_valid(s, r, o, context):
                self.logger.debug(f"  âŠ— Context invalid: ({s}, {r}, {o})")
                continue
        
           # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹
            validated.append((s, r, o))
            seen_triplets.add(triplet_key)
    
        removed_count = len(triplets) - len(validated)
        if removed_count > 0:
            self.logger.info(f"  â†’ Validator removed {removed_count} inconsistent triplets")
    
        return validated

    def _has_contradictory_relation(
        self,
        subject: str,
        relation: str,
        object_: str,
        existing_triplets: List[Tuple[str, str, str]]
    ) -> bool:
        """
        æ—¢å­˜ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã¨çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
    
        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            existing_triplets: æ—¢ã«æ¤œè¨¼æ¸ˆã¿ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ
    
        Returns:
            True: çŸ›ç›¾ã‚ã‚Š, False: çŸ›ç›¾ãªã—
        """
    # çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ã®ãƒšã‚¢
        contradictory_pairs = [
        # åŸå› ã¨çµæœã®é€†è»¢
            ('causes', 'caused_by'),
            ('creates', 'created_by'),
            ('produces', 'produced_by'),
        
        # åŒ…å«é–¢ä¿‚ã®é€†è»¢
            ('part_of', 'contains'),
            ('component_of', 'has_component'),
            ('member_of', 'has_member'),
        
        # è‚¯å®šã¨å¦å®š
            ('is', 'is_not'),
            ('has', 'lacks'),
            ('enables', 'prevents'),
        
        # æ™‚é–“çš„çŸ›ç›¾
            ('before', 'after'),
            ('precedes', 'follows'),
        ]
    
        subject_lower = subject.lower()
        object_lower = object_.lower()
        relation_lower = relation.lower().replace('_', ' ').replace('-', ' ')
    
        for s_exist, r_exist, o_exist in existing_triplets:
            s_exist_lower = s_exist.lower()
            o_exist_lower = o_exist.lower()
            r_exist_lower = r_exist.lower().replace('_', ' ').replace('-', ' ')
        
        # åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢ã§ç•°ãªã‚‹é–¢ä¿‚
            if ((subject_lower == s_exist_lower and object_lower == o_exist_lower) or
                (subject_lower == o_exist_lower and object_lower == s_exist_lower)):
            
            # çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ã®ãƒšã‚¢ã‚’ãƒã‚§ãƒƒã‚¯
                for rel1, rel2 in contradictory_pairs:
                    if ((relation_lower == rel1 and r_exist_lower == rel2) or
                        (relation_lower == rel2 and r_exist_lower == rel1)):
                        self.logger.debug(
                            f"  Found contradiction: "
                            f"({subject}, {relation}, {object_}) vs "
                            f"({s_exist}, {r_exist}, {o_exist})"
                        )
                        return True
    
        return False

    def _is_contextually_valid(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str,
        min_score: float = 0.3
    ) -> bool:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦å¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯
    
        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            min_score: æœ€å°ã‚¹ã‚³ã‚¢é–¾å€¤
    
        Returns:
            True: å¦¥å½“, False: ä¸é©åˆ‡
        """
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
        score = self._score_context_alignment(subject, relation, object_, context)
    
        return score >= min_score

    def _compute_triplet_quality(
        self,
        subject: str,
        relation: str,
        object_: str
    ) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–ï¼š
        - é–¢ä¿‚ã®æ˜ç¢ºæ€§ï¼ˆ0.4ï¼‰
        - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…·ä½“æ€§ï¼ˆ0.3ï¼‰
        - æ–‡æ³•çš„æ­£ã—ã•ï¼ˆ0.3ï¼‰
        
        Returns:
            0.0~1.0 ã®ã‚¹ã‚³ã‚¢
        """
        score = 0.0
        
        # ============================================================
        # 1. é–¢ä¿‚ã®æ˜ç¢ºæ€§ï¼ˆ0.4ï¼‰
        # ============================================================
        relation_score = self._score_relation(relation)
        score += relation_score * 0.4
        
        # ============================================================
        # 2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…·ä½“æ€§ï¼ˆ0.3ï¼‰
        # ============================================================
        entity_score = self._score_entities(subject, object_)
        score += entity_score * 0.3
        
        # ============================================================
        # 3. æ–‡æ³•çš„æ­£ã—ã•ï¼ˆ0.3ï¼‰
        # ============================================================
        grammar_score = self._score_grammar(subject, relation, object_)
        score += grammar_score * 0.3
        
        return min(max(score, 0.0), 1.0)
    
    def _map_triplets_to_documents(
        self,
        triplets: List[Tuple[str, str, str]],
        documents: List[Document]
    ) -> Dict[Document, List[Tuple[str, str, str]]]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚°
    
        Args:
            triplets: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    
        Returns:
            {Document: [triplets]} ã®è¾æ›¸
        """
        mapping = {doc: [] for doc in documents}
    
    # å„ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆãŒã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹åˆ¤å®š
        for s, r, o in triplets:
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            for doc in documents:
                doc_text_lower = doc.text.lower()
            
            # ä¸»èªã¾ãŸã¯ç›®çš„èªãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹
                if (s.lower() in doc_text_lower or o.lower() in doc_text_lower):
                    mapping[doc].append((s, r, o))
                    break  # æœ€åˆã«ãƒãƒƒãƒã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦
            else:
            # ã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚‚ãƒãƒƒãƒã—ãªã„å ´åˆã¯æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦
                if documents:
                    mapping[documents[0]].append((s, r, o))
    
    # ç©ºã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        mapping = {doc: trips for doc, trips in mapping.items() if trips}
    
        self.logger.info(f"  Mapped {len(triplets)} triplets to {len(mapping)} documents")
    
        return mapping
    
    def _score_relation(self, relation: str) -> float:
        """
        é–¢ä¿‚ã®æ˜ç¢ºæ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        
        Returns:
            0.0ï¼ˆæœ€æ‚ªï¼‰ï½ 1.0ï¼ˆæœ€è‰¯ï¼‰
        """
        relation_lower = relation.lower().strip()
        
        # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆå³åº§ã«0.0ï¼‰
        if relation_lower in self.relation_blacklist:
            return 0.0
        
        # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
        if len(relation_lower) < 2:
            return 0.0
        
        # é«˜å“è³ªãªé–¢ä¿‚ï¼ˆå°‚é–€çš„ãƒ»å…·ä½“çš„ï¼‰
        high_quality_relations = {
            # å› æœé–¢ä¿‚
            'causes', 'results_in', 'leads_to', 'enables', 'triggers',
            'produces', 'generates', 'influences', 'affects',
            
            # æ§‹æˆé–¢ä¿‚
            'part_of', 'component_of', 'consists_of', 'comprises',
            'contains', 'includes',
            
            # ä½¿ç”¨é–¢ä¿‚
            'uses', 'utilizes', 'employs', 'applies', 'leverages',
            'implements', 'adopts',
            
            # æ´¾ç”Ÿé–¢ä¿‚
            'based_on', 'derived_from', 'inspired_by', 'extends',
            'improves_upon', 'builds_on',
            
            # å°‚é–€é–¢ä¿‚
            'optimizes', 'parameterizes', 'regularizes', 'approximates',
            'encodes', 'decodes', 'transforms', 'projects',
            
            # æ¯”è¼ƒé–¢ä¿‚
            'outperforms', 'surpasses', 'exceeds', 'improves',
        }
        
        if relation_lower in high_quality_relations:
            return 1.0
        
        # ä¸­å“è³ªãªé–¢ä¿‚ï¼ˆä¸€èˆ¬çš„ã ãŒæœ‰ç”¨ï¼‰
        medium_quality_relations = {
            'is_a', 'type_of', 'instance_of', 'subclass_of',
            'related_to', 'associated_with', 'connected_to',
            'depends_on', 'requires', 'needs',
        }
        
        if relation_lower in medium_quality_relations:
            return 0.7
        
        # å‹•è©å½¢å¼ï¼ˆ-s, -ed, -ingï¼‰ãªã‚‰ä¸­ç¨‹åº¦
        if re.match(r'\w+(s|ed|ing)$', relation_lower):
            return 0.6
        
        # ãã‚Œä»¥å¤–ã¯ä½å“è³ª
        return 0.3
    
    def _score_entities(self, subject: str, object_: str) -> float:
        """
        ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…·ä½“æ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        
        Returns:
            0.0ï¼ˆæŠ½è±¡çš„ãƒ»æ›–æ˜§ï¼‰ï½ 1.0ï¼ˆå…·ä½“çš„ï¼‰
        """
        score = 0.0
        
        # ä¸¡æ–¹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒã‚§ãƒƒã‚¯
        for entity in [subject, object_]:
            entity_lower = entity.lower().strip()
            
            # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
            if len(entity_lower) < 2:
                continue
            
            # ä»£åè©ï¼ˆä½å“è³ªï¼‰
            pronouns = {'it', 'this', 'that', 'these', 'those', 'they', 'them'}
            if entity_lower in pronouns:
                score += 0.0
                continue
            
            # å˜èªæ•°ï¼ˆè¤‡æ•°å˜èª = ã‚ˆã‚Šå…·ä½“çš„ï¼‰
            word_count = len(entity_lower.split())
            if word_count >= 3:
                score += 1.0
            elif word_count == 2:
                score += 0.8
            else:
                score += 0.5
        
        # 2ã¤ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å¹³å‡
        return score / 2.0
    
    def _score_grammar(
        self,
        subject: str,
        relation: str,
        object_: str
    ) -> float:
        """
        æ–‡æ³•çš„æ­£ã—ã•ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        
        Returns:
            0.0ï¼ˆæ–‡æ³•çš„ã«ãŠã‹ã—ã„ï¼‰ï½ 1.0ï¼ˆæ­£ã—ã„ï¼‰
        """
        score = 1.0
        
        # å…¨ã¦å°æ–‡å­—ï¼ˆæŠ½å‡ºãƒŸã‚¹ã®å¯èƒ½æ€§ï¼‰
        if subject.islower() and object_.islower():
            score -= 0.2
        
        # æ•°å­—ã ã‘ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆä½å“è³ªï¼‰
        if subject.isdigit() or object_.isdigit():
            score -= 0.3
        
        # è¨˜å·ã®ã¿
        if not re.search(r'[a-zA-Z]', subject) or not re.search(r'[a-zA-Z]', object_):
            score -= 0.5
        
        # ä¸»èªã¨ç›®çš„èªãŒåŒã˜ï¼ˆè‡ªå·±å‚ç…§ï¼‰
        if subject.lower() == object_.lower():
            score -= 0.5
        

    # ------------------------------------------------------------
    # 2. é–¢ä¿‚ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    # ------------------------------------------------------------
    
        relation_lower = relation.lower().strip()
    
        # é–¢ä¿‚ãŒç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
        if len(relation_lower) < 2:
            score -= 0.4
    
        # é–¢ä¿‚ãŒãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ï¼ˆä½å“è³ªï¼‰
        if relation_lower in self.relation_blacklist:
            score -= 0.3
    
        # é–¢ä¿‚ãŒè¨˜å·ã®ã¿
        if not re.search(r'[a-zA-Z]', relation):
            score -= 0.4
    
    # ------------------------------------------------------------
    # 3. ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå…¨ä½“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    # ------------------------------------------------------------
    
        # ä¸»èªã¨é–¢ä¿‚ãŒåŒã˜ï¼ˆä¾‹: "uses uses object"ï¼‰
        if subject.lower() == relation_lower:
            score -= 0.3
    
    # ç›®çš„èªã¨é–¢ä¿‚ãŒåŒã˜ï¼ˆä¾‹: "subject uses uses"ï¼‰
        if object_.lower() == relation_lower:
            score -= 0.3
    
    # 3ã¤ã¨ã‚‚åŒã˜ï¼ˆæœ€æ‚ªï¼‰
        if subject.lower() == relation_lower == object_.lower():
            score -= 0.5

        return max(score, 0.0)

    def commit_to_graph(self, documents: List[Document], graph_store: Neo4jGraphStore):
        """Neo4jã«ã‚°ãƒ©ãƒ•ã‚’æŠ•å…¥"""
        #ã€€æ¥ç¶šç¢ºèªã€€===========================================
        try:
            graph_store.query("RETURN 1")
            self.logger.info("âœ… Neo4j connection verified")
        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j connection failed: {type(e).__name__}")
            raise  # æ¥ç¶šã§ããªã„ãªã‚‰å‡¦ç†ã‚’ä¸­æ–­
        # 2. ã‚°ãƒ©ãƒ•ç”Ÿæˆ ===========================================
        try:
            with self.hlogger.section("Graph Generation"):
                llm = OpenAI(
                    model=self.config['llm_model'],
                    timeout=self.config['llm_timeout']
                )
            #    storage_context = StorageContext.from_defaults(graph_store=graph_store)
            
                local_graph_store = SimpleGraphStore()
                local_storage = StorageContext.from_defaults(graph_store=local_graph_store)

                self.logger.info("Building local knowledge graph...")
                index = KnowledgeGraphIndex.from_documents(
                    documents,
                    storage_context=local_storage, 
                    llm=llm,
                    transformations=[SimpleNodeParser.from_defaults(chunk_size=512)],
                    embed_model=self.embed_model,
                    show_progress=True,
                    max_triplets_per_chunk=self.config['max_triplets_per_chunk']    # 15
                )
            
                kg = index.get_networkx_graph()
                self.logger.info(f"âœ… Initial graph: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
                all_triples = []

                for subj, obj, data in kg.edges(data=True):
                    rel = data.get('relation', 'RELATED')
                    all_triples.append((subj, rel, obj))
                # rel_mapå‡¦ç†
                if hasattr(local_graph_store, 'get_rel_map'):
                    try:
                        rel_map = local_graph_store.get_rel_map()
                        self.logger.debug(f"rel_map structure: {type(rel_map)}")
        
                        for subj, relations in rel_map.items():
                        # relations ãŒè¾æ›¸ã‹ã€ãƒªã‚¹ãƒˆã‹ç¢ºèª
                            if isinstance(relations, dict):
                                # è¾æ›¸ã®å ´åˆ
                                for rel, objs in relations.items():
                                    if isinstance(objs, list):
                                        for obj in objs:
                                            if (subj, rel, obj) not in all_triples:
                                                all_triples.append((subj, rel, obj))
                                    else:
                                        if (subj, rel, objs) not in all_triples:
                                            all_triples.append((subj, rel, objs))
                            elif isinstance(relations, list):
                                # ãƒªã‚¹ãƒˆã®å ´åˆ
                                for item in relations:
                                    if isinstance(item, tuple) and len(item) == 2:
                                        rel, obj = item
                                        if (subj, rel, obj) not in all_triples:
                                            all_triples.append((subj, rel, obj))
                    except Exception as e:
                        self.logger.warning(f"Could not parse rel_map: {e}")

                self.logger.info(f"Extracted {len(all_triples)} triples (before filtering)")

# Self-RAGçµ±åˆ 
# å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
                if self.config.get('enable_triplet_filter', True):
                    filtered_triples, rejected_triples, filter_stats = self.filter_triplets(
                        all_triples,
                        quality_threshold=self.config.get('triplet_quality_threshold', 0.3)
                    )
                    all_triples = filtered_triples
    
                    self.logger.info(
                        f"After filtering: {len(all_triples)} triples "
                        f"(rejection rate: {filter_stats['rejection_rate']:.1%})"
                    )

# Self-RAGã‚’é©ç”¨ï¼ˆãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†ï¼‰
                if self.config.get('enable_self_rag', False):
                    with self.hlogger.section("Self-RAG Refinement"):
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ
                        refined_all_triples = []
                        total_self_rag_stats = {
                            'attempted': 0,
                            'succeeded': 0,
                            'failed': 0
                        }
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                        doc_triplet_map = self._map_triplets_to_documents(all_triples, documents)
        
                        for doc_idx, (doc, doc_triplets) in enumerate(doc_triplet_map.items()):
                            if not doc_triplets:
                                continue
            
                            try:
                                refined_triplets, stats = self.self_rag_triplets(
                                    doc_triplets,
                                    doc.text,
                                    llm=llm  # æ—¢å­˜ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
                                )

                                refined_all_triples.extend(refined_triplets)
                
                # çµ±è¨ˆã‚’é›†è¨ˆ
                                if stats.get('self_rag_applied'):
                                    ref_stats = stats['refinement_stats']
                                    total_self_rag_stats['attempted'] += ref_stats['attempted']
                                    total_self_rag_stats['succeeded'] += ref_stats['succeeded']
                                    total_self_rag_stats['failed'] += ref_stats['failed']
                
                                if (doc_idx + 1) % 10 == 0:
                                    self.logger.info(f"  Processed {doc_idx + 1}/{len(doc_triplet_map)} documents...")
            
                            except Exception as e:
                                self.logger.warning(f"  Self-RAG failed for doc {doc_idx}: {type(e).__name__}")
                        # å¤±æ•—æ™‚ã¯å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ä¿æŒ
                                refined_all_triples.extend(doc_triplets)
        
        # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æ›´æ–°
                        all_triples = refined_all_triples
        
                        self.logger.info(
                            f"âœ… Self-RAG complete: "
                            f"{total_self_rag_stats['succeeded']} improved, "
                            f"{total_self_rag_stats['attempted']} attempted, "
                            f"final count: {len(all_triples)}"
                        )
                # å†åº¦å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
                if self.config.get('enable_triplet_filter', True):
                    filtered_triples,rejected_triples, filter_stats = self.filter_triplets(
                        all_triples,
                        quality_threshold=self.config.get('triplet_quality_threshold', 0.3)
                    )
                    all_triples = filtered_triples
                
                    # çµ±è¨ˆæƒ…å ±ã‚’æ´»ç”¨
                    self.logger.info(
                        f"After filtering: {len(all_triples)} triples "
                        f"(rejection rate: {filter_stats['rejection_rate']:.1%})"
                    )

                    # å“è³ªãŒä½ã„å ´åˆã¯è­¦å‘Š
                    if filter_stats['avg_quality'] < 0.5:
                        self.logger.warning("âš ï¸  Low average triplet quality!")

                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ãƒªã‚¸ã‚§ã‚¯ãƒˆä¾‹ã‚’è¡¨ç¤º
                    if rejected_triples and self.logger.level <= logging.DEBUG:
                        self.logger.debug("Sample rejected triplets:")
                        for s, r, o in rejected_triples[:3]:
                            self.logger.debug(f"  ({s}, {r}, {o})")

                # ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å…¨ãƒˆãƒªãƒ—ãƒ«ã‚’å…±æœ‰
                for doc in documents:
                    doc.metadata['triples'] = all_triples

        except Exception as e:
            self.logger.error(
                f"ğŸš¨ Graph generation failed: {type(e).__name__}"
            )
            raise

        # Entity Linking
        try:
            with self.hlogger.section("Entity Linking"):
                kg, entity_mapping = self.link_entities(
                    kg,
                    similarity_threshold=self.config['entity_linking_threshold'],
                    use_embedding=True
                )
                
                # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæ›´æ–°
                updated_triples = []
                for s, r, o in all_triples:
                    s_new = entity_mapping.get(s, s)
                    o_new = entity_mapping.get(o, o)
                    if s_new != o_new:  # è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å¤–
                        updated_triples.append((s_new, r, o_new))
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                for doc in documents:
                    doc.metadata['triples'] = updated_triples
                
                self.logger.info(f"Updated triples: {len(all_triples)} â†’ {len(updated_triples)}")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸  Entity linking failed: {e}")
            # Entity Linkingå¤±æ•—ã§ã‚‚å‡¦ç†ã¯ç¶™ç¶š        

        # ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã«çµ±åˆã€€================================
        try:
            with self.hlogger.section("Merging Path Information"):
                self.merge_paths_into_kg(kg, documents)
                self.logger.info(f"âœ… Path info merged: {len(kg.nodes)} nodes, {len(kg.edges)} edges")

        except Exception as e:
            self.logger.warning(f"âš ï¸  Path merging failed: {type(e).__name__} - {str(e)[:100]}")
    
        self.logger.info("  â†’ Continuing without path information")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¨˜éŒ²
        if self.logger.level <= logging.DEBUG:
            self.logger.debug(f"Path merge traceback:\n{traceback.format_exc()}")
    
    # documentsã‹ã‚‰pathsæƒ…å ±ã‚’å‰Šé™¤ï¼ˆä¸­é€”åŠç«¯ãªãƒ‡ãƒ¼ã‚¿ã‚’æ®‹ã•ãªã„ï¼‰
        for doc in documents:
            doc.metadata.pop('paths', None)
            doc.metadata.pop('path_distances', None)

        # RAPLæœ€é©åŒ–
        try:
            with self.hlogger.section("Graph Optimization (RAPL)"):
                optimized_kg = self._optimize_graph_rapl(kg, documents)
                self.logger.info(
                    f"âœ… Optimized graph: {len(optimized_kg.nodes)} nodes, "
                    f"{len(optimized_kg.edges)} edges"
                )
        except Exception as e:
            self.logger.error(  
                f"ğŸš¨ Graph optimization failed: {e}")
            optimized_kg = kg 

    # Multi-hop ãƒ‘ã‚¹æ¢ç´¢ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§ä»£è¡¨çš„ãªãƒ‘ã‚¹ã‚’è¨ˆç®—ï¼‰
        try:
            with self.hlogger.section("Multi-hop Path Pre-computation"):
                self._precompute_representative_paths(optimized_kg, documents)
                self.logger.info("âœ… Representative paths computed and stored")
    
        except Exception as e:
            self.logger.warning(f"âš ï¸  Path pre-computation failed: {type(e).__name__} - {str(e)[:100]}")
            self.logger.info("  â†’ Continuing without pre-computed paths")
        
            if self.logger.level <= logging.DEBUG:
                self.logger.debug(f"Path pre-computation traceback:\n{traceback.format_exc()}")
    
            # æœ€é©åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’Neo4jã«åæ˜ 
        try:
            with self.hlogger.section("Updating Neo4j"):
                result = self._update_neo4j_structure(optimized_kg, graph_store)
            
            # result ãŒ None ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if result is None:
                    result = {'updated': 0, 'skipped': 0, 'failed': 0, 'error_details': []}
                    
                    self.logger.warning("âš ï¸  _update_neo4j_structure returned None")

            # çµæœã‚µãƒãƒªãƒ¼
                self.logger.info(
                    f"âœ… Neo4j update complete:\n"
                    f"   - Updated: {result.get('updated', 0)} edges\n"
                    f"   - Skipped: {result.get('skipped', 0)} edges\n"
                    f"   - Failed: {result.get('failed', 0)} edges"
                )
            
            # å¤±æ•—ç‡ãŒé«˜ã„å ´åˆã¯è­¦å‘Š
                total = result.get('updated', 0) + result.get('failed', 0)
                if total > 0 and result.get('failed', 0) / total > 0.3:
                    self.logger.warning(
                        f"âš ï¸  High failure rate ({result.get('failed', 0)/total:.1%}). "
                        f"Check Neo4j constraints and data format."
                    )
    
        except Exception as e:
            self.logger.error(f"ğŸš¨ Neo4j update failed: {e}")
            raise

    def merge_paths_into_kg(self, kg, documents: List[Document]):
        """
        kg: networkx.Graph (triples turned into nodes/edges)
        documents: the same documents that have metadata['paths'] etc.
        This will:
          - count how many times each entity appears in top-k paths
          - add edge/node attributes: top_path_count, avg_path_length
        """
        from collections import Counter, defaultdict
        path_entity_counts = Counter()
        entity_path_lengths = defaultdict(list)

        for doc in documents:
            paths = doc.metadata.get('paths', [])  # each path is a str like "A -> B -> C" OR list; adapt if needed
            distances = doc.metadata.get('path_distances', [])
            for i, p in enumerate(paths):
                # normalize path representation
                if isinstance(p, str):
                    nodes = [n.strip() for n in p.split('->') if n.strip()]
                elif isinstance(p, (list, tuple)):
                   nodes = list(p)
                else:
                    continue

                dist = distances[i] if i < len(distances) else len(nodes)-1
                for n in nodes:
                    path_entity_counts[n] += 1
                    entity_path_lengths[n].append(dist)

                # if the path describes relations, you could also add edges for consecutive nodes
                for a, b in zip(nodes, nodes[1:]):
                    if kg.has_edge(a, b):
                        # add a path_support counter on existing edge
                        kg[a][b].setdefault('path_support', 0)
                        kg[a][b]['path_support'] += 1
                    else:
                        kg.add_edge(a, b, relation='path_inferred', path_support=1)

        # inject aggregated attrs to nodes
        for n in kg.nodes():
            cnt = path_entity_counts.get(n, 0)
            lens = entity_path_lengths.get(n, [])
            avg_len = sum(lens)/len(lens) if lens else None
            kg.nodes[n]['path_top_count'] = cnt
            if avg_len is not None:
                kg.nodes[n]['path_avg_length'] = avg_len

    def _optimize_graph_rapl(self, kg, documents):
        """
        RAPL æœ€é©åŒ–
        """
    
    # 1. Triples æŠ½å‡º
        doc_triples = {}
        for idx, doc in enumerate(documents):
            triples = doc.metadata.get("triples", [])
            if triples:  # ç©ºãƒªã‚¹ãƒˆã¯é™¤å¤–
                doc_triples[idx] = triples
        
        all_triples = [t for lst in doc_triples.values() for t in lst]
    
        self.logger.info(f"Total triples: {len(all_triples)}")
    
    # Weight æ ¼ç´é ˜åŸŸã®åˆæœŸåŒ–
        for u, v in kg.edges():
            kg[u][v]["intra_raw"] = 0.0
            kg[u][v]["inter_raw"] = 0.0
    
    # 2. Intra: æ–‡æ›¸å†… triple é–“ç›¸äº’ä½œç”¨
        self.logger.info("Computing intra-interactions...")
        intra_collector = ErrorCollector(self.logger)
        intra_edges = 0
    
        for doc_id, triples in doc_triples.items():
            try:
                entities = set()
                for s, _, o in triples:
                    entities.add(s)
                    entities.add(o)
        
        # Triple é–“ã®ç›¸äº’ä½œç”¨ï¼ˆé–¢ä¿‚ã®ç›¸æ€§ã‚’è€ƒæ…®ï¼‰
                for i in range(len(triples)):
                    s1, r1, o1 = triples[i]
                    for j in range(i + 1, len(triples)):
                        s2, r2, o2 = triples[j]
                
                # é–¢ä¿‚ã®ç›¸æ€§
                        try:
                            rel_compat = self._compute_relation_compatibility(r1, r2)
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…±æœ‰åº¦
                            shared = len({s1, o1} & {s2, o2})
                            shared_score = shared * 0.5
                
                # çµ±åˆé‡ã¿
                            w = rel_compat * 0.6 + shared_score * 0.4
                
                            if w > 0.3:
                                if kg.has_edge(s1, o1):
                                    kg[s1][o1]["intra_raw"] += w
                                if kg.has_edge(s2, o2):
                                    kg[s2][o2]["intra_raw"] += w
                            intra_collector.add_success()
                        except Exception as e:
                            intra_collector.add_error(
                                context=f"doc_{doc_id}_triple_{i}_{j}",
                                error=e,
                                triple1=(s1, r1, o1),
                                triple2=(s2, r2, o2)
                            )
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢é–“ã®ã‚¨ãƒƒã‚¸è¿½åŠ 
                for e1 in entities:
                    for e2 in entities:
                        if e1 != e2:
                            try:
                                w = self._compute_intra_weight(e1, e2, triples, kg)
                                if w > 0.5:
                                    if kg.has_edge(e1, e2):
                                        kg[e1][e2]["weight"] = kg[e1][e2].get("weight", 0) + w
                                    else:
                                        kg.add_edge(e1, e2, relation="intra_doc", weight=w)
                                        intra_edges += 1
                            except Exception as e:
                                intra_collector.add_error(
                                    context=f"entity_pair_{e1}_{e2}",
                                    error=e
                                )
            except Exception as e:
                self.logger.error(f"Failed to process document {doc_id}: {type(e).__name__}")
                continue

        intra_collector.report("Intra-document processing", threshold=0.3)
        self.logger.info(f"Added {intra_edges} intra-document edges")
    
    # 3. Inter: å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é«˜é€ŸåŒ–
        self.logger.info("Computing inter-interactions (optimized & sampled)...")
        inter_collector = ErrorCollector(self.logger)
    
    # 3-1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£â†’Triple ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        entity_to_triples = defaultdict(set)
        for idx, (s, r, o) in enumerate(all_triples):
            entity_to_triples[s].add(idx)
            entity_to_triples[o].add(idx)
    
        # 3-2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å‡ºç¾é »åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆä¸Šä½ã®ã¿å‡¦ç†ï¼‰
        entity_freq = [(entity, len(triple_indices)) 
                       for entity, triple_indices in entity_to_triples.items()]
        entity_freq.sort(key=lambda x: x[1], reverse=True)
    
    # ä¸Šä½100ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿å‡¦ç†ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
        max_entities = min(100, len(entity_freq))
        top_entities = set(entity for entity, _ in entity_freq[:max_entities])
    
        self.logger.info(
            f"  Sampled {max_entities}/{len(entity_to_triples)} entities "
            f"(covering {sum(freq for _, freq in entity_freq[:max_entities])} triples)"
        )
    
    # 3-2. å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹ Triple ãƒšã‚¢ã®ã¿è¨ˆç®—
        seen_pairs = set()
        inter_count = 0
    
        for _entity, triple_indices in entity_to_triples.items():
            if _entity not in top_entities:
                continue  # ä¸Šä½ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(triple_indices) < 3:
                continue  
        
            indices = list(triple_indices)
            for i in range(len(indices)):
                for j in range(i + 1, len(indices)):
                    idx1, idx2 = indices[i], indices[j]
                    pair = (min(idx1, idx2), max(idx1, idx2))
                    if pair in seen_pairs:
                        continue
                    seen_pairs.add(pair)
                
                # é‡ã¿è¨ˆç®—
                    try:
                        t1 = all_triples[idx1]
                        t2 = all_triples[idx2]
                        w = self._compute_inter_weight(t1, t2, kg=kg)
                
                        if w > self.config['relation_compat_threshold']: 
                            s1, _, o1 = t1
                            s2, _, o2 = t2
                    
                    # åŒæ–¹å‘ã«é‡ã¿ã‚’åŠ ç®—
                            if kg.has_edge(s1, o1):
                                kg[s1][o1]["inter_raw"] = kg[s1][o1].get("inter_raw", 0.0) + w
                            if kg.has_edge(s2, o2):
                                kg[s2][o2]["inter_raw"] = kg[s2][o2].get("inter_raw", 0.0) + w
                    
                            inter_count += 1
                        inter_collector.add_success()

                    except Exception as e:
                        inter_collector.add_error(
                            context=f"triple_pair_{idx1}_{idx2}",
                            error=e
                        )

        inter_collector.report("Inter-document processing", threshold=0.3)
        self.logger.info(f"Added {inter_count} meaningful inter-interactions")
    
    # 4. Document-level linking
        self.logger.info("Computing document-level connections...")
    
        try:
            entity_docs = {}
            for doc_id, triples in doc_triples.items():
                for s, _, o in triples:
                    entity_docs.setdefault(s, set()).add(doc_id)
                    entity_docs.setdefault(o, set()).add(doc_id)
    
            doc_pairs = {}
            bridge_entities = []
    
            for entity_name, doc_set in entity_docs.items():
                if len(doc_set) > 1:
                    docs = list(doc_set)
                    for i, d1 in enumerate(docs):
                        for d2 in docs[i+1:]:
                            pair = (d1, d2)
                            doc_pairs[pair] = doc_pairs.get(pair, 0) + 1
            
                    if len(doc_set) > 2:
                        bridge_entities.append((entity_name, len(doc_set)))

                # ãƒ–ãƒªãƒƒã‚¸ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ­ã‚°
            if bridge_entities:
                bridge_entities.sort(key=lambda x: x[1], reverse=True)
                self.logger.info("Top bridge entities:")
                for entity_name, count in bridge_entities[:5]:
                    self.logger.info(f"  '{entity_name}': {count} documents")

            inter_doc_count = 0
            for (d1, d2), ct in doc_pairs.items():
                if ct > 2:
                    n1 = f"doc_{d1}"
                    n2 = f"doc_{d2}"
                
                    if not kg.has_node(n1):
                        kg.add_node(n1, type="document")
                    if not kg.has_node(n2):
                        kg.add_node(n2, type="document")
                
                    kg.add_edge(n1, n2, relation="inter_doc", weight=ct)
                    inter_doc_count += 1
            self.logger.info(f"Added {inter_doc_count} inter-document links")

        except Exception as e:
            self.logger.error(f"Document linking failed: {type(e).__name__} - {str(e)[:100]}")

            kg = self._normalize_edge_weights(kg, doc_triples, method='minmax')
            self.logger.info("Finalizing edge weights with normalization...")
    
            for u, v, d in kg.edges(data=True):
                intra = d.get("intra_normalized", d.get("intra_raw", 0.0))
                inter = d.get("inter_normalized", d.get("inter_raw", 0.0))
        
        # RAPLè«–æ–‡: intraé‡è¦– + interè£œå®Œ
                d["weight"] = min(0.7 * intra + 0.3 * inter, 1.0)    
    
            self.logger.info(f"Weight calculation complete: {len(kg.edges())} edges")
            return kg

    def _normalize_edge_weights(
        self,
        kg: nx.Graph,
        doc_triples: Dict[int, List[Tuple]],
        method: str = 'minmax'
    ) -> nx.Graph:
        """
        ã‚¨ãƒƒã‚¸é‡ã¿ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«æ­£è¦åŒ–
        
        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            doc_triples: {doc_id: [(s, r, o), ...]} ã®è¾æ›¸
            method: 'minmax' ã¾ãŸã¯ 'zscore'
        
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•
        """
        self.logger.info(f"Normalizing edge weights (method={method})...")
        
        # ============================================================
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«é‡ã¿ã‚’åé›†
        # ============================================================
        doc_edge_weights = defaultdict(lambda: {'intra': [], 'inter': []})
        edge_to_docs = defaultdict(set)  # ã‚¨ãƒƒã‚¸ãŒã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹
        
        for doc_id, triples in doc_triples.items():
            doc_entities = set()
            for s, _, o in triples:
                doc_entities.add(s)
                doc_entities.add(o)
            
            # ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢é€£ã™ã‚‹ã‚¨ãƒƒã‚¸ã‚’æ¢ã™
            for u, v, data in kg.edges(data=True):
                if u in doc_entities or v in doc_entities:
                    edge_key = (u, v)
                    edge_to_docs[edge_key].add(doc_id)
                    
                    intra_raw = data.get('intra_raw', 0.0)
                    inter_raw = data.get('inter_raw', 0.0)
                    
                    if intra_raw > 0:
                        doc_edge_weights[doc_id]['intra'].append(intra_raw)
                    if inter_raw > 0:
                        doc_edge_weights[doc_id]['inter'].append(inter_raw)
        
        # ============================================================
        # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
        # ============================================================
        norm_params = {}
        
        for doc_id, weights in doc_edge_weights.items():
            params = {}
            
            for weight_type in ['intra', 'inter']:
                values = weights[weight_type]
                
                if not values:
                    params[weight_type] = None
                    continue
                
                if method == 'minmax':
                    min_val = min(values)
                    max_val = max(values)
                    params[weight_type] = {
                        'min': min_val,
                        'max': max_val,
                        'range': max_val - min_val
                    }
                
                elif method == 'zscore':
                    mean_val = np.mean(values)
                    std_val = np.std(values)
                    params[weight_type] = {
                        'mean': mean_val,
                        'std': std_val if std_val > 0 else 1.0
                    }
            
            norm_params[doc_id] = params
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self._log_normalization_stats(doc_edge_weights, norm_params)
        
        # ============================================================
        # 3. ã‚¨ãƒƒã‚¸ã”ã¨ã«æ­£è¦åŒ–ã‚’é©ç”¨
        # ============================================================
        normalized_count = 0
        
        for u, v, data in kg.edges(data=True):
            edge_key = (u, v)
            related_docs = edge_to_docs.get(edge_key, set())
            
            if not related_docs:
                continue
            
            # ã“ã®ã‚¨ãƒƒã‚¸ã«é–¢é€£ã™ã‚‹å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ­£è¦åŒ–å€¤ã‚’å¹³å‡
            intra_normalized = []
            inter_normalized = []
            
            for doc_id in related_docs:
                if doc_id not in norm_params:
                    continue
                
                params = norm_params[doc_id]
                intra_raw = data.get('intra_raw', 0.0)
                inter_raw = data.get('inter_raw', 0.0)
                
                # Intraæ­£è¦åŒ–
                if params['intra'] and intra_raw > 0:
                    norm_val = self._normalize_value(
                        intra_raw,
                        params['intra'],
                        method
                    )
                    intra_normalized.append(norm_val)
                
                # Interæ­£è¦åŒ–
                if params['inter'] and inter_raw > 0:
                    norm_val = self._normalize_value(
                        inter_raw,
                        params['inter'],
                        method
                    )
                    inter_normalized.append(norm_val)
            
            # æ­£è¦åŒ–å¾Œã®å€¤ã‚’å¹³å‡
            if intra_normalized:
                data['intra_normalized'] = np.mean(intra_normalized)
                normalized_count += 1
            else:
                data['intra_normalized'] = data.get('intra_raw', 0.0)
            
            if inter_normalized:
                data['inter_normalized'] = np.mean(inter_normalized)
            else:
                data['inter_normalized'] = data.get('inter_raw', 0.0)
        
        self.logger.info(f"  â†’ Normalized {normalized_count} edges")
        
        return kg
    
    def _normalize_value(
        self,
        value: float,
        params: dict,
        method: str
    ) -> float:
        """
        å˜ä¸€ã®å€¤ã‚’æ­£è¦åŒ–
        
        Args:
            value: æ­£è¦åŒ–ã™ã‚‹å€¤
            params: æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            method: 'minmax' ã¾ãŸã¯ 'zscore'
        
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸå€¤
        """
        if method == 'minmax':
            min_val = params['min']
            max_val = params['max']
            range_val = params['range']
            
            if range_val < 1e-9:
                return 0.5  # å…¨ã¦åŒã˜å€¤ã®å ´åˆã¯ä¸­é–“å€¤
            
            # [0, 1] ã«æ­£è¦åŒ–
            normalized = (value - min_val) / range_val
            return max(0.0, min(1.0, normalized))
        
        elif method == 'zscore':
            mean_val = params['mean']
            std_val = params['std']
            
            # z-scoreã‚’è¨ˆç®—å¾Œã€sigmoidã§ [0, 1] ã«å¤‰æ›
            z = (value - mean_val) / std_val
            sigmoid = 1 / (1 + np.exp(-z))
            return sigmoid
        
        return value
    
    def _log_normalization_stats(
        self,
        doc_edge_weights: dict,
        norm_params: dict
    ):
        """æ­£è¦åŒ–çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›"""
        self.logger.info("  Normalization statistics:")
        
        for doc_id in list(norm_params.keys())[:3]:  # æœ€åˆã®3ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            params = norm_params[doc_id]
            
            intra_weights = doc_edge_weights[doc_id]['intra']
            inter_weights = doc_edge_weights[doc_id]['inter']
            
            if intra_weights:
                self.logger.info(
                    f"    Doc {doc_id} intra: "
                    f"min={min(intra_weights):.3f}, "
                    f"max={max(intra_weights):.3f}, "
                    f"mean={np.mean(intra_weights):.3f}"
                )
            
            if inter_weights:
                self.logger.info(
                    f"    Doc {doc_id} inter: "
                    f"min={min(inter_weights):.3f}, "
                    f"max={max(inter_weights):.3f}, "
                    f"mean={np.mean(inter_weights):.3f}"
                )

    # 5. çµ±åˆé‡ã¿
        kg = self._normalize_edge_weights(kg, doc_triples, method='minmax')
        self.logger.info("Finalizing edge weights with normalization...")
    
        for u, v, d in kg.edges(data=True):
            intra = d.get("intra_normalized", d.get("intra_raw", 0.0))
            inter = d.get("inter_normalized", d.get("inter_raw", 0.0))
        
        # RAPLè«–æ–‡: intraé‡è¦– + interè£œå®Œ
            d["weight"] = min(0.7 * intra + 0.3 * inter, 1.0)    
        self.logger.info(f"Weight calculation complete: {len(kg.edges())} edges")
        return kg
    
    def _group_triples_by_document(self, kg, documents):
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’Documentåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        # ç°¡æ˜“å®Ÿè£…: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®š
        doc_triples = {}
        
        for i, doc in enumerate(documents):
            doc_id = f"doc_{i}"
            doc_triples[doc_id] = []
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒDocumentå†…ã«å‡ºç¾ã™ã‚‹ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º
            for s, o, data in kg.edges(data=True):
                if s in doc.text or o in doc.text:
                    doc_triples[doc_id].append((s, data.get('relation', ''), o))
        return doc_triples
    
    def _compute_intra_weight(self, e1: str, e2: str, triples: List, kg=None) -> float:
        """
        åŒä¸€Documentå†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“é‡ã¿è¨ˆç®—
        
        Args:
            e1, e2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å
            triples: (s, r, o) ã®ãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
    # ------------------------------------------------------------
    # 1) å…±èµ·é »åº¦ï¼ˆåŸºæœ¬ï¼‰
    # ------------------------------------------------------------
        cooccur = sum(
            1 for s, _, o in triples
            if (s == e1 and o == e2) or (s == e2 and o == e1)
        )
        co_norm = min(cooccur / 5.0, 1.0)   # æ­£è¦åŒ–

    # ------------------------------------------------------------
    # 2) é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§
    # ------------------------------------------------------------
        rel_pairs = [
            (r, True) for s, r, o in triples
            if (s == e1 and o == e2)
        ] + [
            (r, False) for s, r, o in triples
            if (s == e2 and o == e1)  # é€†å‘ã
        ]
    
        if not rel_pairs:
            rel_bonus = 0.0
        else:
            # é–¢ä¿‚ã®å¤šæ§˜æ€§
            unique_rels = set(r for r, _ in rel_pairs)
            diversity_bonus = min(len(unique_rels) * 0.2, 0.6)

        # æ–¹å‘ã®ä¸€è²«æ€§ï¼ˆåŒã˜å‘ããŒå¤šã„ã»ã©å¼·ã„é–¢ä¿‚ï¼‰
            same_direction_count = sum(1 for _, is_forward in rel_pairs if is_forward)
            opposite_direction_count = len(rel_pairs) - same_direction_count

        # é–¢ä¿‚ã®è³ªï¼ˆåŒã˜å‘ãã‹é€†å‘ãã‹ã§è©•ä¾¡ï¼‰
            if same_direction_count > opposite_direction_count:
                direction_score = same_direction_count / len(rel_pairs)
            else:
            # é€†æ–¹å‘ãŒå¤šã„ = åŒæ–¹å‘ã®é–¢ä¿‚ï¼ˆã“ã‚Œã‚‚æœ‰ç”¨ï¼‰
                direction_score = 0.7  # ã‚„ã‚„é«˜ã‚ã«è©•ä¾¡

            rel_bonus = diversity_bonus * 0.5 + direction_score * 0.5

    # ------------------------------------------------------------
    # 3) ãƒ‘ã‚¹ã‚µãƒãƒ¼ãƒˆï¼ˆkgã« path_support ãŒã‚ã‚Œã°ï¼‰
    # ------------------------------------------------------------
        path_bonus = 0.0
        if kg is not None and kg.has_edge(e1, e2):
            path_bonus = min(kg[e1][e2].get("path_support", 0) * 0.1, 0.5)

    # ------------------------------------------------------------
    # 4) åˆæˆ
    # ------------------------------------------------------------
        weight = co_norm * 0.5 + rel_bonus * 0.4 + path_bonus * 0.1
        return min(weight, 1.0)


    def _compute_inter_weight(self, t1: tuple, t2: tuple, kg=None): 
        """inter-triple interaction weightè¨ˆç®—"""

        s1, r1, o1 = t1
        s2, r2, o2 = t2

        # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆæœ€é‡è¦ï¼‰
        shared = len({s1, o1} & {s2, o2})
        shared_bonus = min(shared * 0.5, 1.0)
        # é–¢ä¿‚ã®ç›¸æ€§è¨ˆç®—
        rel_compatibility = safe_execute(
            self._compute_relation_compatibility,
            args=(r1, r2),
            default=0.3,  
            logger=self.logger,
            context=f"relation_compatibility({r1}, {r2})"
        )
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
        sim_bonus = 0.0
        try:        
            e1 = self.get_cached_embedding(s1, cache_type='entity')
            e2 = self.get_cached_embedding(s2, cache_type='entity')

            # æ­£è¦åŒ–æ¸ˆã¿ãªã®ã§ç›´æ¥å†…ç©ã‚’è¨ˆç®—
            sim = float(np.dot(e1, e2))
            sim_bonus = max(sim, 0) * 0.3

        except Exception as e:
            if not hasattr(self, '_embedding_error_warned'):
                    self.logger.warning(f"âš ï¸  Embedding similarity errors detected")
                    self._embedding_error_warned = True
            
        # 3) graph path-based supportï¼ˆkgãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
        path_bonus = 0.0
        if kg is not None:
            try:
            # 2-hopä»¥å†…ã§ã¤ãªãŒã£ã¦ãŸã‚‰è©•ä¾¡        
                if kg.has_node(s1) and kg.has_node(s2):
                    length = nx.shortest_path_length(kg, s1, s2)
                    if length <= 2:
                        path_bonus = 0.3 * (1.0 - length / 3.0)  # è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            except nx.NetworkXNoPath:
                pass
            except nx.NodeNotFound:
                if self.logger.level <= logging.DEBUG:
                    self.logger.debug(f"Node not found in graph: {s1} or {s2}")
            except Exception as e:
                if self.logger.level <= logging.DEBUG:
                    self.logger.debug(f"Path calc failed ({s1}->{s2}): {type(e).__name__}")

        # 4) ç·åˆ
        w = (
            shared_bonus * 0.4 +       # å…±æœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            rel_compatibility * 0.3 +   # é–¢ä¿‚ã®ç›¸æ€§ï¼ˆã“ã“ã«çµ±åˆæ¸ˆã¿ï¼‰
            sim_bonus * 0.2 +           # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
            path_bonus * 0.1            # ãƒ‘ã‚¹è·é›¢
        )

        return min(w, 1.0)
    
    def _compute_relation_compatibility(self, r1: str, r2: str) -> float:
        """
        é–¢ä¿‚ã®ç›¸æ€§ã‚¹ã‚³ã‚¢
        """
        # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢çµ±ä¸€ï¼‰
        r1 = r1.lower().replace('-', '_')
        r2 = r2.lower().replace('-', '_')
    # 1. å®Œå…¨ä¸€è‡´
        if r1 == r2:
            return 1.0
    
    # 2. é€†é–¢ä¿‚ã®ãƒšã‚¢ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        inverse_pairs = {
            ("cause_of", "caused_by"),
            ("cause_of", "effect_of"), 
            ("part_of", "has_part"),
            ("component_of", "has_component"),
            ("parent_of", "child_of"),
            ("author_of", "written_by"),
            ("owns", "owned_by"),
            ("manages", "managed_by"),
            ("teaches", "taught_by"),
            ("supervises", "supervised_by"),
        }
    
        if (r1, r2) in inverse_pairs or (r2, r1) in inverse_pairs:
            return 0.9
    
    # 3. é–¢é€£ã™ã‚‹é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        related_groups = [
        # å› æœé–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "cause_of", "caused_by", "leads_to", "results_in", 
                "triggers", "produces", "generates", "effect_of"
            },
        
        # æ§‹æˆè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "part_of", "has_part", "component_of", "has_component",
                "contains", "includes", "consists_of", "comprises"
            },
        
        # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "member_of", "has_member", "belongs_to", "works_at", 
                "employed_by", "affiliated_with"
            },
        
        # æ™‚é–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "before", "after", "during", "precedes", "follows",
                "happens_before", "happens_after"
            },
        
        # ç©ºé–“é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "located_in", "location_of", "near", "adjacent_to",
                "contains", "inside", "outside"
            },
        
        # å±æ€§ãƒ»æ€§è³ªã‚°ãƒ«ãƒ¼ãƒ—
            {
                "is_a", "type_of", "instance_of", "has_property",
                "characterized_by", "defined_by"
            },
        
        # ç›¸äº’ä½œç”¨ã‚°ãƒ«ãƒ¼ãƒ—
            {
                "interacts_with", "collaborates_with", "competes_with",
                "influences", "affected_by"
            },
        ]
    
        for group in related_groups:
            if r1 in group and r2 in group:
                return 0.7
    
    # 4. åŒã˜ã‚«ãƒ†ã‚´ãƒªï¼ˆå‹•è©ã®æ€§è³ªã§åˆ¤å®šï¼‰
    # ä¾‹: action ç³»ã€state ç³»ãªã©
        action_verbs = {
            "creates", "builds", "develops", "produces", "makes",
            "constructs", "designs", "implements", "generates",
            "enables", "powers", "leverages", "accelerates"
            # ï¼ˆML/AIå°‚é–€ï¼‰
            "utilizes", "parameterizes", "fine_tunes", "approximates",
            "encodes", "regularizes", "iterates", "optimizes",
            "traverses", "samples", "augments", "normalizes",
            "quantizes", "distills", "ensembles", "prunes",
            "compresses", "aggregates", "fuses", "aligns",
            "projects", "embeds", "transforms", "adapts",
            
            # CVç³»
            "detects", "segments", "classifies", "recognizes",
            "extracts", "filters", "convolves", "pools",
            
            # NLPç³»
            "tokenizes", "parses", "generates_text", "translates",
            "attends_to", "masks", "predicts",
            
            # Graphç³»
            "propagates", "aggregates_neighbors", "diffuses",
            "clusters", "partitions", "samples_neighbors"
        }
    
        state_verbs = {
            "is", "has", "contains", "includes", "comprises",
            "exists", "represents", "defines", "consists_of",
            "maintains", "preserves", "exhibits", "displays"
        }
    
        relation_verbs = {
            "relates_to", "associated_with", "connected_to",
            "linked_to", "corresponds_to", "depends_on",
            "derived_from", "based_on", "inspired_by"
        }

        # --- 3-4. è¨ˆç®—å‹•è© ---
        computational_verbs = {
            "computes", "calculates", "evaluates", "measures",
            "estimates", "infers", "learns", "trains",
            "updates", "backpropagates", "forward_passes"
        }
        
        # --- 3-5. æ¯”è¼ƒå‹•è© ---
        comparison_verbs = {
            "outperforms", "surpasses", "exceeds", "improves_upon",
            "compares_to", "contrasts_with", "benchmarks_against"
        }

            # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°
        verb_categories = [
            action_verbs,
            state_verbs,
            relation_verbs,
            computational_verbs,
            comparison_verbs
        ]
                
        for category in verb_categories:
            if r1 in category and r2 in category:
                return 0.5
    
    # 5. åŸ‹ã‚è¾¼ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
        try:
            emb1 = self.relation_embedder.get_text_embedding(r1)
            emb2 = self.relation_embedder.get_text_embedding(r2)
            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)
            return max(0.3, float(sim))
        except Exception:
            return 0.3

    def _update_neo4j_structure(self, kg, graph_store):
        """
        Neo4jæ›´æ–°
        """        
        batch_query  = """
        UNWIND $batch AS row
        MERGE (a:Concept {name: row.source})
        MERGE (b:Concept {name: row.target})
        MERGE (a)-[r:RELATED]->(b)
        ON CREATE SET r.weight = row.weight
        ON MATCH SET r.weight = row.weight
        """
        collector = ErrorCollector(self.logger)

        batch = []
        batch_size = 1000  # 1000ä»¶ã”ã¨ã«é€ä¿¡
        
        for s, o, data in kg.edges(data=True):
            weight = data.get('weight', 0.0)

            if weight <= self.config['final_weight_cutoff']: 
                collector.add_skip()
                continue 
            
            # ãƒãƒƒãƒã«è¿½åŠ 
            batch.append({
                'source': s,
                'target': o,
                'weight': float(weight)
            })
        
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰é€ä¿¡
            if len(batch) >= batch_size:

                try:
                    graph_store.query(batch_query, {'batch': batch})
                    collector.add_success(count=len(batch))
                
                    self.logger.debug(f"  Sent batch of {len(batch)} edges")
                    batch = []  # ãƒãƒƒãƒã‚’ã‚¯ãƒªã‚¢
            
                except Exception as e:
                    collector.add_error(
                        context=f"batch_{len(batch)}_edges",
                        error=e
                    )
                # å¤±æ•—ã—ãŸãƒãƒƒãƒã¯ç ´æ£„ï¼ˆã¾ãŸã¯å€‹åˆ¥å‡¦ç†ï¼‰
                    batch = []
    
    # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’é€ä¿¡
        if batch:
            try:
                graph_store.query(batch_query, {'batch': batch})
                collector.add_success(count=len(batch))
                self.logger.debug(f"  Sent final batch of {len(batch)} edges")
        
            except Exception as e:
                collector.add_error(
                    context=f"final_batch_{len(batch)}_edges",
                    error=e
                )
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆè‡ªå‹•ã§ãƒ­ã‚°å‡ºåŠ›ï¼‰
        collector.report("Neo4j edge update", threshold=0.3)
    # æˆ»ã‚Šå€¤ã‚‚å–å¾—å¯èƒ½
        return collector.get_summary()        

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Crystal Cluster beta')
    parser.add_argument('json_file', help='Clean documents JSON file')
    parser.add_argument('--neo4j-uri', default='bolt://localhost:7687')
    parser.add_argument('--neo4j-user', default='neo4j')
    parser.add_argument('--neo4j-pass', required=True)
    parser.add_argument('--dual-chunk', action='store_true', help='Enable dual-chunk mode')
    parser.add_argument('--test-query', help='Test retrieval with a query')
    parser.add_argument('--debug', action='store_true')
    parser.add_argument(
        '--llm',
        choices=['gpt-4o-mini', 'claude-3.5-sonnet'],
        default='gpt-4o-mini',
        help='LLM model for triplet extraction'
    )
 # Self-RAGå¼•æ•°
    parser.add_argument(
        '--enable-self-rag',
        action='store_true',
        help='Enable Self-RAG for triplet refinement'
    )
    parser.add_argument(
        '--self-rag-threshold',
        type=float,
        default=0.5,
        help='Self-RAG confidence threshold (default: 0.5)'
    )
    parser.add_argument(
        '--self-rag-refiner',
        choices=['gpt-4o-mini', 'gpt-4o', 'claude-3.5-sonnet'],
        default='gpt-4o',
        help='LLM model for Self-RAG refinement (default: gpt-4o)'
    )
    parser.add_argument(
        '--enable-duplicate-check',
        action='store_true',
        default=True,
        help='Enable duplicate detection (default: enabled)'
    )
    parser.add_argument(
        '--no-duplicate-check',
        dest='enable_duplicate_check',
        action='store_false',
        help='Disable duplicate detection'
    )
    parser.add_argument(
        '--duplicate-similarity',
        type=float,
        default=0.85,
        help='Similarity threshold for fuzzy duplicate detection (default: 0.85)'
    )

    args = parser.parse_args()
    
    print("â„ï¸ Crystal Cluster beta")
    print(f"ğŸ¤– LLM: {args.llm}")
    if args.dual_chunk:
        print("ğŸ”€ Dual-chunk mode enabled")   
    if args.enable_self_rag:
        print(f"ğŸ”„ Self-RAG enabled (refiner: {args.self_rag_refiner})")         
    print("â”" * 42)

    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®æ§‹ç¯‰
    custom_config = {}
    
    if args.llm != 'gpt-4o-mini':
        custom_config['llm_model'] = args.llm

    # Self-RAGè¨­å®š
    if args.enable_self_rag:
        custom_config['enable_self_rag'] = True
        custom_config['self_rag_confidence_threshold'] = args.self_rag_threshold
        custom_config['self_rag_refiner_model'] = args.self_rag_refiner
    else:
        custom_config['enable_self_rag'] = False

    cluster = CrystalCluster(
        log_level=logging.DEBUG if args.debug else logging.INFO,
        use_dual_chunk=args.dual_chunk,
        custom_config=custom_config if custom_config else None
    )
    documents = cluster.load_documents(args.json_file)
    
    graph_store = Neo4jGraphStore(
        username=args.neo4j_user,
        password=args.neo4j_pass,
        url=args.neo4j_uri
    )

    if args.dual_chunk:
        # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰
        result = cluster.commit_to_graph_with_retrieval(documents, graph_store)
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªãŒã‚ã‚Œã°æ¤œç´¢
        if args.test_query:
            print(f"\nğŸ” Testing retrieval: '{args.test_query}'")

            hits = cluster.retrieve(
                result['retrieval_store'], 
                args.test_query, 
                top_k=3,
                chunk_mapping=result.get('chunk_mapping')
            )
            if not hits:
                print("  âš ï¸  No results found")
            else:        
            
                for i, (score, doc, graph_chunk_ids) in enumerate(hits, 1):
                    print(f"\n{i}. Score: {score:.3f}")
                    print(f"   Text: {doc.text[:150]}...")
                    
                                # è¿½åŠ æƒ…å ±ã‚‚è¡¨ç¤º
                    if graph_chunk_ids:
                        print(f"   ({len(graph_chunk_ids)} graph chunks linked)")
                        if len(graph_chunk_ids) > 3:
                            print(f"   ... and {len(graph_chunk_ids) - 3} more")
                    else:
                        print(f"   Graph chunks: (none)")

    else:

        cluster.commit_to_graph(documents, graph_store)
    
    print("\nâœ¨ Complete!")