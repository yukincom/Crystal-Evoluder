"""
ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹
"""
import numpy as np
from typing import List, Tuple, Dict, Any, Optional

from ..shared import safe_execute

class TripletFilter:
    """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®å“è³ªç®¡ç†ã¨Self-RAGã‚’æ‹…å½“"""

    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger
        self.total_self_rag_tokens = 0

        # ğŸ”§ è¿½åŠ : åŸºæœ¬ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
        self.mode = config.get('mode', 'api')
    
        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸåŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        if self.mode == 'api':
            self.base_model = config.get('api_model', 'gpt-4o-mini')
        else:
            self.base_model = config.get('ollama_model', '')
    
        # Self-RAGç”¨ãƒ¢ãƒ‡ãƒ«
        self.critic_model = config.get('self_rag_critic_model') or self.base_model
        self.refiner_model = config.get('self_rag_refiner_model') or self.base_model
    
        self.logger.info(f"TripletFilter initialized:")
        self.logger.info(f"  Base model: {self.base_model}")
        self.logger.info(f"  Critic model: {self.critic_model}")
        self.logger.info(f"  Refiner model: {self.refiner_model}")

        # é–¢ä¿‚ã‚¿ã‚¤ãƒ—ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        self.relation_blacklist = {
            'is', 'has', 'are', 'was', 'were',
            'the', 'a', 'an',
            'of', 'in', 'on', 'at',
        }
    # BGE-M3ã‚’ã‚¯ãƒ©ã‚¹ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å›ã ã‘ï¼‰
        self.embedder = None
        self.blacklist_embs = None
        self.useful_rel_embs = None

        if hasattr(self, 'embedding_cache') and self.embedding_cache is not None:
            try:
                self.blacklist_embs = [
                    self.embedding_cache.get_embedding(rel.lower())
                    for rel in self.relation_blacklist
                ]

                self.logger.info("âœ… Blacklist embeddings precomputed with BGE-M3 cache")

                # æœ‰ç”¨é–¢ä¿‚ãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼šè«–æ–‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹é–¢ä¿‚ã‚’è¿½åŠ ï¼‰
                useful_relations = [
                    "causes", "affects", "treats", "indicates",
                    "associated_with", "correlates_with", "leads_to"
                ]
                self.useful_rel_embs = [
                    self.embedding_cache.get_embedding(rel.lower())
                    for rel in useful_relations
                ]

                self.logger.info("âœ… Useful relation embeddings precomputed with BGE-M3 cache")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Precomputing embeddings failed: {e}")

    def filter_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        quality_threshold: float = 0.3
    ) -> Tuple[List[Tuple], List[Tuple], Dict]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å“è³ªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        Args:
            triplets: [(subject, relation, object), ...] ã®ãƒªã‚¹ãƒˆ
            quality_threshold: å“è³ªã‚¹ã‚³ã‚¢ã®é–¾å€¤ï¼ˆ0.0~1.0ï¼‰

        Returns:
            (filtered_triplets, rejected_triplets, stats)
        """
        self.logger.info(f"ğŸ” Filtering {len(triplets)} triplets...")

        filtered = []
        rejected = []
        quality_scores = []

        for s, r, o in triplets:
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            score = self._compute_triplet_quality(s, r, o)
            quality_scores.append(score)

            if score >= quality_threshold:
                filtered.append((s, r, o))
            else:
                rejected.append((s, r, o))
                self.logger.debug(
                    f"  Rejected: ({s}, {r}, {o}) [score={score:.2f}]"
                )

        # çµ±è¨ˆæƒ…å ±
        stats = {
            'original': len(triplets),
            'filtered': len(filtered),
            'rejected': len(rejected),
            'avg_quality': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'rejection_rate': len(rejected) / len(triplets) if triplets else 0
        }

        self.logger.info(
            f"  â†’ Kept {len(filtered)} triplets, "
            f"rejected {len(rejected)} ({stats['rejection_rate']:.1%})"
        )
        self.logger.info(f"  â†’ Avg quality: {stats['avg_quality']:.2f}")

        return filtered, rejected, stats

    def self_rag_triplets(
        self,
        triplets: List[Tuple[str, str, str]],
        chunk_text: str,
        ai_router
    ) -> Tuple[List[Tuple], Dict]:
        """
        Self-RAG: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’è©•ä¾¡ã—ã€ä½å“è³ªãªã‚‚ã®ã‚’å†ç”Ÿæˆ

        """
        if not self.config.get('enable_self_rag', False):
            return triplets, {'self_rag_applied': False}

        # ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—ãƒã‚§ãƒƒã‚¯
        token_budget = self.config.get('self_rag_token_budget', 100000)

        if self.total_self_rag_tokens >= token_budget:
            self.logger.warning(
                f"âš ï¸  Self-RAG token budget exhausted "
                f"({self.total_self_rag_tokens}/{token_budget}), skipping"
            )
            return triplets, {
                'self_rag_applied': False,
                'budget_exhausted': True
            }

        self.logger.info(f"ğŸ”„ Self-RAG: Evaluating {len(triplets)} triplets...")

        # Critic: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’è©•ä¾¡
        evaluated_triplets = []

        for s, r, o in triplets:
            confidence = self._critic_evaluate_triplet(s, r, o, chunk_text)
            evaluated_triplets.append({
                'triplet': (s, r, o),
                'confidence': confidence,
                'needs_refinement': confidence < self.config['self_rag_confidence_threshold']
            })

        # çµ±è¨ˆ
        low_confidence_count = sum(1 for t in evaluated_triplets if t['needs_refinement'])
        avg_confidence = np.mean([t['confidence'] for t in evaluated_triplets])

        self.logger.info(
            f"  â†’ Avg confidence: {avg_confidence:.2f}, "
            f"Low confidence: {low_confidence_count}/{len(triplets)}"
        )

        # Refiner: ä½å“è³ªãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ
        refined_triplets = []
        refinement_stats = {
            'attempted': 0,
            'succeeded': 0,
            'failed': 0,
            'tokens_used': 0
        }

        for triplet_info in evaluated_triplets:
            # äºˆç®—ãƒã‚§ãƒƒã‚¯
            if self.total_self_rag_tokens >= token_budget:
                self.logger.info("  â†’ Budget limit reached, stopping refinement")
                # æ®‹ã‚Šã¯å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ä¿æŒ
                refined_triplets.append(triplet_info['triplet'])
                continue

            if triplet_info['needs_refinement']:
                # å†ç”Ÿæˆã‚’è©¦ã¿ã‚‹
                refined, tokens_used = self._refiner_regenerate_triplet(
                    triplet_info['triplet'],
                    chunk_text,
                    ai_router
                )

                refinement_stats['attempted'] += 1
                refinement_stats['tokens_used'] += tokens_used
                self.total_self_rag_tokens += tokens_used

                if refined:
                    # å†è©•ä¾¡
                    s, r, o = refined
                    new_confidence = self._critic_evaluate_triplet(s, r, o, chunk_text)

                    if new_confidence > triplet_info['confidence']:
                        # æ”¹å–„ã•ã‚ŒãŸå ´åˆã¯ç½®ãæ›ãˆ
                        refined_triplets.append(refined)
                        refinement_stats['succeeded'] += 1

                        self.logger.debug(
                            f"  âœ“ Refined: {triplet_info['triplet']} â†’ {refined} "
                            f"(confidence: {triplet_info['confidence']:.2f} â†’ {new_confidence:.2f})"
                        )
                    else:
                        # æ”¹å–„ã•ã‚Œãªã‹ã£ãŸå ´åˆã¯å…ƒã‚’ä¿æŒ
                        refined_triplets.append(triplet_info['triplet'])
                        refinement_stats['failed'] += 1
                else:
                    # å†ç”Ÿæˆå¤±æ•—æ™‚ã¯å…ƒã‚’ä¿æŒ
                    refined_triplets.append(triplet_info['triplet'])
                    refinement_stats['failed'] += 1
            else:
                # é«˜å“è³ªãªã‚‚ã®ã¯ãã®ã¾ã¾
                refined_triplets.append(triplet_info['triplet'])

        # ============================================================
        # 3. Validator: æœ€çµ‚æ¤œè¨¼
        # ============================================================
        validated_triplets = self._validator_check_consistency(
            refined_triplets,
            chunk_text
        )

        # çµ±è¨ˆæƒ…å ±
        stats = {
            'self_rag_applied': True,
            'original_count': len(triplets),
            'refined_count': len(validated_triplets),
            'avg_confidence': float(avg_confidence),
            'low_confidence_count': low_confidence_count,
            'refinement_stats': refinement_stats,
            'total_tokens_used': self.total_self_rag_tokens
        }

        self.logger.info(
            f"  â†’ Self-RAG complete: "
            f"{refinement_stats['succeeded']} improved, "
            f"{refinement_stats['failed']} kept original"
            f"tokens: {refinement_stats['tokens_used']}"
        )

        return validated_triplets, stats

    def _critic_evaluate_triplet(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str
    ) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®å“è³ªã‚’è©•ä¾¡ï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: 0.0ï½1.0ï¼‰

        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©é«˜å“è³ªï¼‰
        """
        scores = []

        # ============================================================
        # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å“è³ªï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
        # ============================================================
        entity_score = self._score_entities(subject, object_)
        scores.append(('entity', entity_score, 0.3))

        # ============================================================
        # 2. é–¢ä¿‚ã®æ˜ç¢ºæ€§ï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
        # ============================================================
        relation_score = self._score_relation(relation)
        scores.append(('relation', relation_score, 0.3))

        # ============================================================
        # 3. æ–‡æ³•çš„æ­£ã—ã•ï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼‰
        # ============================================================
        grammar_score = self._score_grammar(subject, relation, object_)
        scores.append(('grammar', grammar_score, 0.2))

        # ============================================================
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®æ•´åˆæ€§ï¼ˆæ–°è¦ï¼‰
        # ============================================================
        context_score = self._score_context_alignment(
            subject, relation, object_, context
        )
        scores.append(('context', context_score, 0.2))

        # ============================================================
        # 5. é‡ã¿ä»˜ãå¹³å‡
        # ============================================================
        total_score = sum(score * weight for _, score, weight in scores)

        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆDEBUGæ™‚ã®ã¿ï¼‰
        if self.logger.level <= 10:  # logging.DEBUG
            score_details = ', '.join(f"{name}={score:.2f}" for name, score, _ in scores)
            self.logger.debug(
                f"  Triplet: ({subject[:20]}, {relation}, {object_[:20]}) "
                f"â†’ {score_details} = {total_score:.2f}"
            )

        return total_score

    def _score_context_alignment(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str
    ) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ•´åˆæ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

        Returns:
            0.0ï¼ˆæ•´åˆæ€§ãªã—ï¼‰ï½ 1.0ï¼ˆå®Œå…¨ã«æ•´åˆï¼‰
        """
        score = 0.0
        context_lower = context.lower()

        # ============================================================
        # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹ã‹
        # ============================================================
        subject_in_context = subject.lower() in context_lower
        object_in_context = object_.lower() in context_lower

        if subject_in_context and object_in_context:
            score += 0.5
        elif subject_in_context or object_in_context:
            score += 0.3
        else:
            # ã©ã¡ã‚‰ã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„å ´åˆã¯ä½ã‚¹ã‚³ã‚¢
            score += 0.1

        # ============================================================
        # 2. é–¢ä¿‚ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆã¨åˆè‡´ã™ã‚‹ã‹
        # ============================================================
        relation_lower = relation.lower().replace('_', ' ')

        # é–¢ä¿‚ã®å‹•è©å½¢ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹ã‹
        if relation_lower in context_lower:
            score += 0.3
        else:
            # é¡ä¼¼è¡¨ç¾ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            relation_synonyms = self._get_relation_synonyms(relation)
            if any(syn in context_lower for syn in relation_synonyms):
                score += 0.2

        # ============================================================
        # 3. ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå…¨ä½“ã®è¿‘æ¥æ€§
        # ============================================================
        # ä¸»èªã¨ç›®çš„èªãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§è¿‘ã„ä½ç½®ã«ã‚ã‚‹ã‹
        if subject_in_context and object_in_context:
            try:
                subject_pos = context_lower.find(subject.lower())
                object_pos = context_lower.find(object_.lower())

                distance = abs(object_pos - subject_pos)

                # è·é›¢ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´ï¼ˆè¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
                if distance < 50:
                    score += 0.2
                elif distance < 100:
                    score += 0.1
            except Exception:
                pass

        return min(score, 1.0)

    def _get_relation_synonyms(self, relation: str) -> List[str]:
        """
        é–¢ä¿‚ã®åŒç¾©èªãƒ»é¡ä¼¼è¡¨ç¾ã‚’è¿”ã™

        Args:
            relation: é–¢ä¿‚å

        Returns:
            åŒç¾©èªã®ãƒªã‚¹ãƒˆ
        """
        # ä¸»è¦ãªé–¢ä¿‚ã®åŒç¾©èªãƒãƒƒãƒ—
        synonym_map = {
            'uses': ['use', 'utilizes', 'employs', 'applies'],
            'causes': ['cause', 'leads to', 'results in', 'triggers'],
            'part_of': ['part of', 'component of', 'belongs to'],
            'is_a': ['is a', 'type of', 'kind of', 'instance of'],
            'has': ['have', 'contains', 'includes', 'comprises'],
            'improves': ['improve', 'enhances', 'optimizes', 'boosts'],
            'based_on': ['based on', 'derived from', 'built on', 'relies on'],
            'enables': ['enable', 'allows', 'permits', 'facilitates'],
            'requires': ['require', 'needs', 'depends on', 'necessitates'],
        }

        relation_lower = relation.lower().replace('_', ' ')

        # å®Œå…¨ä¸€è‡´ã‚’æ¢ã™
        for key, synonyms in synonym_map.items():
            if relation_lower == key.replace('_', ' ') or relation_lower in synonyms:
                return synonyms

        # ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®é–¢ä¿‚ã®ã¿
        return [relation_lower]

    def _refiner_regenerate_triplet(
        self,
        original_triplet: Tuple[str, str, str],
        chunk_text: str,
        ai_router: Any
    ) -> Tuple[Optional[Tuple[str, str, str]], int]:
        """
        ä½å“è³ªãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’å†ç”Ÿæˆ

        Args:
            original_triplet: å…ƒã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ
            chunk_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            ai_router: AIRouterã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

        Returns:
            æ”¹å–„ã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        s, r, o = original_triplet

        # LLMãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        if ai_router is None:
            self.logger.error("AIRouter not provided for refinement")
            return None, 0


        # ============================================================
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        # ============================================================
        prompt = f"""Given the following text, improve the quality of this knowledge triplet.

    Original triplet:
    - Subject: {s}
    - Relation: {r}
    - Object: {o}

    Text context:
    {chunk_text[:500]}

    Please provide an improved triplet that:
    1. Uses more specific and descriptive entities
    2. Uses a clear and meaningful relation
    3. Accurately reflects the text content
    4. Avoids vague terms like "it", "this", "that"

    Return ONLY the improved triplet in this exact format:
    Subject: [improved subject]
    Relation: [improved relation]
    Object: [improved object]

    If the original triplet cannot be improved, return "NO_IMPROVEMENT".
    """

        # ============================================================
        # LLMã§å†ç”Ÿæˆ
        # ============================================================
        try:
            # AIRouterçµŒç”±ã§Refinerãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ç”Ÿæˆï¼ï¼
            response = ai_router.complete(
                prompt=prompt,
                model=self.refiner_model 
            )
            response_text = response.text.strip()

            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šï¼ˆç°¡æ˜“ï¼‰
            prompt_tokens = len(prompt) // 4
            response_tokens = len(response_text) // 4
            total_tokens = prompt_tokens + response_tokens

            # "NO_IMPROVEMENT"ãƒã‚§ãƒƒã‚¯
            if "NO_IMPROVEMENT" in response_text.upper():
                return None, total_tokens

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            refined = self._parse_triplet_response(response_text)

            if refined:
                return refined, total_tokens
            else:
                self.logger.debug(f"  Failed to parse refinement response")
                return None, total_tokens

        except Exception as e:
            self.logger.debug(f"  Refinement failed: {type(e).__name__}")
            return None, prompt_tokens

    def _parse_triplet_response(self, response: str) -> Optional[Tuple[str, str, str]]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º

        Args:
            response: LLMã®å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            (subject, relation, object) ã¾ãŸã¯ None
        """
        try:
            lines = response.strip().split('\n')

            subject = None
            relation = None
            object_ = None

            for line in lines:
                line = line.strip()

                if line.startswith('Subject:'):
                    subject = line.replace('Subject:', '').strip()
                elif line.startswith('Relation:'):
                    relation = line.replace('Relation:', '').strip()
                elif line.startswith('Object:'):
                    object_ = line.replace('Object:', '').strip()

            # ã™ã¹ã¦ãŒæŠ½å‡ºã§ããŸã‹ç¢ºèª
            if subject and relation and object_:
                # ç©ºç™½ã‚„è¨˜å·ã®ã¿ã§ãªã„ã‹ç¢ºèª
                if (len(subject.strip()) > 1 and
                    len(relation.strip()) > 1 and
                    len(object_.strip()) > 1):
                    return (subject, relation, object_)

            return None

        except Exception as e:
            self.logger.debug(f"  Parse error: {e}")
            return None

    def _validator_check_consistency(
        self,
        triplets: List[Tuple[str, str, str]],
        context: str
    ) -> List[Tuple[str, str, str]]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ä¸€è²«æ€§ã¨çŸ›ç›¾ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            triplets: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æ¤œè¨¼æ¸ˆã¿ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆï¼ˆçŸ›ç›¾ãŒã‚ã‚‹ã‚‚ã®ã¯é™¤å¤–ï¼‰
        """
        validated = []
        seen_triplets = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

        for s, r, o in triplets:
            # ============================================================
            # 1. é‡è¤‡ãƒã‚§ãƒƒã‚¯
            # ============================================================
            triplet_key = (s.lower(), r.lower(), o.lower())
            if triplet_key in seen_triplets:
                self.logger.debug(f"  âŠ— Duplicate: ({s}, {r}, {o})")
                continue

            # ============================================================
            # 2. è‡ªå·±å‚ç…§ãƒã‚§ãƒƒã‚¯ï¼ˆä¸»èªã¨ç›®çš„èªãŒåŒã˜ï¼‰
            # ============================================================
            if s.lower().strip() == o.lower().strip():
                self.logger.debug(f"  âŠ— Self-reference: ({s}, {r}, {o})")
                continue

            # ============================================================
            # 3. é€†é–¢ä¿‚ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
            # ============================================================
            if self._has_contradictory_relation(s, r, o, validated):
                self.logger.debug(f"  âŠ— Contradictory: ({s}, {r}, {o})")
                continue

            # ============================================================
            # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¦¥å½“æ€§ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            # ============================================================
            if not self._is_contextually_valid(s, r, o, context):
                self.logger.debug(f"  âŠ— Context invalid: ({s}, {r}, {o})")
                continue

            # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹
            validated.append((s, r, o))
            seen_triplets.add(triplet_key)

        removed_count = len(triplets) - len(validated)
        if removed_count > 0:
            self.logger.info(f"  â†’ Validator removed {removed_count} inconsistent triplets")

        return validated

    def _has_contradictory_relation(
        self,
        subject: str,
        relation: str,
        object_: str,
        existing_triplets: List[Tuple[str, str, str]]
    ) -> bool:
        """
        æ—¢å­˜ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã¨çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            existing_triplets: æ—¢ã«æ¤œè¨¼æ¸ˆã¿ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ

        Returns:
            True: çŸ›ç›¾ã‚ã‚Š, False: çŸ›ç›¾ãªã—
        """
        # çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ã®ãƒšã‚¢
        contradictory_pairs = [
            # åŸå› ã¨çµæœã®é€†è»¢
            ('causes', 'caused_by'),
            ('creates', 'created_by'),
            ('produces', 'produced_by'),

            # åŒ…å«é–¢ä¿‚ã®é€†è»¢
            ('part_of', 'contains'),
            ('component_of', 'has_component'),
            ('member_of', 'has_member'),

            # è‚¯å®šã¨å¦å®š
            ('is', 'is_not'),
            ('has', 'lacks'),
            ('enables', 'prevents'),

            # æ™‚é–“çš„çŸ›ç›¾
            ('before', 'after'),
            ('precedes', 'follows'),
        ]

        subject_lower = subject.lower()
        object_lower = object_.lower()
        relation_lower = relation.lower().replace('_', ' ').replace('-', ' ')

        for s_exist, r_exist, o_exist in existing_triplets:
            s_exist_lower = s_exist.lower()
            o_exist_lower = o_exist.lower()
            r_exist_lower = r_exist.lower().replace('_', ' ').replace('-', ' ')

            # åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢ã§ç•°ãªã‚‹é–¢ä¿‚
            if ((subject_lower == s_exist_lower and object_lower == o_exist_lower) or
                (subject_lower == o_exist_lower and object_lower == s_exist_lower)):

                # çŸ›ç›¾ã™ã‚‹é–¢ä¿‚ã®ãƒšã‚¢ã‚’ãƒã‚§ãƒƒã‚¯
                for rel1, rel2 in contradictory_pairs:
                    if ((relation_lower == rel1 and r_exist_lower == rel2) or
                        (relation_lower == rel2 and r_exist_lower == rel1)):
                        self.logger.debug(
                            f"  Found contradiction: "
                            f"({subject}, {relation}, {object_}) vs "
                            f"({s_exist}, {r_exist}, {o_exist})"
                        )
                        return True

        return False

    def _is_contextually_valid(
        self,
        subject: str,
        relation: str,
        object_: str,
        context: str,
        min_score: float = 0.3
    ) -> bool:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦å¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            subject: ä¸»èª
            relation: é–¢ä¿‚
            object_: ç›®çš„èª
            context: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            min_score: æœ€å°ã‚¹ã‚³ã‚¢é–¾å€¤

        Returns:
            True: å¦¥å½“, False: ä¸é©åˆ‡
        """
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
        score = self._score_context_alignment(subject, relation, object_, context)

        return score >= min_score

    def _compute_triplet_quality(self, s: str, r: str, o: str) -> float:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®å“è³ªã‚’BGE-M3 + ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆAPIå®Œå…¨æ’é™¤ç‰ˆï¼‰

        Args:
            s: subject (ä¸»èª)
            r: relation (é–¢ä¿‚)
            o: object (ç›®çš„èª)

        Returns:
            float: å“è³ªã‚¹ã‚³ã‚¢ (0.0ã€œ1.0)
        """
        score = 1.0

        # 1. åŸºæœ¬çš„ãªæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã€LLMä¸è¦ï¼‰
        s_lower = s.lower().strip()
        r_lower = r.lower().strip()
        o_lower = o.lower().strip()

        # ç©ºãƒ»çŸ­ã™ãã‚‹ãƒã‚§ãƒƒã‚¯
        if len(s_lower) < 2 or len(r_lower) < 2 or len(o_lower) < 2:
            score -= 0.4

        # æ•°å­—ã ã‘/è¨˜å·ã ã‘ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        if s_lower.isdigit() or o_lower.isdigit() or not any(c.isalnum() for c in s_lower):
            score -= 0.3
        if r_lower in self.relation_blacklist:
            score -= 0.3

        # è‡ªå·±å‚ç…§ï¼ˆä¸»èªã¨ç›®çš„èªãŒåŒã˜ï¼‰
        if s_lower == o_lower:
            score -= 0.5

        # ä¸»èª/é–¢ä¿‚/ç›®çš„èªãŒé‡è¤‡
        if s_lower == r_lower or o_lower == r_lower:
            score -= 0.3

        # 2. BGE-M3ã‚’ä½¿ã£ãŸé–¢ä¿‚å“è³ªãƒã‚§ãƒƒã‚¯
        if hasattr(self, 'embedding_cache') and self.embedding_cache is not None:
            try:
                # é–¢ä¿‚æ–‡å­—åˆ—ã‚’åŸ‹ã‚è¾¼ã¿
                r_emb = self.embedding_cache.get_embedding(r.lower())
                # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã¨ã®æœ€å¤§é¡ä¼¼åº¦
                if hasattr(self, 'blacklist_embs') and self.blacklist_embs:
                    max_sim = max(
                        np.dot(r_emb, be) / (np.linalg.norm(r_emb) * np.linalg.norm(be) + 1e-9)
                        for be in self.blacklist_embs 
                    )
                    score -= max_sim * 0.8  # é¡ä¼¼åº¦0.8ä»¥ä¸Šã§å¤§å¹…æ¸›ç‚¹
                    # æœ‰ç”¨é–¢ä¿‚ãƒªã‚¹ãƒˆï¼ˆäº‹å‰å®šç¾©ï¼‰ã¨ã®æœ€å¤§é¡ä¼¼åº¦ï¼ˆé«˜ã„ã»ã©åŠ ç‚¹ï¼‰
                if hasattr(self, 'useful_rel_embs') and self.useful_rel_embs:
                    max_useful_sim = max(
                        np.dot(r_emb, ue) / (np.linalg.norm(r_emb) * np.linalg.norm(ue) + 1e-9)
                        for ue in self.useful_rel_embs
                    )
                    score += max_useful_sim * 0.4
            
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…·ä½“æ€§ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒãƒ«ãƒ é•·ã§ç°¡æ˜“åˆ¤å®šï¼‰
                s_emb = self.embedding_cache.get_embedding(s_lower)
                o_emb = self.embedding_cache.get_embedding(o_lower)
                s_specificity = min(1.0, np.linalg.norm(s_emb) / 0.5)  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
                o_specificity = min(1.0, np.linalg.norm(o_emb) / 0.5)
                score += (s_specificity + o_specificity) * 0.3
            except Exception as e:
                self.logger.warning(f"BGE-M3 quality check failed in triplet: {e}. Using rule-based only.")    

        return max(min(score, 1.0), 0.0)

    def _map_triplets_to_documents(
        self,
        triplets: List[Tuple[str, str, str]],
        documents: List[Any]
    ) -> Dict[Any, List[Tuple[str, str, str]]]:
        """
        ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚°

        Args:
            triplets: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ

        Returns:
            {Document: [triplets]} ã®è¾æ›¸
        """
        mapping = {doc: [] for doc in documents}

        # å„ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆãŒã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹åˆ¤å®š
        for s, r, o in triplets:
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            for doc in documents:
                doc_text_lower = doc.text.lower()

                # ä¸»èªã¾ãŸã¯ç›®çš„èªãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹
                if (s.lower() in doc_text_lower or o.lower() in doc_text_lower):
                    mapping[doc].append((s, r, o))
                    break  # æœ€åˆã«ãƒãƒƒãƒã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦
            else:
                # ã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚‚ãƒãƒƒãƒã—ãªã„å ´åˆã¯æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦
                if documents:
                    mapping[documents[0]].append((s, r, o))

        # ç©ºã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        mapping = {doc: trips for doc, trips in mapping.items() if trips}

        self.logger.info(f"  Mapped {len(triplets)} triplets to {len(mapping)} documents")

        return mapping

    def _score_relation(self, relation: str) -> float:
        """
        é–¢ä¿‚ã®æ˜ç¢ºæ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

        Returns:
            0.0ï¼ˆæœ€æ‚ªï¼‰ï½ 1.0ï¼ˆæœ€è‰¯ï¼‰
        """
        relation_lower = relation.lower().strip()

        # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆå³åº§ã«0.0ï¼‰
        if relation_lower in self.relation_blacklist:
            return 0.0

        # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
        if len(relation_lower) < 2:
            return 0.0

        # é«˜å“è³ªãªé–¢ä¿‚ï¼ˆå°‚é–€çš„ãƒ»å…·ä½“çš„ï¼‰
        high_quality_relations = {
            # å› æœé–¢ä¿‚
            'causes', 'results_in', 'leads_to', 'enables', 'triggers',
            'produces', 'generates', 'influences', 'affects',

            # æ§‹æˆé–¢ä¿‚
            'part_of', 'component_of', 'consists_of', 'comprises',
            'contains', 'includes',

            # ä½¿ç”¨é–¢ä¿‚
            'uses', 'utilizes', 'employs', 'applies', 'leverages',
            'implements', 'adopts',

            # æ´¾ç”Ÿé–¢ä¿‚
            'based_on', 'derived_from', 'inspired_by', 'extends',
            'improves_upon', 'builds_on',

            # å°‚é–€é–¢ä¿‚
            'optimizes', 'parameterizes', 'regularizes', 'approximates',
            'encodes', 'decodes', 'transforms', 'projects',

            # æ¯”è¼ƒé–¢ä¿‚
            'outperforms', 'surpasses', 'exceeds', 'improves',
        }

        if relation_lower in high_quality_relations:
            return 1.0

        # ä¸­å“è³ªãªé–¢ä¿‚ï¼ˆä¸€èˆ¬çš„ã ãŒæœ‰ç”¨ï¼‰
        medium_quality_relations = {
            'is_a', 'type_of', 'instance_of', 'subclass_of',
            'related_to', 'associated_with', 'connected_to',
            'depends_on', 'requires', 'needs',
        }

        if relation_lower in medium_quality_relations:
            return 0.7

        # å‹•è©å½¢å¼ï¼ˆ-s, -ed, -ingï¼‰ãªã‚‰ä¸­ç¨‹åº¦
        if any(relation_lower.endswith(suffix) for suffix in ['s', 'ed', 'ing']):
            return 0.6

        # ãã‚Œä»¥å¤–ã¯ä½å“è³ª
        return 0.3

    def _score_entities(self, subject: str, object_: str) -> float:
        """
        ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…·ä½“æ€§ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

        Returns:
            0.0ï¼ˆæŠ½è±¡çš„ãƒ»æ›–æ˜§ï¼‰ï½ 1.0ï¼ˆå…·ä½“çš„ï¼‰
        """
        score = 0.0

        # ä¸¡æ–¹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒã‚§ãƒƒã‚¯
        for entity in [subject, object_]:
            entity_lower = entity.lower().strip()

            # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
            if len(entity_lower) < 2:
                continue

            # ä»£åè©ï¼ˆä½å“è³ªï¼‰
            pronouns = {'it', 'this', 'that', 'these', 'those', 'they', 'them'}
            if entity_lower in pronouns:
                score += 0.0
                continue

            # å˜èªæ•°ï¼ˆè¤‡æ•°å˜èª = ã‚ˆã‚Šå…·ä½“çš„ï¼‰
            word_count = len(entity_lower.split())
            if word_count >= 3:
                score += 1.0
            elif word_count == 2:
                score += 0.8
            else:
                score += 0.5

        # 2ã¤ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å¹³å‡
        return score / 2.0

    def _score_grammar(
        self,
        subject: str,
        relation: str,
        object_: str
    ) -> float:
        """
        æ–‡æ³•çš„æ­£ã—ã•ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

        Returns:
            0.0ï¼ˆæ–‡æ³•çš„ã«ãŠã‹ã—ã„ï¼‰ï½ 1.0ï¼ˆæ­£ã—ã„ï¼‰
        """
        score = 1.0

        # å…¨ã¦å°æ–‡å­—ï¼ˆæŠ½å‡ºãƒŸã‚¹ã®å¯èƒ½æ€§ï¼‰
        if subject.islower() and object_.islower():
            score -= 0.2

        # æ•°å­—ã ã‘ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆä½å“è³ªï¼‰
        if subject.isdigit() or object_.isdigit():
            score -= 0.3

        # è¨˜å·ã®ã¿
        if not any(c.isalnum() for c in subject) or not any(c.isalnum() for c in object_):
            score -= 0.5

        # ä¸»èªã¨ç›®çš„èªãŒåŒã˜ï¼ˆè‡ªå·±å‚ç…§ï¼‰
        if subject.lower() == object_.lower():
            score -= 0.5

        # ------------------------------------------------------------
        # 2. é–¢ä¿‚ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # ------------------------------------------------------------

        relation_lower = relation.lower().strip()

        # é–¢ä¿‚ãŒç©ºã¾ãŸã¯çŸ­ã™ãã‚‹
        if len(relation_lower) < 2:
            score -= 0.4

        # é–¢ä¿‚ãŒãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ï¼ˆä½å“è³ªï¼‰
        if relation_lower in self.relation_blacklist:
            score -= 0.3

        # é–¢ä¿‚ãŒè¨˜å·ã®ã¿
        if not any(c.isalnum() for c in relation):
            score -= 0.4

        # ------------------------------------------------------------
        # 3. ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå…¨ä½“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        # ------------------------------------------------------------

        # ä¸»èªã¨é–¢ä¿‚ãŒåŒã˜ï¼ˆä¾‹: "uses uses object"ï¼‰
        if subject.lower() == relation_lower:
            score -= 0.3

        # ç›®çš„èªã¨é–¢ä¿‚ãŒåŒã˜ï¼ˆä¾‹: "subject uses uses"ï¼‰
        if object_.lower() == relation_lower:
            score -= 0.3

        # 3ã¤ã¨ã‚‚åŒã˜ï¼ˆæœ€æ‚ªï¼‰
        if subject.lower() == relation_lower == object_.lower():
            score -= 0.5

        return max(score, 0.0)