"""
Crystal Geode
Knowledge Crystallization System - 

ãƒ‘ãƒ¼ã‚¹ â†’ Quality Check â†’ JSONå‡ºåŠ›
"""

# ============================================================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================
import os
import re
import logging
import concurrent.futures
import time
import argparse
import json
import requests

from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Tuple, Any, Set
from sentence_transformers import SentenceTransformer

from llama_index.core import Document
from llama_index.graph_stores.neo4j import Neo4jGraphStore

from grobid_client.grobid_client import GrobidClient

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from shared.logger import (
    HierarchicalLogger,
    setup_logger,
)
from shared.quality_checker import DataQualityChecker
from shared.duplicate_checker import ProvenanceManager
from shared.utils.hashing import compute_file_hash

from shared.text_utils import (
    clean_text,
    chunk_by_paragraphs,
)
from shared.file_utils import (
    collect_files,
#    sanitize_filename,
#    detect_encoding,
    detect_format
)
from shared.parsers import parse_tei, parse_markdown, parse_txt, parse_docx, parse_html, parse_pdf

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# ============================================================
# ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
# ============================================================
class CrystalGeode:
    """Crystal Geode - æ–‡æ›¸ãƒ‘ãƒ¼ã‚¹å°‚ç”¨"""
    
    def __init__(self, config: Optional[Dict] = None, log_level: int = logging.INFO):
        self.config = config or {}
        self.crystal = None
        self.metadata = {}
        self.logger = setup_logger('CrystalGeode', log_level)
        self.hlogger = HierarchicalLogger(self.logger)
       
        # BGE-M3å…±æœ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã“ã“ã§åˆæœŸåŒ–ï¼ˆ1å›ã ã‘ãƒ­ãƒ¼ãƒ‰ï¼‰
        from .model.embed import ensure_bge_m3, EmbeddingCache 
        try:
            self.embed_model = ensure_bge_m3()  # è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
            self.embedding_cache = EmbeddingCache(embed_model=self.embed_model)
            self.logger.info("âœ… BGE-M3 shared embedding cache initialized")
        except Exception as e:
            self.logger.warning(f"âš ï¸ BGE-M3 initialization failed: {e}. Embedding features disabled.")
            self.embedding_cache = None

        # Grobidè¨­å®š
        self.grobid_url = self.config.get('grobid_url', 'http://localhost:8070')
        self.grobid_available = self._check_grobid()
        self.grobid_client = None
        
        if self.grobid_available:
            self._init_grobid()
            self.logger.info(f"âœ… Grobid server available at {self.grobid_url}")
        else:
            self.logger.warning("âš ï¸  Grobid server not available (PDF support disabled)")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯è¨­å®š
        self.enable_duplicate_check = config.get('enable_duplicate_check', True)
        self.enable_provenance = config.get('enable_provenance', True)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.file_hash_cache: Set[str] = set()  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.doc_hash_cache: Set[str] = set()   # å¾Œã§CrystalClusterã«æ¸¡ã™
        
        # Provenance Manager
        if self.enable_provenance:
            
            self.provenance_mgr = ProvenanceManager(logger=self.logger)
        else:
            self.provenance_mgr = None
        
        # Neo4jæ¥ç¶šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.neo4j_store = self._init_neo4j_connection()
        
        # èµ·å‹•æ™‚ã«æ—¢å­˜ãƒãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰
        if self.enable_duplicate_check and self.neo4j_store:
            self.load_file_hashes_from_neo4j()
        
        self.logger.info("Crystal Geode bata")
    
    def _check_grobid(self) -> bool:
        """Grobidã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            
            response = requests.get(f"{self.grobid_url}/api/isalive", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def _init_grobid(self):
        """Grobidã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        
        self.grobid_client = GrobidClient(
            grobid_server=self.config.get('grobid_url', 'http://localhost:8070'),
            timeout=120
        )
        self.logger.info("âœ… Grobid client initialized")

    def _init_neo4j_connection(self) -> Optional[Any]:
        """Neo4jæ¥ç¶šã‚’åˆæœŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        neo4j_config = self.config.get('neo4j', {})
        
        if not neo4j_config.get('enabled', False):
            self.logger.info("Neo4j integration disabled")
            return None
        
        try:           
            store = Neo4jGraphStore(
                username=neo4j_config.get('username', 'neo4j'),
                password=neo4j_config.get('password'),
                url=neo4j_config.get('url', 'bolt://localhost:7687')
            )
            
            store.query("RETURN 1")
            self.logger.info("âœ… Neo4j connection established")
            
            return store
        
        except Exception as e:
            self.logger.warning(f"âš ï¸  Neo4j connection failed: {type(e).__name__}")
            return None

    def load_file_hashes_from_neo4j(self):
        """Neo4jã‹ã‚‰æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ä¸€æ‹¬å–å¾—"""
        if not self.neo4j_store:
            return
        
        query = """
        MATCH (d:Document)
        WHERE d.file_hash IS NOT NULL
        RETURN d.file_hash AS file_hash
        """
        
        try:
            results = self.neo4j_store.query(query)
            
            for record in results:
                self.file_hash_cache.add(record['file_hash'])
            
            self.logger.info(
                f"ğŸ“¥ Loaded {len(self.file_hash_cache)} existing file hashes from Neo4j"
            )
        
        except Exception as e:
            self.logger.warning(f"Failed to load file hashes from Neo4j: {e}")

    def check_file_duplicate_and_provenance(
        self,
        file_path: str
    ) -> Optional[Dict[str, Any]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ + Provenanceç”Ÿæˆ
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            Provenanceè¾æ›¸ï¼ˆé‡è¤‡ã®å ´åˆã¯Noneï¼‰
        """
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        try:
            file_hash = compute_file_hash(file_path, algorithm='sha256')
        except Exception as e:
            self.logger.error(f"Failed to compute hash for {file_path}: {e}")
            return None
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if self.enable_duplicate_check and file_hash in self.file_hash_cache:
            self.logger.warning(
                f"âŠ— Duplicate file skipped: {file_path} "
                f"(hash: {file_hash[:8]}...)"
            )
            return None
        
        # Provenanceç”Ÿæˆ
        if self.enable_provenance and self.provenance_mgr:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            source_type = Path(file_path).suffix.lstrip('.')
            
            provenance = self.provenance_mgr.create_provenance(
                source_path=file_path,
                source_type=source_type,
                file_hash=file_hash,
                metadata={
                    'parsed_by': 'crystal_geode',
                    'version': 'beta'
                }
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
            self.file_hash_cache.add(file_hash)
            
            self.logger.info(
                f"âœ“ New file registered: {Path(file_path).name} "
                f"(hash: {file_hash[:8]}...)"
            )
            
            return provenance
        
        else:
            # Provenanceç„¡åŠ¹æ™‚ã¯ç°¡æ˜“è¾æ›¸ã‚’è¿”ã™
            self.file_hash_cache.add(file_hash)
            return {
                'file_hash': file_hash,
                'source_path': file_path,
                'source_type': Path(file_path).suffix.lstrip('.')
            }

    def crystallize(self, input_path: str, format: str = 'auto') -> List[Document]:
        """çµæ™¶åŒ–: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹"""
        self.logger.info("Crystallizing knowledge structure...")

        input_path = Path(input_path).expanduser()

        if not input_path.exists():
            raise FileNotFoundError(f"File not found: {input_path}")
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ + Provenanceç”Ÿæˆ
        provenance = self.check_file_duplicate_and_provenance(str(input_path))
    
        if provenance is None:
            # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ« â†’ ã‚¹ã‚­ãƒƒãƒ—
            self.logger.warning(f"âŠ— Skipping duplicate file: {input_path.name}")
            return []

        if format == 'auto':
            format = detect_format(str(input_path))

        parsers = {
            'tei': parse_tei,
            'markdown': parse_markdown,
            'txt': parse_txt,
            'docx': parse_docx,
            'html': parse_html,
            'pdf': parse_pdf
        }
        try:
            if format not in parsers:
                raise ValueError(f"Unsupported format: {format}")

        # ãƒ‘ãƒ¼ã‚µãƒ¼å‘¼ã³å‡ºã—
            if format == 'pdf':
                documents, metadata = parsers[format](input_path, grobid_client=self.grobid_client, logger=self.logger)
            elif format == 'txt':
                documents, metadata = parsers[format](input_path, config=self.config, logger=self.logger)
            else:
                documents, metadata = parsers[format](input_path, logger=self.logger)

        except Exception as e:
            self.logger.error(f"âŒ Failed to parse {input_path.name}: {e}")
            raise
    
       # 4. Provenanceæƒ…å ±ã‚’å„Documentã«æ³¨å…¥
        for doc in documents:
        # æ—¢å­˜ã®metadataã‚’ä¿æŒã—ã¤ã¤Provenanceã‚’è¿½åŠ 
            doc.metadata.update({
               # Provenanceæƒ…å ±
               'file_hash': provenance['file_hash'],
               'source_path': provenance['source_path'],
                'source_name': provenance['source_name'],
                'source_type': provenance['source_type'],
                'ingested_at': provenance['ingested_at'],
                'version': provenance['version'],
                'pipeline_stage': 'geode_parse',
            
            # ãƒ‘ãƒ¼ã‚¹æƒ…å ±
                'parsed_at': datetime.now().isoformat(),
                'parsed_by': 'crystal_geode',
                'format': format,
            })

        self.crystal = documents
        self.metadata = metadata
        self.logger.info(
            f"âœ¨ Crystal structure stabilized: {len(documents)} nodes "
            f"(hash: {provenance['file_hash'][:8]}...)"
        )

        return documents

    def batch_crystallize(
        self,
        input_dir: str,
        patterns: List[str] = None,
        max_workers: int = 4,
        fail_fast: bool = False,
        output_json: str = None
    ) -> Dict[str, Any]:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†
    
        Args:
            input_dir: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            patterns: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: ['*.md', '*.pdf']ï¼‰
           max_workers: ä¸¦åˆ—å‡¦ç†æ•°
            fail_fast: True=æœ€åˆã®ã‚¨ãƒ©ãƒ¼ã§åœæ­¢, False=å…¨éƒ¨è©¦ã™
           output_json: JSONå‡ºåŠ›ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
        Returns:
            {
                'success': {filepath: [Document, ...], ...},
                'failed': [(filepath, error_msg), ...],
                'skipped': [filepath, ...],  # é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—
                'stats': {...}
            }
        """
        self.logger.info(f"Starting batch crystallization: {input_dir}")
    
        # ========================================
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        # ========================================
        if patterns is None:
            patterns = ['*.md', '*.docx', '*.html', '*.txt', '*.tei.xml']
        
            # Grobidæœ‰åŠ¹æ™‚ã®ã¿PDFã‚’è¿½åŠ 
            if self.grobid_available:
                patterns.append('*.pdf')
                self.logger.info("âœ… PDF processing enabled")
            else:
                self.logger.warning("âš ï¸  PDF skipped (Grobid server not available)")
    
        files = collect_files(input_dir, patterns=patterns)
        self.logger.info(f"Found {len(files)} files")
    
        # ========================================
        # 2. çµæœæ ¼ç´
        # ========================================
        results = {
            'success': {},
            'failed': [],
            'skipped': [],  # é‡è¤‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            'stats': {}
        }
    
    # ========================================
    # 3. ä¸¦åˆ—å‡¦ç†
    # ========================================
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {}
        
            for f in files:
                future = executor.submit(self._crystallize_with_retry, str(f))
                future_to_file[future] = f
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
            if HAS_TQDM:
                iterator = tqdm(
                    concurrent.futures.as_completed(future_to_file),
                    total=len(future_to_file),
                    desc="Crystallizing"
                )
            else:
                iterator = concurrent.futures.as_completed(future_to_file)
        
        # çµæœåé›†
            for future in iterator:
                file_path = future_to_file[future]
            
                try:
                    docs = future.result(timeout=300)
                
                # ç©ºãƒªã‚¹ãƒˆ = é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—
                    if not docs:
                        results['skipped'].append(str(file_path))
                    else:
                        results['success'][str(file_path)] = docs
            
                except concurrent.futures.TimeoutError:
                    error_msg = "Processing timeout (>5min)"
                    results['failed'].append((str(file_path), error_msg))
                    self.logger.error(f"âŒ Failed: {file_path.name} - {error_msg}")
                
                    if fail_fast:
                        executor.shutdown(wait=False)
                        break
            
                except Exception as e:
                    error_msg = str(e)
                    results['failed'].append((str(file_path), error_msg))
                    self.logger.error(f"âŒ Failed: {file_path.name} - {error_msg}")
                
                    if fail_fast:
                        executor.shutdown(wait=False)
                        break
    
    # ========================================
    # 4. çµ±è¨ˆæƒ…å ±
    # ========================================
        results['stats'] = {
            'total': len(files),
            'success': len(results['success']),
            'failed': len(results['failed']),
            'skipped': len(results['skipped']),
            'total_documents': sum(len(docs) for docs in results['success'].values())
        }
    
    # ========================================
    # 5. ã‚µãƒãƒªãƒ¼
    # ========================================
        self.logger.info(
            f"\n{'='*60}\n"
            f"âœ… Batch Complete!\n"
            f"   Success: {results['stats']['success']}\n"
            f"   Failed:  {results['stats']['failed']}\n"
            f"   Skipped: {results['stats']['skipped']} (duplicates)\n"
            f"   Total documents: {results['stats']['total_documents']}\n"
            f"{'='*60}"
        )
    
    # ========================================
    # 6. å¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    # ========================================
        if results['failed']:
            self._save_error_report(results['failed'], input_dir)
    
    # ========================================
    # 7. JSONå‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ========================================
        if output_json:
            self._save_batch_results(results, output_json)
    
        return results

    def _crystallize_with_retry(
        self,
        file_path: str,
        max_retries: int = 3
    ) -> List[Document]:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ã crystallize
    
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
    
        Returns:
            Documentã®ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰
        """
        for attempt in range(max_retries):
            try:
                return self.crystallize(file_path)
        
            except Exception as e:
                if attempt == max_retries - 1:
                # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã§å¤±æ•— â†’ ä¾‹å¤–ã‚’ä¸Šã’ã‚‹
                    raise
            
                self.logger.warning(
                    f"âš ï¸  Retry {attempt+1}/{max_retries}: {Path(file_path).name}\n"
                    f"   Error: {e}"
                )
                time.sleep(2 ** attempt)  # exponential backoff

    def parse_and_check(self, input_file: str, review_dir: str = './review') -> Dict[str, Any]:
        """ãƒ‘ãƒ¼ã‚¹ â†’ Quality Check â†’ åœæ­¢"""

        # 1. Crystallize
        with self.hlogger.section("Parsing"):
            self.crystallize(input_file)

        # 2. Quality Check
        with self.hlogger.section("Quality Check"):
            checker = DataQualityChecker(logger=self.logger)
            result = checker.check_documents(self.crystal, output_dir=review_dir)

        # 3. çµæœä¿å­˜
        with self.hlogger.section("Saving Results"):
            # Clean ãƒ‡ãƒ¼ã‚¿
            clean_path = Path(review_dir) / 'clean_documents.json'
            self._save_documents(result['clean'], clean_path)

            # çµ±è¨ˆæƒ…å ±
            stats_path = Path(review_dir) / 'stats.json'
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(result['stats'], f, indent=2)

        self.logger.info(
            f"\n{'='*60}\n"
            f"âœ… Parsing Complete!\n"
            f"   Clean documents: {result['stats']['clean']}\n"
            f"   Flagged documents: {result['stats']['flagged']}\n"
            f"\n"
            f"ğŸ“ Output:\n"
            f"   Clean data: {clean_path}\n"
            f"   Review queue: {review_dir}/review_queue.csv\n"
            f"\n"
            f"â–¶ï¸  Next Step:\n"
            f"   1. Review: {review_dir}/review_queue.csv\n"
            f"   2. Run: crystal_committer.py {clean_path}\n"
            f"{'='*60}"
        )

        return result

    def save_parsed_data(self, output_path: str):
        """ãƒ‘ãƒ¼ã‚¹çµæœã‚’JSONã§ä¿å­˜"""
        if not self.crystal:
            raise ValueError("No data to save. Run crystallize() first")
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            'metadata': self.metadata,
            'documents': [
                {
                    'text': doc.text,
                    'metadata': doc.metadata
                }
                for doc in self.crystal
            ],
            'created_at': datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ Saved parsed data: {output_path}")
        

    def _save_error_report(
        self,
        failed: List[Tuple[str, str]],
        base_dir: str
    ):
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_path = Path(base_dir) / 'crystal_geode_errors.json'
    
        report = {
            'timestamp': datetime.now().isoformat(),
            'failed_count': len(failed),
            'failed_files': [
                {'file': filepath, 'error': error}
                for filepath, error in failed
            ]
        }
    
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
        self.logger.info(f"ğŸ“‹ Error report saved: {report_path}")

    def _save_batch_results(
        self,
        results: Dict[str, Any],
        output_path: str
    ):
        """ãƒãƒƒãƒå‡¦ç†çµæœã‚’JSONä¿å­˜"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Documentã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        serializable_results = {
            'metadata': {
                'batch_timestamp': datetime.now().isoformat(),
                'stats': results['stats']
            },
            'documents': {}
        }
    
        for filepath, docs in results['success'].items():
            serializable_results['documents'][filepath] = [
                {
                    'text': doc.text,
                    'metadata': doc.metadata
                }
                for doc in docs
            ]
    
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
        self.logger.info(f"ğŸ’¾ Batch results saved: {output_path}")

# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================
if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description='Crystal Geode Beta')
    
    parser.add_argument('command', choices=['parse', 'batch'])
    parser.add_argument('input_file', help='Input file or directory')
    parser.add_argument('--format', default='auto')
    parser.add_argument('--review-dir', default='./review')
    parser.add_argument('--debug', action='store_true')
    parser.add_argument('--max-workers', type=int, default=4)
    
    args = parser.parse_args()
    
    print("ğŸŒ‹ Crystal Geode Beta")
    print("â”" * 42)
    
    app = CrystalGeode(log_level=logging.DEBUG if args.debug else logging.INFO)
    
    if args.command == 'parse':
        app.parse_and_check(args.input_file, review_dir=args.review_dir)
    
    elif args.command == 'batch':
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè£…
        app.batch_crystallize(args.input_file, max_workers=args.max_workers)
    
    print("âœ¨ Complete!")