# ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”¨
import re
from typing import List, Tuple
import requests
import openai

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

class TripletExtractor:
    """
    ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæŠ½å‡ºç”¨AI
    
    Modes:
        - "api": OpenAI APIï¼ˆgpt-4o-mini, claude-3.5-sonnetç­‰ï¼‰
        - "ollama": Ollamaï¼ˆllama3.1:14Bç­‰ï¼‰
        - "local": Hugging Face Transformersï¼ˆRebel, Qwen2.5ç­‰ï¼‰
    """
    
    def __init__(
        self,
        mode: str,
        model_name: str,
        api_key: str = None,
        device: str = "mps",
        ollama_url: str = "http://localhost:11434"
    ):
        """
        Args:
            mode: "api" ã¾ãŸã¯ "local"
            model_name: 
                - API: "gpt-4o-mini", "claude-3.5-sonnet"
                - Ollama: "llama3.1:14B", "qwen2.5:14B"
                - Local: "Babelscape/rebel-large", "Qwen/Qwen2.5-14B"
            api_key: OpenAI API keyï¼ˆAPI modeæ™‚ï¼‰
            device: "mps", "cuda", "cpu"
            ollama_url: Ollama URL
        """
        self.mode = mode
        self.model_name = model_name
        self.ollama_url = ollama_url 
        
        if mode == "ollama":  
            print(f"ğŸ   Ollama mode: {model_name} at {ollama_url}")
            self._check_ollama()

        if mode == "local":
            print(f"ğŸ“¦ Loading local model: {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            # GPU/MPSå¯¾å¿œ
            if device == "mps" and hasattr(self.model, "to"):
                self.model.to("mps")
            elif device == "cuda":
                self.model.to("cuda")
            
            print(f"âœ… Local model loaded on {device}")
        
        elif mode == "api":
            if not api_key:
                raise ValueError("API key is required for API mode")
            
            openai.api_key = api_key
            print(f"âœ… API mode: {model_name}")
        
        else:
            raise ValueError(f"Invalid mode: {mode}. Use 'api' or 'local'")
    
    def _check_ollama(self):  # ğŸ‘ˆ è¿½åŠ 
        """OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
            if response.status_code == 200:
                print(f"âœ… Ollama is running at {self.ollama_url}")
            else:
                print(f"âš ï¸  Ollama responded with status {response.status_code}")
        except requests.exceptions.ConnectionError:
            raise RuntimeError(
                f"âŒ Cannot connect to Ollama at {self.ollama_url}\n"
                "Please start Ollama:\n"
                "  ollama serve\n"
                f"  ollama pull {self.model_name}"
            )
            
    def extract(self, text: str, max_triplets: int = 15) -> List[Tuple[str, str, str]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆã‚’æŠ½å‡º
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            max_triplets: æœ€å¤§ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæ•°
        
        Returns:
            [(subject, relation, object), ...] ã®ãƒªã‚¹ãƒˆ
        """
        if self.mode == "ollama": 
            return self._extract_ollama(text, max_triplets)
        elif self.mode == "local":
            return self._extract_local(text, max_triplets)
        elif self.mode == "api":
            return self._extract_api(text, max_triplets)
    
    def _extract_ollama(self, text: str, max_triplets: int) -> List[Tuple]:  # ğŸ‘ˆ è¿½åŠ 
        """OllamaçµŒç”±ã§æŠ½å‡º"""
        prompt = f"""Extract knowledge graph triples from the following text.
Return up to {max_triplets} triples in JSON array format: [["subject", "relation", "object"], ...]

Text: {text}

Return ONLY the JSON array, no explanation:"""
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "temperature": 0.3,
                    "stream": False
                },
                timeout=120
            )
            
            response.raise_for_status()
            result = response.json()
            content = result.get('response', '')
            
            # JSONãƒ‘ãƒ¼ã‚¹
            triplets = self._parse_json_output(content)
            return triplets[:max_triplets]
        
        except Exception as e:
            print(f"âš ï¸  Ollama extraction failed: {e}")
            return []
    def _extract_local(self, text: str, max_triplets: int) -> List[Tuple]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§æŠ½å‡º"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            num_beams=3
        )
        
        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ¢ãƒ‡ãƒ«ä¾å­˜ï¼‰
        triplets = self._parse_rebel_output(decoded)
        return triplets[:max_triplets]
    
    def _extract_api(self, text: str, max_triplets: int) -> List[Tuple]:
        """APIçµŒç”±ã§æŠ½å‡º"""
        prompt = f"""Extract knowledge graph triples from the following text.
Return up to {max_triplets} triples in the format: (subject, relation, object)

Text: {text}

Triples:"""
        
        try:
            response = openai.ChatCompletion.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                timeout=90,
                temperature=0.1
            )
            
            content = response["choices"][0]["message"]["content"]
            triplets = self._parse_llm_output(content)
            return triplets[:max_triplets]
        
        except Exception as e:
            print(f"âš ï¸  API extraction failed: {e}")
            return []
    
    def _parse_json_output(self, text: str) -> List[Tuple]:  # ğŸ‘ˆ è¿½åŠ 
        """JSONå½¢å¼ã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹"""
        import json
        
        # JSONã‚’æŠ½å‡ºï¼ˆå‰å¾Œã®èª¬æ˜æ–‡ã‚’é™¤å»ï¼‰
        # ```json ... ``` å½¢å¼ã«å¯¾å¿œ
        text = text.strip()
        if '```json' in text:
            text = text.split('```json')[1].split('```')[0]
        elif '```' in text:
            text = text.split('```')[1].split('```')[0]
        
        try:
            triplets_list = json.loads(text)
            
            # ãƒªã‚¹ãƒˆã®å„è¦ç´ ã‚’ã‚¿ãƒ—ãƒ«ã«å¤‰æ›
            result = []
            for item in triplets_list:
                if isinstance(item, list) and len(item) == 3:
                    result.append((str(item[0]).strip(), str(item[1]).strip(), str(item[2]).strip()))
            
            return result
        
        except json.JSONDecodeError:
            print(f"âš ï¸  Failed to parse JSON: {text[:100]}...")
            return []
    
    def _parse_rebel_output(self, text: str) -> List[Tuple]:
        """Rebel ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹"""
        # å®Ÿè£…ä¾‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        triplets = []
        # ... ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯ ...
        return triplets
    
    def _parse_llm_output(self, text: str) -> List[Tuple]:
        """LLMå‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹"""
        
        triplets = []
        
        # (subject, relation, object) å½¢å¼ã‚’æ¢ã™
        pattern = r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
        matches = re.findall(pattern, text)
        
        for s, r, o in matches:
            triplets.append((s.strip(), r.strip(), o.strip()))
        
        return triplets