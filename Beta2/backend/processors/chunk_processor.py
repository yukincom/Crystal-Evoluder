"""
ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¯ãƒ©ã‚¹
"""
import hashlib
from typing import List, Tuple

from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter


class ChunkProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’æ‹…å½“"""

    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger

    def create_dual_documents(
        self,
        documents: List[Document]
    ) -> Tuple[List[Document], List[Document]]:
        """
        æ—¢å­˜ã®Documentã‹ã‚‰ Graphç”¨ ã¨ Retrievalç”¨ ã®2ç¨®é¡ã‚’ä½œã‚‹

        Args:
            documents: load_documents() ã§ä½œæˆã—ãŸDocumentãƒªã‚¹ãƒˆ

        Returns:
            (graph_docs, retrieval_docs)
        """
        graph_splitter, retrieval_splitter = self.get_dual_splitters()
        graph_docs = []
        retrieval_docs = []

        for doc in documents:
            base_meta = dict(doc.metadata)

            # ------------------------------------------------------------
            # Graphç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå°ã•ã‚ï¼‰
            # ------------------------------------------------------------
            try:
                graph_nodes = graph_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(graph_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'structural',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    graph_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Graph splitting failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã†
                md = dict(base_meta)
                md['chunk_type'] = 'structural'
                graph_docs.append(Document(text=doc.text, metadata=md))

            # ------------------------------------------------------------
            # Retrievalç”¨ãƒãƒ£ãƒ³ã‚¯ï¼ˆå¤§ãã‚ï¼‰
            # ------------------------------------------------------------
            try:
                retrieval_nodes = retrieval_splitter.get_nodes_from_documents([doc])
                for j, node in enumerate(retrieval_nodes):
                    md = dict(base_meta)
                    md.update({
                        'chunk_type': 'semantic',
                        'chunk_index': j,
                        'chunk_size': len(node.text)
                    })
                    retrieval_docs.append(Document(
                        text=node.text,
                        metadata=md
                    ))
            except Exception as e:
                self.logger.warning(f"Retrieval splitting failed: {e}")
                md = dict(base_meta)
                md['chunk_type'] = 'semantic'
                retrieval_docs.append(Document(text=doc.text, metadata=md))

        self.logger.info(
            f"ğŸ“„ Created {len(graph_docs)} graph chunks, "
            f"{len(retrieval_docs)} retrieval chunks"
        )

        return graph_docs, retrieval_docs

    def get_dual_splitters(self) -> Tuple[SentenceSplitter, SentenceSplitter]:
        """
        Graphç”¨ã¨Retrievalç”¨ã®2ç³»çµ±ã‚’è¿”ã™ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç‰ˆï¼‰
        """
        # Graphç”¨ï¼šå°ã•ã‚ãƒãƒ£ãƒ³ã‚¯
        graph_splitter = SentenceSplitter(
            chunk_size=self.config['graph_chunk_size'],
            chunk_overlap=self.config['graph_chunk_overlap'],
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )

        # Retrievalç”¨ï¼šä¸­ã‚µã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯ï¼ˆ512ã«å¤‰æ›´ï¼‰
        retrieval_splitter = SentenceSplitter(
            chunk_size=self.config['retrieval_chunk_size'],  # 512
            chunk_overlap=self.config['retrieval_chunk_overlap'],  # 100
            paragraph_separator="\n\n",
            secondary_chunking_regex=r"[.!?ã€‚ï¼?]\s+"
        )

        self.logger.info(
            f"Splitters: graph={self.config['graph_chunk_size']}, "
            f"retrieval={self.config['retrieval_chunk_size']}"
        )

        return graph_splitter, retrieval_splitter

    def _find_overlapping_chunks(
        self,
        start: int,
        end: int,
        graph_docs: List[Document]
    ) -> List[str]:
        """
        æŒ‡å®šç¯„å›²ã¨é‡ãªã‚‹Graphãƒãƒ£ãƒ³ã‚¯ã®IDã‚’è¿”ã™

        Args:
            start, end: æ–‡å­—ä½ç½®
            graph_docs: åŒä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®Graphãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ

        Returns:
            é‡ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã®IDãƒªã‚¹ãƒˆ
        """
        overlapping = []

        for doc in graph_docs:
            g_start = doc.metadata.get('start_char', 0)
            g_end = doc.metadata.get('end_char', 0)

            # ç¯„å›²ã®é‡ãªã‚Šãƒã‚§ãƒƒã‚¯
            if not (end <= g_start or start >= g_end):
                overlapping.append(doc.metadata['chunk_id'])

        return overlapping

    def _generate_chunk_id(self, text: str, source_id: str, index: int) -> str:
        """
        ãƒãƒ£ãƒ³ã‚¯ã®ä¸€æ„ãªIDã‚’ç”Ÿæˆ

        Args:
            text: ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
            source_id: å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ID
            index: ãƒãƒ£ãƒ³ã‚¯ç•ªå·

        Returns:
            'doc123_chunk5_a7f3e9b2' ã®ã‚ˆã†ãªä¸€æ„ID
        """
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ï¼ˆæœ€åˆã®100æ–‡å­—ã‹ã‚‰ï¼‰
        text_hash = hashlib.md5(text[:100].encode()).hexdigest()[:8]
        return f"{source_id}_chunk{index}_{text_hash}"