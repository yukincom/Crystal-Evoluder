"""
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¯ãƒ©ã‚¹
"""
import json
import numpy as np
from typing import List, Dict, Any, Optional, Tuple

from llama_index.core import Document

from shared import ContentLevelDuplicateChecker, load_and_validate_paths

class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†ã‚’æ‹…å½“"""

    def __init__(self, logger):
        self.logger = logger

        LANGUAGE_CHUNK_CONFIG = {
            'en': {
                'retrieval_chunk_size': 320,
                'retrieval_chunk_overlap': 120,
                'graph_chunk_size': 512,
                'graph_chunk_overlap': 50,
            },
            'ja': {
                # æ—¥æœ¬èªã¯1æ–‡å­—ã‚ãŸã‚Šã®æƒ…å ±å¯†åº¦ãŒé«˜ã„ãŸã‚ã€ã‚µã‚¤ã‚ºã‚’ç¸®å°
                'retrieval_chunk_size': 200,    # è‹±èªã®ç´„60%
                'retrieval_chunk_overlap': 80,
                'graph_chunk_size': 350,        # è‹±èªã®ç´„70%
                'graph_chunk_overlap': 35,

            },
        }
    
    def _detect_document_language(self, text: str) -> str:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»è¦è¨€èªã‚’æ¤œå‡º
    
        Args:
            text: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    
        Returns:
            è¨€èªã‚³ãƒ¼ãƒ‰ ('en', 'ja', 'zh', etc.)
        """
    # ç°¡æ˜“åˆ¤å®šï¼ˆæœ¬æ ¼çš„ã«ã¯langdetectã‚’ä½¿ã†ï¼‰
        sample = text[:500]  # æœ€åˆã®500æ–‡å­—ã§åˆ¤å®š
    
    # æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰ã®å‰²åˆ
        ja_chars = sum(1 for c in sample if '\u3040' <= c <= '\u30ff' or '\u4e00' <= c <= '\u9faf')
        ja_ratio = ja_chars / max(len(sample), 1)
    
        if ja_ratio > 0.3:
            return 'ja'
    
    # ä¸­å›½èªç°¡ä½“å­—ã®åˆ¤å®šï¼ˆå¿…è¦ãªã‚‰ï¼‰
    # zh_chars = sum(1 for c in sample if '\u4e00' <= c <= '\u9faf')
    # if zh_chars / max(len(sample), 1) > 0.3:
    #     return 'zh'
    
        return 'en'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‹±èª


    def _get_language_aware_config(self, documents: List[Document]) -> Dict[str, int]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¾¤ã®è¨€èªã‚’æ¤œå‡ºã—ã€é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯è¨­å®šã‚’è¿”ã™
    
        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    
        Returns:
            è¨€èªåˆ¥ãƒãƒ£ãƒ³ã‚¯è¨­å®š
        """
    # æœ€åˆã®æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§è¨€èªã‚’åˆ¤å®š
        sample_size = min(5, len(documents))
        lang_counts = {}
    
        for doc in documents[:sample_size]:
            lang = self._detect_document_language(doc.text)
            lang_counts[lang] = lang_counts.get(lang, 0) + 1
    
    # æœ€ã‚‚å¤šã„è¨€èªã‚’æ¡ç”¨
        primary_lang = max(lang_counts.items(), key=lambda x: x[1])[0]
    
        config = LANGUAGE_CHUNK_CONFIG.get(primary_lang, LANGUAGE_CHUNK_CONFIG['en'])
    
        self.logger.info(
            f"ğŸŒ Detected primary language: {primary_lang} "
            f"(chunk_size: graph={config['graph_chunk_size']}, "
            f"retrieval={config['retrieval_chunk_size']})"
        )

        return config
    
# ChunkProcessor.create_dual_documents ã‚’æ›´æ–°
    def create_dual_documents(
        self,
        documents: List[Document],
        auto_detect_language: bool = True
    ) -> Tuple[List[Document], List[Document]]:
        """
        ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆè¨€èªå¯¾å¿œç‰ˆï¼‰
        """
    # è¨€èªæ¤œå‡ºã—ã¦ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if auto_detect_language:
            lang_config = self._get_language_aware_config(documents)
        
        # ä¸€æ™‚çš„ã«è¨­å®šã‚’ä¸Šæ›¸ã
            original_config = {
                'graph_chunk_size': self.config['graph_chunk_size'],
                'graph_chunk_overlap': self.config['graph_chunk_overlap'],
                'retrieval_chunk_size': self.config['retrieval_chunk_size'],
                'retrieval_chunk_overlap': self.config['retrieval_chunk_overlap'],
            }
        
            self.config.update(lang_config)
    
    # æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
        graph_docs = self._create_graph_chunks(documents)
        retrieval_docs = self._create_retrieval_chunks(documents)
    
    # è¨­å®šã‚’å…ƒã«æˆ»ã™
        if auto_detect_language:
            self.config.update(original_config)

        return graph_docs, retrieval_docs
    
    def load_documents(
        self,
        json_path: str,
        raw_docs: Optional[List[str]] = None,
        path_pickle: Optional[str] = None,
        kg=None,
        enable_duplicate_check: bool = True
    ) -> List[Document]:
        """
        JSON ã¨ ç”Ÿãƒ†ã‚­ã‚¹ãƒˆä¸¡æ–¹ã‹ã‚‰ Document ã‚’ä½œã‚‹

        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            raw_docs: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            path_pickle: ãƒ‘ã‚¹æƒ…å ±ã®Pickleãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ‘ã‚¹æƒ…å ±çµ±åˆæ™‚ã«å¿…è¦ï¼‰
            enable_duplicate_check: é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’æœ‰åŠ¹åŒ–

        Returns:
            Documentã®ãƒªã‚¹ãƒˆï¼ˆãƒ‘ã‚¹æƒ…å ±ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼‰
        """

        if enable_duplicate_check:
            content_checker = ContentLevelDuplicateChecker(
                similarity_threshold=0.85,
                neo4j_store=self.neo4j_store,  
                logger=self.logger
            )

            self.logger.info("ğŸ” Checking for content duplicates...")

        documents = []

        # --- JSON å´ ---
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # json_path ã® documents ã‚’è¿½åŠ 
        for i, doc in enumerate(data.get('documents', [])):
            doc_text = doc['text']
            if enable_duplicate_check:
                is_duplicate, similar_docs = content_checker.check_duplicate(
                    doc_text,
                    check_fuzzy=True,
                    check_neo4j=True
                )
                if is_duplicate:
                    self.logger.info(
                        f"  âŠ— Skipping duplicate document {i} "
                        f"(similar to: {similar_docs[0].get('doc_id')})"
                    )
                    continue  # é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                # é‡è¤‡ã§ãªã„å ´åˆã¯ç™»éŒ²
                content_checker.add_content(
                    text=doc_text,
                    doc_id=f"json_{i}",
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    },
                    save_to_neo4j=True,
                    store_full_text=False  # å…¨æ–‡ã¯ä¿å­˜ã—ãªã„ï¼ˆã‚µã‚¤ã‚ºå‰Šæ¸›ï¼‰
                )

            documents.append(
                Document(
                    text=doc['text'],
                    metadata={
                        "source": "json",
                        "json_id": i,
                        **doc.get("metadata", {})
                    }
                )
            )

        # --- ç”Ÿãƒ†ã‚­ã‚¹ãƒˆå´ ---
        if raw_docs:
            for i, text in enumerate(raw_docs):
                if enable_duplicate_check:
                    is_duplicate, similar_docs = content_checker.check_duplicate(
                        text,
                        check_fuzzy=True,
                        check_neo4j=True
                    )

                    if is_duplicate:
                        self.logger.info(
                            f"  âŠ— Skipping duplicate raw document {i}"
                        )
                        continue

                    content_checker.add_content(
                        text=text,
                        doc_id=f"raw_{i}",
                        metadata={"source": "raw", "raw_id": i},
                        save_to_neo4j=True,
                        store_full_text=False
                    )

                documents.append(
                    Document(
                        text=text,
                        metadata={
                            "source": "raw",
                            "raw_id": i
                        }
                    )
                )

        json_count = len(data.get('documents', []))
        raw_count = len(raw_docs) if raw_docs else 0

        self.logger.info(
            f"ğŸ“‚ Loaded {len(documents)} documents "
            f"({json_count} from JSON, {raw_count} raw texts)"
        )

        # --- ãƒ‘ã‚¹æƒ…å ±ã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰---
        if path_pickle and kg is not None:
            path_dicts = load_and_validate_paths(path_pickle, self.logger)
            if path_dicts:
                self.logger.info("Augmenting documents with path information...")
                documents = self.augment_documents_with_paths(
                    documents,
                    path_dicts,
                    kg,
                    entity_embeddings=None  # å¿…è¦ã«å¿œã˜ã¦æ¸¡ã™
                )
                self.logger.info(f"âœ… Path information added to {len(documents)} documents")
            else:
                self.logger.warning("Path information could not be loaded, continuing without it")

        return documents

    def augment_documents_with_paths(
        self,
        documents: List[Document],
        path_dicts: List[Dict],
        kg,
        entity_embeddings: Dict[str, Any] = None,
        match_key='question'
    ) -> List[Document]:
        """
        documents ã«å¯¾å¿œã™ã‚‹ path æƒ…å ±ã‚’æ³¨å…¥

        Args:
            documents: Documentã®ãƒªã‚¹ãƒˆ
            path_dicts: load_path_dicts ã®æˆ»ã‚Šå€¤
            kg: ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•
            entity_embeddings: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            match_key: documents ã¨ path_dicts ã‚’çªãåˆã‚ã›ã‚‹ã‚­ãƒ¼

        Returns:
            ãƒ‘ã‚¹æƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸ documents
        """

        # defensive
        if entity_embeddings is None:
            entity_embeddings = {}

        # path_dictsãŒç©ºãªã‚‰å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        if len(path_dicts) == 0:
            self.logger.info("  â†’ No path information available, returning original documents")
            return documents

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼š path_dicts ã‚’ match_key ã§å¼•ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹
        pd_map = {}
        for p in path_dicts:
            key = p.get(match_key)
            if key is not None:
                pd_map[key] = p

        augmented = []
        matched_count = 0

        for doc in documents:
            meta = dict(getattr(doc, 'metadata', {}) or {})
            doc_key = meta.get(match_key)

            matched = None
            if doc_key is not None and doc_key in pd_map:
                matched = pd_map[doc_key]
                matched_count += 1
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆå†…ã« match_key ã®æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ path_dict ã‚’æ¢ã™
                text = getattr(doc, 'text', '') or ''
                for k, p in pd_map.items():
                    if isinstance(k, str) and k in text:
                        matched = p
                        matched_count += 1
                        break

            paths_meta = []
            if matched:
                for path in matched.get('translated_paths', []):
                    # path: list of node names (entities)
                    path_len = len(path)
                    edge_weights = []
                    path_node_pairs = list(zip(path[:-1], path[1:])) if path_len >= 2 else []
                    for u, v in path_node_pairs:
                        if kg.has_edge(u, v):
                            edge_weights.append(kg[u][v].get('weight', 0.0))
                        elif kg.has_edge(v, u):
                            edge_weights.append(kg[v][u].get('weight', 0.0))
                        else:
                            # edge ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ 0.0 ã‚’å…¥ã‚Œã¦ãŠã
                            edge_weights.append(0.0)

                    avg_edge_weight = float(np.mean(edge_weights)) if edge_weights else 0.0
                    sum_edge_weight = float(np.sum(edge_weights)) if edge_weights else 0.0

                    # path å†…ãƒãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°ã€ãƒãƒ¼ãƒ‰é–“é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆå¹³å‡ãƒšã‚¢é¡ä¼¼åº¦ï¼‰
                    pair_sims = []
                    for i in range(len(path) - 1):
                        e1 = entity_embeddings.get(path[i])
                        e2 = entity_embeddings.get(path[i + 1])
                        if e1 is not None and e2 is not None:
                            # safe numpy dot / norms
                            denom = (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-9)
                            pair_sims.append(float(np.dot(e1, e2) / denom))
                    avg_pair_sim = float(np.mean(pair_sims)) if pair_sims else None

                    # æœ€çŸ­è·é›¢ï¼ˆkg ä¸Šï¼‰ â€” å­˜åœ¨ã—ãªã‘ã‚Œã° None
                    shortest = None
                    try:
                        if path_len >= 2:
                            # path ã®ç«¯åŒå£«ã®æœ€çŸ­é•·ã‚’è¨ˆç®—ï¼ˆä¾‹ï¼‰
                            s1, s2 = path[0], path[-1]
                            if kg.has_node(s1) and kg.has_node(s2):
                                shortest = int(kg.shortest_path_length(s1, s2))
                    except Exception:
                        pass

                    paths_meta.append({
                        'path': path,
                        'path_length_nodes': path_len,
                        'avg_edge_weight': avg_edge_weight,
                        'sum_edge_weight': sum_edge_weight,
                        'avg_adjacent_node_sim': avg_pair_sim,
                        'kg_shortest_between_ends': shortest
                    })

            # attach (æ—¢å­˜ metadata ã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼)
            new_meta = dict(meta)
            new_meta['paths'] = paths_meta
            # create a new Document preserving original text & adding metadata (or mutate in place if ok)
            new_doc = Document(text=getattr(doc, 'text', ''), metadata=new_meta)
            augmented.append(new_doc)
            # ãƒãƒƒãƒã—ãªã‹ã£ãŸå ´åˆã‚‚å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿æŒ
            augmented.append(doc)

        self.logger.info(
            f"  â†’ Matched {matched_count}/{len(documents)} documents with path information"
        )
        return augmented
    
class DocumentLoader:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

    def __init__(self, logger=None):
        self.logger = logger
        self.processor = DocumentProcessor(logger)

    def load_from_json(self, json_path: str) -> List[Document]:
        """
        JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰

        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            Documentã®ãƒªã‚¹ãƒˆ
        """
        return self.processor.load_documents(json_path)

    def load_from_text(self, texts: List[str]) -> List[Document]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰

        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ

        Returns:
            Documentã®ãƒªã‚¹ãƒˆ
        """
        return self.processor.load_documents(
            json_path="",  # ç©ºã®JSONãƒ‘ã‚¹
            raw_docs=texts
        )
