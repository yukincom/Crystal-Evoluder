"""
Multi-hopæ¢ç´¢ã‚¯ãƒ©ã‚¹
"""
import numpy as np
from typing import List, Dict, Any, Set, Tuple

import networkx as nx
from ..model. embed import ensure_bge_m3
from ..builders.retrieval_builder import RetrievalBuilder


class MultiHopExplorer:
    """Multi-hopæ¢ç´¢ã‚’æ‹…å½“"""

    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger
        self.visited_paths = set()

    def explore_multi_hop_paths(
        self,
        kg: nx.Graph,
        query: str,
        retrieval_chunks: List[str] = None,
        max_steps: int = 5,
        top_k_per_hop: int = 3,
        confidence_threshold: float = 0.7,
        extend_on_low_confidence: bool = True
    ) -> Dict[str, Any]:
        """
        Multi-hopæ¢ç´¢ã‚’å®Ÿè¡Œ

        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            retrieval_chunks: é–‹å§‹ç‚¹ã¨ãªã‚‹ãƒãƒ£ãƒ³ã‚¯IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_steps: æœ€å¤§ãƒ›ãƒƒãƒ—æ•°
            top_k_per_hop: å„ãƒ›ãƒƒãƒ—ã§æ¢ç´¢ã™ã‚‹ä¸Šä½Kå€‹
            confidence_threshold: ä¿¡é ¼åº¦ã®é–¾å€¤
            extend_on_low_confidence: ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã«æ¢ç´¢ã‚’æ‹¡å¼µã™ã‚‹ã‹

        Returns:
            {
                'paths': ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚ŒãŸãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ,
                'entities': è¨ªå•ã—ãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£,
                'confidence': ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢,
                'steps_used': å®Ÿéš›ã«ä½¿ç”¨ã—ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°,
                'evidence': ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
            }
        """
        self.logger.info(f"ğŸ” Starting multi-hop exploration (max_steps={max_steps})")

        # ============================================================
        # 1. é–‹å§‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ±ºå®š
        # ============================================================
        start_entities = set()

        if retrieval_chunks:
            # Retrievalã§å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é–‹å§‹
            start_entities = self._resolve_entities_from_chunks(retrieval_chunks, kg)

        if not start_entities:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
            start_entities = self._extract_query_entities(query, kg, top_k=5)

        if not start_entities:
            self.logger.warning("âš ï¸  No starting entities found")
            return {
                'paths': [],
                'entities': [],
                'confidence': 0.0,
                'steps_used': 0,
                'evidence': []
            }

        self.logger.info(f"  â†’ Starting from {len(start_entities)} entities: {list(start_entities)[:3]}...")

        # ============================================================
        # 2. å„é–‹å§‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æ¢ç´¢
        # ============================================================
        all_paths = []
        visited_entities = set()
        evidence_texts = []

        for start_entity in list(start_entities)[:top_k_per_hop]:
            if start_entity not in kg.nodes():
                self.logger.debug(f"  Entity '{start_entity}' not in graph, skipping")
                continue

            path_result = self._explore_from_entity(
                kg,
                start_entity,
                query,
                max_steps=max_steps,
                visited=set()
            )

            all_paths.extend(path_result['paths'])
            visited_entities.update(path_result['visited'])

            # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
            for path_info in path_result['paths'][:5]:  # Top 5ã®ã¿
                path = path_info['path']
                evidence_texts.append(' â†’ '.join(path))

        # å…¨ä½“ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—
        if all_paths:
            confidence = np.mean([p['score'] for p in all_paths])
        else:
            confidence = 0.0

        current_step = max_steps

        self.logger.info(
            f"  â†’ Found {len(all_paths)} paths with confidence {confidence:.2f}"
        )

        # ============================================================
        # 3. ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯æ‹¡å¼µ
        # ============================================================
        if extend_on_low_confidence and confidence < confidence_threshold:
            extended_steps = max_steps + 2
            self.logger.info(
                f"  â†’ Low confidence ({confidence:.2f} < {confidence_threshold}), "
                f"extending to {extended_steps} steps"
            )

            # å†æ¢ç´¢
            extended_paths = []
            for start_entity in list(start_entities)[:top_k_per_hop]:
                if start_entity not in kg.nodes():
                    continue

                path_result = self._explore_from_entity(
                    kg,
                    start_entity,
                    query,
                    max_steps=extended_steps,
                    visited=set()  # ãƒªã‚»ãƒƒãƒˆ
                )

                extended_paths.extend(path_result['paths'])
                confidence = max(confidence, path_result['confidence'])

            if len(extended_paths) > len(all_paths):
                all_paths = extended_paths
                current_step = extended_steps
                self.logger.info(f"  â†’ Extended search found {len(all_paths)} paths")

        # ============================================================
        # 4. ãƒ‘ã‚¹ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        # ============================================================
        ranked_paths = self._rank_paths(all_paths, query, kg)

        return {
            'paths': ranked_paths[:10],  # Top 10
            'entities': list(visited_entities),
            'confidence': confidence,
            'steps_used': current_step,
            'evidence': evidence_texts
        }

    def _precompute_representative_paths(
        self,
        kg: nx.Graph,
        documents: List[Any],
        num_sample_queries: int = 10
    ) -> None:
        """
        ä»£è¡¨çš„ãªã‚¯ã‚¨ãƒªã§ãƒ‘ã‚¹ã‚’äº‹å‰è¨ˆç®—ã—ã€ã‚°ãƒ©ãƒ•ã«ä¿å­˜

        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            num_sample_queries: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªæ•°
        """
        self.logger.info(f"Computing representative paths for {num_sample_queries} sample queries...")

        # ============================================================
        # 1. ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ
        # ============================================================
        sample_queries = self._generate_sample_queries(documents, kg, num_sample_queries)

        if not sample_queries:
            self.logger.warning("  â†’ No sample queries generated, skipping path pre-computation")
            return

        self.logger.info(f"  Generated {len(sample_queries)} sample queries")

        # ============================================================
        # 2. å„ã‚¯ã‚¨ãƒªã§Multi-hopæ¢ç´¢ã‚’å®Ÿè¡Œ
        # ============================================================
        all_paths = []
        path_count = 0

        for i, query in enumerate(sample_queries):
            try:
                result = self.explore_multi_hop_paths(
                    kg=kg,
                    query=query,
                    max_steps=5,
                    top_k_per_hop=3,
                    extend_on_low_confidence=False  # äº‹å‰è¨ˆç®—ã§ã¯æ‹¡å¼µã—ãªã„
                )

                # é«˜å“è³ªãªãƒ‘ã‚¹ã®ã¿ä¿å­˜ï¼ˆconfidence > 0.5ï¼‰
                for path_info in result['paths']:
                    if path_info.get('final_score', 0) > 0.5:
                        all_paths.append(path_info)
                        path_count += 1

                if (i + 1) % 5 == 0:
                    self.logger.info(f"  Processed {i+1}/{len(sample_queries)} queries...")

            except Exception as e:
                self.logger.debug(f"  Query '{query[:30]}...' failed: {type(e).__name__}")
                continue

        self.logger.info(f"  â†’ Computed {path_count} high-quality paths")

        # ============================================================
        # 3. ãƒ‘ã‚¹æƒ…å ±ã‚’ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸ã«ä¿å­˜
        # ============================================================
        self._store_paths_in_graph(kg, all_paths)

    def _generate_sample_queries(
        self,
        documents: List[Any],
        kg: nx.Graph,
        num_queries: int = 10
    ) -> List[str]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ä»£è¡¨çš„ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ

        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•
            num_queries: ç”Ÿæˆã™ã‚‹ã‚¯ã‚¨ãƒªæ•°

        Returns:
            ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
        """
        queries = []

        # ============================================================
        # æˆ¦ç•¥1: ä¸­å¿ƒæ€§ã®é«˜ã„ãƒãƒ¼ãƒ‰ã‚’ã‚¯ã‚¨ãƒªã«ã™ã‚‹
        # ============================================================
        try:
            # æ¬¡æ•°ä¸­å¿ƒæ€§ã‚’è¨ˆç®—
            degree_centrality = nx.degree_centrality(kg)

            # ä¸Šä½ãƒãƒ¼ãƒ‰ã‚’å–å¾—
            top_nodes = sorted(
                degree_centrality.items(),
                key=lambda x: x[1],
                reverse=True
            )[:num_queries // 2]

            # ãƒãƒ¼ãƒ‰åã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨
            for node, _ in top_nodes:
                queries.append(f"What is {node}?")
                queries.append(f"How does {node} work?")

        except Exception as e:
            self.logger.debug(f"Centrality-based query generation failed: {e}")

        # ============================================================
        # æˆ¦ç•¥2: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        # ============================================================
        for doc in documents[:num_queries // 2]:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«'question'ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
            question = doc.metadata.get('question')
            if question:
                queries.append(question)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®æ–‡ã‚’ä½¿ç”¨
                text = doc.text.strip()
                if text:
                    first_sentence = text.split('.')[0][:100]
                    if len(first_sentence) > 10:
                        queries.append(first_sentence)

        # ============================================================
        # æˆ¦ç•¥3: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢ã®é–¢ä¿‚ã‚’å•ã†ã‚¯ã‚¨ãƒª
        # ============================================================
        try:
            # é‡ã¿ã®é«˜ã„ã‚¨ãƒƒã‚¸ã‚’å–å¾—
            high_weight_edges = sorted(
                kg.edges(data=True),
                key=lambda x: x[2].get('weight', 0),
                reverse=True
            )[:num_queries // 3]

            for u, v, data in high_weight_edges:
                relation = data.get('relation', 'related to')
                queries.append(f"How is {u} {relation} {v}?")

        except Exception as e:
            self.logger.debug(f"Edge-based query generation failed: {e}")

        # é‡è¤‡ã‚’é™¤å»ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        queries = list(set(queries))
        import random
        random.shuffle(queries)

        return queries[:num_queries]

    def _store_paths_in_graph(
        self,
        kg: nx.Graph,
        paths: List[Dict]
    ) -> None:
        """
        è¨ˆç®—ã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰/ã‚¨ãƒƒã‚¸å±æ€§ã«ä¿å­˜

        Args:
            kg: NetworkXã‚°ãƒ©ãƒ•
            paths: ãƒ‘ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        self.logger.info("  Storing path information in graph...")

        # ============================================================
        # 1. å„ãƒãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ãƒ‘ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        # ============================================================
        from collections import defaultdict
        node_path_counts = defaultdict(int)
        node_avg_scores = defaultdict(list)

        for path_info in paths:
            path = path_info.get('path', [])
            score = path_info.get('final_score', 0)

            for node in path:
                if kg.has_node(node):
                    node_path_counts[node] += 1
                    node_avg_scores[node].append(score)

        # ãƒãƒ¼ãƒ‰ã«å±æ€§ã‚’è¿½åŠ 
        for node in kg.nodes():
            kg.nodes[node]['path_frequency'] = node_path_counts.get(node, 0)

            scores = node_avg_scores.get(node, [])
            if scores:
                kg.nodes[node]['avg_path_score'] = float(np.mean(scores))
            else:
                kg.nodes[node]['avg_path_score'] = 0.0

        # ============================================================
        # 2. å„ã‚¨ãƒƒã‚¸ãŒå«ã¾ã‚Œã‚‹ãƒ‘ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        # ============================================================
        edge_path_counts = defaultdict(int)
        edge_avg_scores = defaultdict(list)

        for path_info in paths:
            path = path_info.get('path', [])
            score = path_info.get('final_score', 0)

            # ãƒ‘ã‚¹å†…ã®é€£ç¶šã™ã‚‹ãƒãƒ¼ãƒ‰ãƒšã‚¢ã‚’ã‚¨ãƒƒã‚¸ã¨ã—ã¦è¨˜éŒ²
            for i in range(len(path) - 1):
                u, v = path[i], path[i + 1]

                # ç„¡å‘ã‚°ãƒ©ãƒ•ã¨ã—ã¦æ‰±ã†
                edge_key = tuple(sorted([u, v]))
                edge_path_counts[edge_key] += 1
                edge_avg_scores[edge_key].append(score)

        # ã‚¨ãƒƒã‚¸ã«å±æ€§ã‚’è¿½åŠ 
        for u, v in kg.edges():
            edge_key = tuple(sorted([u, v]))

            kg[u][v]['path_frequency'] = edge_path_counts.get(edge_key, 0)

            scores = edge_avg_scores.get(edge_key, [])
            if scores:
                kg[u][v]['avg_path_score'] = float(np.mean(scores))
            else:
                kg[u][v]['avg_path_score'] = 0.0

        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        total_nodes_with_paths = sum(1 for n in kg.nodes() if kg.nodes[n]['path_frequency'] > 0)
        total_edges_with_paths = sum(1 for u, v in kg.edges() if kg[u][v]['path_frequency'] > 0)

        self.logger.info(
            f"  â†’ {total_nodes_with_paths}/{len(kg.nodes())} nodes and "
            f"{total_edges_with_paths}/{len(kg.edges())} edges have path information"
        )

    def _extract_query_entities(
        self,
        query: str,
        kg: nx.Graph,
        top_k: int = 5
    ) -> Set[str]:
        """
        ã‚¯ã‚¨ãƒªã‹ã‚‰é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            kg: NetworkXã‚°ãƒ©ãƒ•
            top_k: ä¸Šä½Kå€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¿”ã™

        Returns:
            ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®ã‚»ãƒƒãƒˆ
        """

        embed_model = ensure_bge_m3()

        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_emb = embed_model.get_text_embedding(query)
        query_emb = np.array(query_emb, dtype=np.float32)
        norm = np.linalg.norm(query_emb)
        if norm > 1e-9:
            query_emb = query_emb / norm

        # å…¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
        entity_scores = []

        for entity in kg.nodes():
            try:
                entity_emb = embed_model.get_text_embedding(entity)
                entity_emb = np.array(entity_emb, dtype=np.float32)
                norm = np.linalg.norm(entity_emb)
                if norm > 1e-9:
                    entity_emb = entity_emb / norm

                similarity = float(np.dot(query_emb, entity_emb))
                entity_scores.append((entity, similarity))

            except Exception:
                continue

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        entity_scores.sort(key=lambda x: x[1], reverse=True)

        # Top K ã‚’è¿”ã™
        top_entities = {entity for entity, _ in entity_scores[:top_k]}

        return top_entities

    def _resolve_entities_from_chunks(
        self,
        chunk_ids: Set[str],
        kg: nx.Graph
    ) -> Set[str]:
        """
        ãƒãƒ£ãƒ³ã‚¯IDã‹ã‚‰å®Ÿéš›ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã«å¤‰æ›

        Args:
            chunk_ids: ãƒãƒ£ãƒ³ã‚¯IDã®ã‚»ãƒƒãƒˆ
            kg: NetworkXã‚°ãƒ©ãƒ•

        Returns:
            ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®ã‚»ãƒƒãƒˆ
        """
        entities = set()

        for chunk_id in chunk_ids:
            # chunk_idãŒã™ã§ã«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®å ´åˆ
            if chunk_id in kg.nodes():
                entities.add(chunk_id)
                continue

            # ============================================================
            # 2. ãƒãƒ£ãƒ³ã‚¯IDã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¨å®š
            # ============================================================

            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "doc_X_chunkY_hash" å½¢å¼
            # â†’ ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰å±æ€§ 'chunk_id' ã‚’æŒã¤ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
            for node, data in kg.nodes(data=True):
                node_chunk_ids = data.get('chunk_ids', [])

                # chunk_ids ãŒæ–‡å­—åˆ—ã®å ´åˆã‚‚ã‚ã‚‹ã®ã§å¯¾å¿œ
                if isinstance(node_chunk_ids, str):
                    node_chunk_ids = [node_chunk_ids]

                if chunk_id in node_chunk_ids:
                    entities.add(node)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒ£ãƒ³ã‚¯IDå†…ã«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åãŒå«ã¾ã‚Œã‚‹
            # ï¼ˆä¾‹: chunk_id = "attention_mechanism_chunk3"ï¼‰
            # â†’ ã‚°ãƒ©ãƒ•å†…ã®ãƒãƒ¼ãƒ‰åãŒchunk_idã«éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            chunk_id_lower = chunk_id.lower()
            for node in kg.nodes():
                node_lower = node.lower()

                # éƒ¨åˆ†ä¸€è‡´ï¼ˆå°‘ãªãã¨ã‚‚5æ–‡å­—ä»¥ä¸Šï¼‰
                if len(node_lower) >= 5 and node_lower in chunk_id_lower:
                    entities.add(node)
                elif len(chunk_id_lower) >= 5 and chunk_id_lower in node_lower:
                    entities.add(node)

        if not entities:
            self.logger.debug(
                f"  Could not resolve entities from {len(chunk_ids)} chunk IDs"
            )

        return entities

    def _explore_from_entity(
        self,
        kg: nx.Graph,
        start_entity: str,
        query: str,
        max_steps: int,
        visited: Set[str]
    ) -> Dict[str, Any]:
        """
        ç‰¹å®šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æ·±ã•å„ªå…ˆæ¢ç´¢

        Returns:
            {
                'paths': [ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ],
                'visited': è¨ªå•ãƒãƒ¼ãƒ‰,
                'steps': æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°,
                'confidence': ä¿¡é ¼åº¦
            }
        """
        paths = []
        visited.add(start_entity)

        embed_model = ensure_bge_m3()

        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        query_emb = embed_model.get_text_embedding(query)
        query_emb = np.array(query_emb, dtype=np.float32)
        norm = np.linalg.norm(query_emb)
        if norm > 1e-9:
            query_emb = query_emb / norm

        # BFS
        queue = [(start_entity, [start_entity], 0)]  # (current, path, depth)
        # ãƒ‘ã‚¹æ•°åˆ¶é™
        max_paths = self.config.get('multihop_max_paths', 50)

        while queue and len(paths) < max_paths:
            current, path, depth = queue.pop(0)

            if depth >= max_steps:
                continue

            # éš£æ¥ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢
            neighbors = list(kg.neighbors(current))

            # å„éš£æ¥ãƒãƒ¼ãƒ‰ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            neighbor_scores = []
            for neighbor in neighbors:
                if neighbor in visited:
                    continue

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®åŸ‹ã‚è¾¼ã¿
                try:
                    entity_emb = embed_model.get_text_embedding(neighbor)
                    entity_emb = np.array(entity_emb, dtype=np.float32)
                    norm = np.linalg.norm(entity_emb)
                    if norm > 1e-9:
                        entity_emb = entity_emb / norm

                    # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦
                    similarity = float(np.dot(query_emb, entity_emb))

                    # ã‚¨ãƒƒã‚¸ã®é‡ã¿
                    edge_weight = kg[current][neighbor].get('weight', 0.5)

                    # ç·åˆã‚¹ã‚³ã‚¢
                    score = similarity * 0.6 + edge_weight * 0.4

                    neighbor_scores.append((neighbor, score))
                except Exception:
                    continue

            # ã‚¹ã‚³ã‚¢ä¸Šä½ã‚’é¸æŠ
            neighbor_scores.sort(key=lambda x: x[1], reverse=True)
            beam_width = self.config.get('multihop_beam_width', 2)
            top_neighbors = neighbor_scores[:beam_width]  # 3 â†’ 2

            for neighbor, score in top_neighbors:
                new_path = path + [neighbor]

                # ãƒ‘ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯
                path_tuple = tuple(new_path)
                if path_tuple in self.visited_paths:
                    continue
                self.visited_paths.add(path_tuple)

                visited.add(neighbor)

                # ãƒ‘ã‚¹ã‚’ä¿å­˜
                paths.append({
                    'path': new_path,
                    'score': score,
                    'depth': depth + 1
                })

                # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                queue.append((neighbor, new_path, depth + 1))

        # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆãƒ‘ã‚¹ã®å¹³å‡ã‚¹ã‚³ã‚¢ï¼‰
        confidence = np.mean([p['score'] for p in paths]) if paths else 0.0

        return {
            'paths': paths,
            'visited': visited,
            'steps': max_steps,
            'confidence': float(confidence)
        }

    def _rank_paths(
        self,
        paths: List[Dict],
        query: str,
        kg: nx.Graph
    ) -> List[Dict]:
        """
        ãƒ‘ã‚¹ã‚’ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        """
        if not paths:
            return []
        embed_model = ensure_bge_m3()

        query_emb = embed_model.get_text_embedding(query)
        query_emb = np.array(query_emb, dtype=np.float32)
        norm = np.linalg.norm(query_emb)
        if norm > 1e-9:
            query_emb = query_emb / norm

        # å„ãƒ‘ã‚¹ã«æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for path_info in paths:
            path = path_info['path']

            # ãƒ‘ã‚¹ã®é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé•·ã™ãã‚‹ã¨ä¿¡é ¼åº¦ä½ä¸‹ï¼‰
            length_penalty = 1.0 / (1.0 + 0.1 * len(path))

            # ã‚¨ãƒƒã‚¸é‡ã¿ã®å¹³å‡
            edge_weights = []
            for i in range(len(path) - 1):
                if kg.has_edge(path[i], path[i+1]):
                    edge_weights.append(kg[path[i]][path[i+1]].get('weight', 0.5))

            avg_edge_weight = np.mean(edge_weights) if edge_weights else 0.5
            #  ãƒ‘ã‚¹å…¨ä½“ã¨ã‚¯ã‚¨ãƒªã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢
            path_query_relevance = 0.0
            entity_similarities = []

            for entity in path:
                try:
                    entity_emb = embed_model.get_text_embedding(entity)
                    entity_emb = np.array(entity_emb, dtype=np.float32)
                    norm = np.linalg.norm(entity_emb)
                    if norm > 1e-9:
                        entity_emb = entity_emb / norm

                    similarity = float(np.dot(query_emb, entity_emb))
                    entity_similarities.append(similarity)
                except Exception:
                    continue

            if entity_similarities:
                # ãƒ‘ã‚¹å†…ã®æœ€å¤§é¡ä¼¼åº¦ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚é–¢é€£ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é‡è¦–ï¼‰
                path_query_relevance = max(entity_similarities)

            # æœ€çµ‚ã‚¹ã‚³ã‚¢
            final_score = (
                path_info['score'] * 0.4 +
                avg_edge_weight * 0.25 +
                length_penalty * 0.15 +
                path_query_relevance * 0.2
            )

            path_info['final_score'] = final_score
            path_info['query_relevance'] = path_query_relevance  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        paths.sort(key=lambda x: x.get('final_score', 0), reverse=True)

        return paths

    def query_with_multihop(
        self,
        query: str,
        kg: nx.Graph,
        retrieval_store: Dict = None,
        max_steps: int = 5,
        top_k_retrieval: int = 5,
        top_k_paths: int = 10
    ) -> Dict[str, Any]:
        """
        Multi-hopæ¢ç´¢ã‚’ä½¿ã£ãŸã‚¯ã‚¨ãƒªå®Ÿè¡Œ

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            kg: NetworkXã‚°ãƒ©ãƒ•
            retrieval_store: Retrievalã‚¹ãƒˆã‚¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_steps: æœ€å¤§ãƒ›ãƒƒãƒ—æ•°
            top_k_retrieval: Retrievalçµæœã®ä¸Šä½Kä»¶
            top_k_paths: è¿”ã™ãƒ‘ã‚¹ã®ä¸Šä½Kä»¶

        Returns:
            {
                'paths': ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¹,
                'retrieval_docs': Retrievalã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ,
                'confidence': ä¿¡é ¼åº¦,
                'answer': çµ±åˆã•ã‚ŒãŸå›ç­”ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            }
        """
        self.logger.info(f"ğŸ” Query: '{query}'")

        results = {
            'paths': [],
            'retrieval_docs': [],
            'confidence': 0.0,
            'answer': None
        }

        # ============================================================
        # 1. Retrievalï¼ˆæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        # ============================================================
        retrieval_chunks = []

        if retrieval_store:
            try:
                embed_model = ensure_bge_m3()
                retriever = RetrievalBuilder(embed_model, self.logger)

                retrieval_results = retriever.retrieve(
                    store=retrieval_store,
                    query=query,
                    top_k=top_k_retrieval
                )

                for score, doc, graph_chunk_ids in retrieval_results:
                    results['retrieval_docs'].append({
                        'text': doc.text,
                        'score': score,
                        'metadata': doc.metadata
                    })
                    retrieval_chunks.extend(graph_chunk_ids)

                self.logger.info(
                    f"  â†’ Retrieval: {len(results['retrieval_docs'])} docs, "
                    f"{len(retrieval_chunks)} graph chunks"
                )

            except Exception as e:
                self.logger.warning(f"âš ï¸  Retrieval failed: {type(e).__name__}")

        # ============================================================
        # 2. Multi-hopæ¢ç´¢
        # ============================================================
        try:
            path_result = self.explore_multi_hop_paths(
                kg=kg,
                query=query,
                retrieval_chunks=retrieval_chunks if retrieval_chunks else None,
                max_steps=max_steps,
                top_k_per_hop=3,
                confidence_threshold=0.7,
                extend_on_low_confidence=True
            )

            results['paths'] = path_result['paths'][:top_k_paths]
            results['confidence'] = path_result['confidence']

            self.logger.info(
                f"  â†’ Multi-hop: {len(results['paths'])} paths, "
                f"confidence={results['confidence']:.2f}"
            )

        except Exception as e:
            self.logger.error(f"ğŸš¨ Multi-hop exploration failed: {type(e).__name__}")
            self.logger.error(f"   {str(e)[:200]}")

            if self.logger.level <= 10:  # logging.DEBUG
                import traceback
                self.logger.debug(traceback.format_exc())

        # ============================================================
        # 3. çµæœã®çµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        # ============================================================
        if results['paths'] and results['retrieval_docs']:
            results['answer'] = self._synthesize_answer(
                query=query,
                paths=results['paths'],
                retrieval_docs=results['retrieval_docs']
            )

        return results

    def _synthesize_answer(
        self,
        query: str,
        paths: List[Dict],
        retrieval_docs: List[Dict]
    ) -> str:
        """
        ãƒ‘ã‚¹ã¨Retrievalãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å›ç­”ã‚’çµ±åˆ

        Args:
            query: ã‚¯ã‚¨ãƒª
            paths: Multi-hopã§ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¹
            retrieval_docs: Retrievalã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

        Returns:
            çµ±åˆã•ã‚ŒãŸå›ç­”æ–‡å­—åˆ—
        """
        # ç°¡æ˜“å®Ÿè£…ï¼ˆLLMã‚’ä½¿ã£ãŸçµ±åˆã¯åˆ¥é€”å®Ÿè£…å¯èƒ½ï¼‰

        answer_parts = []

        # ãƒ‘ã‚¹ã‹ã‚‰ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
        answer_parts.append("**From Knowledge Graph:**")
        for i, path_info in enumerate(paths[:3], 1):
            path = path_info['path']
            score = path_info.get('final_score', 0)
            path_str = ' â†’ '.join(path)
            answer_parts.append(f"{i}. {path_str} (score: {score:.2f})")

        # Retrievalãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
        answer_parts.append("\n**From Documents:**")
        for i, doc_info in enumerate(retrieval_docs[:3], 1):
            text_preview = doc_info['text'][:150] + "..."
            score = doc_info['score']
            answer_parts.append(f"{i}. {text_preview} (score: {score:.2f})")

        return '\n'.join(answer_parts)