"""
AI Router - API/Ollama Ëá™ÂãïÂàá„ÇäÊõø„Åà„Ç∑„Çπ„ÉÜ„É†
"""
import os
import json
import requests
from enum import Enum
from typing import Dict, Any, Optional, Union
from openai import OpenAI

from .logger import setup_logger

class TaskType(Enum):
    """„Çø„Çπ„ÇØ„Çø„Ç§„Éó"""
    TRIPLET_EXTRACTION = "triplet_extraction"
    QUALITY_CHECK = "quality_check"
    SELF_RAG = "self_rag"
    GENERAL = "general"

class AIRouter:
    """AI Router - OpenAI API „Å® Ollama „ÅÆËá™ÂãïÂàá„ÇäÊõø„Åà"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, logger=None):
        self.config = config or {}
        self.logger = logger or setup_logger('AIRouter')

        # „Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö
        self.task_defaults = {
            TaskType.TRIPLET_EXTRACTION: {
                'api_model': 'gpt-4o-mini',
                'ollama_model': 'llama3.1:70b',
                'temperature': 0.3,
                'max_tokens': 1024
            },
            TaskType.QUALITY_CHECK: {
                'api_model': 'gpt-4o-mini',
                'ollama_model': 'qwen2.5:32b',
                'temperature': 0.1,
                'max_tokens': 512
            },
            TaskType.SELF_RAG: {
                'api_model': 'gpt-4o-mini',
                'ollama_model': 'llama3.1:70b',
                'temperature': 0.7,
                'max_tokens': 2048
            },
            TaskType.GENERAL: {
                'api_model': 'gpt-4o-mini',
                'ollama_model': 'llama3.1:8b',
                'temperature': 0.5,
                'max_tokens': 1024
            }
        }

        # AI„É´„Éº„ÉÜ„Ç£„É≥„Ç∞Ë®≠ÂÆö
        self.ai_routing = self.config.get('ai_routing', {
            'mode': 'api',  # 'api' or 'ollama'
            'ollama_url': 'http://localhost:11434',
            'api_key': os.environ.get('OPENAI_API_KEY')
        })

        # „É¢„Éº„ÉâË®≠ÂÆö
        self.mode = self.ai_routing.get('mode', 'api')
        self.ollama_url = self.ai_routing.get('ollama_url', 'http://localhost:11434')
        self.api_key = self.ai_routing.get('api_key') or os.environ.get('OPENAI_API_KEY')

        # OllamaÊé•Á∂öÁ¢∫Ë™ç
        self.ollama_available = self._check_ollama()

        # OpenAI„ÇØ„É©„Ç§„Ç¢„É≥„ÉàÂàùÊúüÂåñ
        self.openai_client = None
        if self.api_key:
            self.openai_client = OpenAI(api_key=self.api_key)

        self.logger.info(f"‚úÖ AI Router initialized (mode: {self.mode})")
        if self.ollama_available:
            self.logger.info("‚úÖ Ollama available")
        else:
            self.logger.warning("‚ö†Ô∏è  Ollama not available (API mode only)")

    def _check_ollama(self) -> bool:
        """Ollama„Çµ„Éº„Éê„Éº„ÅåËµ∑Âãï„Åó„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

    def call(self, task: Union[TaskType, str], prompt: str, system_prompt: str = "", **kwargs) -> str:
        """
        AIÂëº„Å≥Âá∫„ÅóÔºàËá™ÂãïÂàá„ÇäÊõø„ÅàÔºâ

        Args:
            task: „Çø„Çπ„ÇØ„Çø„Ç§„Éó
            prompt: „É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„Éà
            system_prompt: „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà
            **kwargs: ËøΩÂä†„Éë„É©„É°„Éº„Çø

        Returns:
            AIÂøúÁ≠î
        """
        if isinstance(task, str):
            task = TaskType(task)

        # „Çø„Çπ„ÇØË®≠ÂÆöÂèñÂæó
        task_config = self.task_defaults.get(task, self.task_defaults[TaskType.GENERAL])

        # „É¢„Éº„ÉâÊ±∫ÂÆö
        if self.mode == 'ollama' and self.ollama_available:
            return self._call_ollama(task_config, prompt, system_prompt, **kwargs)
        else:
            return self._call_api(task_config, prompt, system_prompt, **kwargs)

    def _call_api(self, task_config: Dict, prompt: str, system_prompt: str, **kwargs) -> str:
        """OpenAI APIÂëº„Å≥Âá∫„Åó"""
        if not self.openai_client:
            raise ValueError("OpenAI API key not configured")

        model = kwargs.get('model', task_config['api_model'])
        temperature = kwargs.get('temperature', task_config['temperature'])
        max_tokens = kwargs.get('max_tokens', task_config['max_tokens'])

        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=120
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"OpenAI API call failed: {e}")
            raise

    def _call_ollama(self, task_config: Dict, prompt: str, system_prompt: str, **kwargs) -> str:
        """OllamaÂëº„Å≥Âá∫„Åó"""
        model = kwargs.get('model', task_config['ollama_model'])
        temperature = kwargs.get('temperature', task_config['temperature'])

        # Ollama API ÂΩ¢Âºè
        data = {
            "model": model,
            "prompt": f"{system_prompt}\n\n{prompt}" if system_prompt else prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": kwargs.get('max_tokens', task_config['max_tokens'])
            }
        }

        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=data,
                timeout=120
            )
            response.raise_for_status()

            result = response.json()
            return result.get('response', '')
        except Exception as e:
            self.logger.error(f"Ollama call failed: {e}")
            raise

    def switch_mode(self, mode: str) -> bool:
        """
        AI„É¢„Éº„ÉâÂàá„ÇäÊõø„Åà

        Args:
            mode: 'api' or 'ollama'

        Returns:
            ÊàêÂäü„Åó„Åü„Åã
        """
        if mode not in ['api', 'ollama']:
            self.logger.error(f"Invalid mode: {mode}")
            return False

        old_mode = self.mode
        self.mode = mode

        if mode == 'ollama' and not self.ollama_available:
            self.logger.warning("‚ö†Ô∏è  Ollama not available, staying in API mode")
            self.mode = 'api'
            return False

        self.logger.info(f"üîÄ Switching AI mode: {old_mode} ‚Üí {self.mode}")
        return True

    def get_status(self) -> Dict[str, Any]:
        """ÁèæÂú®„ÅÆ„Çπ„ÉÜ„Éº„Çø„ÇπÂèñÂæó"""
        return {
            'mode': self.mode,
            'ollama_available': self.ollama_available,
            'ollama_url': self.ollama_url,
            'api_configured': bool(self.api_key),
            'models': {
                'triplet_extraction': self.task_defaults[TaskType.TRIPLET_EXTRACTION]['ollama_model'] if self.mode == 'ollama' else self.task_defaults[TaskType.TRIPLET_EXTRACTION]['api_model'],
                'quality_check': self.task_defaults[TaskType.QUALITY_CHECK]['ollama_model'] if self.mode == 'ollama' else self.task_defaults[TaskType.QUALITY_CHECK]['api_model'],
                'self_rag': self.task_defaults[TaskType.SELF_RAG]['ollama_model'] if self.mode == 'ollama' else self.task_defaults[TaskType.SELF_RAG]['api_model']
            }
        }