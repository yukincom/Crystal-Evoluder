"""
AI Router - API/Ollama è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆã‚·ã‚¹ãƒ†ãƒ 
"""
import os
import json
import requests
from enum import Enum
from typing import Dict, Any, Optional, Union
from openai import OpenAI

from .logger import setup_logger

class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—"""
    TRIPLET = "triplet"
    QUALITY_CHECK = "quality_check"
    SELF_RAG_REFINER = "refiner"

class AIRouter:
    """AI Router - API ã¨ Ollama ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, logger = None):
        self.config = config
        self.mode = config['ai']['mode']
        self.api_model = config['ai']['api_model']
        self.ollama_model = config['ai']['ollama_model']
        self.ai_routing = self.config.get('ai_routing', {
            'mode': 'api',  # 'api' or 'ollama'
            'ollama_url': 'http://localhost:11434',
            'api_key': os.environ.get('OPENAI_API_KEY')
        })

        # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
        self.mode = self.config.get('ai_routing', {}).get('mode', 'api')
        self.api_model = self.config.get('api_model', 'gpt-4o-mini')
        self.ollama_model = self.config.get('ollama_model', '')
        
        # quality_checkï¼ˆç©ºãªã‚‰ model ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        quality_check_mode = config['ai'].get('quality_mode', self.mode)
        self.quality_check_model = (
            config['ai'].get('quality_check_ollama_model') if quality_check_mode == 'ollama'
            else config['ai'].get('quality_check_api_model')) or self.model

        # refinerï¼ˆç©ºãªã‚‰ model ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        refiner_mode = config['ai'].get('refiner_mode', self.mode)
        self.refiner_model = (
            config['ai'].get('refiner_ollama_model') if refiner_mode == 'ollama'
            else config['ai'].get('refiner_api_model')) or self.model

        # AIãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®š
        self.ai_routing = self.config.get('ai_routing', {
            'mode': 'api',  # 'api' or 'ollama'
            'ollama_url': 'http://localhost:11434',
            'api_key': os.environ.get('OPENAI_API_KEY')
        })
        
        self.logger.info(f"âœ… AI Router initialized (mode: {self.mode})")
        self.logger.info(f"   TRIPLET model: {self.model}")
        self.logger.info(f"   Quality check model: {self.quality_check_model}")
        self.logger.info(f"   Refiner model: {self.refiner_model}")

    def call(self, task: Union[TaskType, str], prompt: str, system_prompt: str = "", **kwargs) -> str:
        """
        AIå‘¼ã³å‡ºã—ï¼ˆè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆï¼‰
        Args:
            task: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemperature, max_tokensãªã©ï¼‰
        """
        if isinstance(task, str):
            task = TaskType(task)
        # â˜… å“è³ªãƒã‚§ãƒƒã‚¯å°‚ç”¨ãƒ‘ã‚¹ï¼ˆæœ€å„ªå…ˆã§å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ï¼‰
                # ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ä½¿ã†ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆã“ã‚Œã ã‘ï¼ï¼‰
        if task == TaskType.QUALITY_CHECK:
            model = self.quality_check_model
            temperature = kwargs.get('temperature', 0.15)
            max_tokens = kwargs.get('max_tokens', 768)
        elif task == TaskType.SELF_RAG_REFINER:
            model = self.refiner_model
            temperature = kwargs.get('temperature', 0.7)
            max_tokens = kwargs.get('max_tokens', 2048)
        elif task == TaskType.TRIPLET:
            model = self.model
            temperature = kwargs.get('temperature', 0.3)
            max_tokens = kwargs.get('max_tokens', 1024)

        # ãƒ¢ãƒ¼ãƒ‰æ±ºå®šï¼ˆå…±é€šéƒ¨åˆ†ã¯ãã®ã¾ã¾ï¼‰

        if self.mode == 'ollama':
            return self._call_ollama(task, prompt, system_prompt, model=model if 'model' in locals() else model, temperature=temperature, max_tokens=max_tokens, **kwargs)
        else:
            return self._call_api(task, prompt, system_prompt, model=model if 'model' in locals() else model, temperature=temperature, max_tokens=max_tokens, **kwargs)


    def _call_api(self, task_config: Dict, prompt: str, system_prompt: str, **kwargs) -> str:
        """OpenAI APIå‘¼ã³å‡ºã—"""
        if not self.openai_client:
            raise ValueError("OpenAI API key not configured")

        model = kwargs.get('model', task_config['api_model'])
        temperature = kwargs.get('temperature', task_config['temperature'])
        max_tokens = kwargs.get('max_tokens', task_config['max_tokens'])

        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=120
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"OpenAI API call failed: {e}")
            raise

    def _call_ollama(self, task_config: Dict, prompt: str, system_prompt: str, **kwargs) -> str:
        """Ollamaå‘¼ã³å‡ºã—"""
        model = kwargs.get('model', task_config['ollama_model'])
        
        # ğŸ”§ ãƒ¢ãƒ‡ãƒ«ãŒç©ºã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if not model:
            raise ValueError(
                f"Ollama model not configured. Please set 'ollama_model' in config."
            )
        
        temperature = kwargs.get('temperature', task_config['temperature'])

        # Ollama API å½¢å¼
        data = {
            "model": model,
            "prompt": f"{system_prompt}\n\n{prompt}" if system_prompt else prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": kwargs.get('max_tokens', task_config['max_tokens'])
            }
        }

        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=data,
                timeout=120
            )
            response.raise_for_status()

            result = response.json()
            return result.get('response', '')
        except Exception as e:
            self.logger.error(f"Ollama call failed: {e}")
            raise

    def switch_mode(self, mode: str) -> bool:
        """
        AIãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ

        Args:
            mode: 'api' or 'ollama'

        Returns:
            æˆåŠŸã—ãŸã‹
        """
        if mode not in ['api', 'ollama']:
            self.logger.error(f"Invalid mode: {mode}")
            return False

        old_mode = self.mode
        self.mode = mode

        if mode == 'ollama' and not self.ollama_available:
            self.logger.warning("âš ï¸  Ollama not available, staying in API mode")
            self.mode = 'api'
            return False

        self.logger.info(f"ğŸ”€ Switching AI mode: {old_mode} â†’ {self.mode}")
        return True

    def get_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""
        def get_model_for_task(task_type: TaskType) -> str:
            key = 'ollama_model' if self.mode == 'ollama' else 'api_model'
            return self.task_defaults[task_type][key]
        return {
            'mode': self.mode,
            'ollama_available': self.ollama_available,
            'ollama_url': self.ollama_url,
            'api_configured': bool(self.api_key),
            'models': {
                'api': self.api_model,
                'ollama': self.ollama_model or '(not set)'
            },
            'quality_check_models': {
                'api': self.quality_check_api_model,
                'ollama': self.quality_check_ollama_model or '(not set)',
                'currently_using': (
                self.quality_check_ollama_model if self.mode == 'ollama'
                else self.quality_check_api_model)
            },
            'task_models': {
                'triplet': get_model_for_task(TaskType.TRIPLET),
                'quality_check': get_model_for_task(TaskType.QUALITY_CHECK),
                'self_rag_refiner': get_model_for_task(TaskType.SELF_RAG_REFINER),
            }
        }

def update_config(self, ai_config: Dict[str, Any]):
    """
    è¨­å®šã‚’å‹•çš„ã«æ›´æ–°ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†ç”Ÿæˆã™ã‚‹
    
    Args:
        ai_config: æ–°ã—ã„AIè¨­å®šï¼ˆè¾æ›¸å½¢å¼ï¼‰
    """
    self.logger.info("ğŸ”„ Updating AI Router configuration...")
    
    # åŸºæœ¬è¨­å®šã‚’æ›´æ–°
    self.mode = ai_config.get('mode', 'api')
    self.api_model = ai_config.get('api_model', 'gpt-4o-mini')
    self.ollama_model = ai_config.get('ollama_model', '')
    
    # å“è³ªãƒã‚§ãƒƒã‚¯å°‚ç”¨ãƒ¢ãƒ‡ãƒ«
    quality_mode = ai_config.get('quality_mode')
    if quality_mode == 'api':
        self.quality_check_api_model = ai_config.get('quality_check_api_model', 'gpt-4o-mini')
    elif quality_mode == 'ollama':
        self.quality_check_ollama_model = ai_config.get('quality_check_ollama_model', '')
    
    # Refinerå°‚ç”¨ãƒ¢ãƒ‡ãƒ«
    refiner_mode = ai_config.get('refiner_mode')
    if refiner_mode == 'api':
        self.refiner_api_model = ai_config.get('refiner_api_model', self.api_model)
    elif refiner_mode == 'ollama':
        self.refiner_ollama_model = ai_config.get('refiner_ollama_model', self.ollama_model)
    
    # APIã‚­ãƒ¼ã‚’æ›´æ–°
    self.api_key = ai_config.get('api_key', '')
    self.ollama_url = ai_config.get('ollama_url', 'http://localhost:11434')
    
    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†ç”Ÿæˆ
    if self.api_key:
        self.openai_client = OpenAI(api_key=self.api_key)
        self.logger.info(f"âœ… OpenAI Client re-initialized (Mode: {self.mode})")
    else:
        self.openai_client = None
        if self.mode == 'api':
            self.logger.warning("âš ï¸ API mode but no API key provided")
    
    # Ollamaæ¥ç¶šã‚’å†ç¢ºèª
    self.ollama_available = self._check_ollama()
    
    # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’æ›´æ–°
    self.task_defaults[TaskType.TRIPLET]['api_model'] = self.api_model
    self.task_defaults[TaskType.TRIPLET]['ollama_model'] = self.ollama_model
    
    self.task_defaults[TaskType.QUALITY_CHECK]['api_model'] = self.quality_check_api_model
    self.task_defaults[TaskType.QUALITY_CHECK]['ollama_model'] = self.quality_check_ollama_model
    
    if refiner_mode:
        self.task_defaults[TaskType.SELF_RAG_REFINER]['api_model'] = self.refiner_api_model if refiner_mode == 'api' else self.api_model
        self.task_defaults[TaskType.SELF_RAG_REFINER]['ollama_model'] = self.refiner_ollama_model if refiner_mode == 'ollama' else self.ollama_model

    self.logger.info("âœ… AI Router configuration updated successfully")
    self.logger.info(f"   Mode: {self.mode}")
    self.logger.info(f"   Base model: {self.api_model if self.mode == 'api' else self.ollama_model}")
    self.logger.info(f"   Ollama available: {self.ollama_available}")    