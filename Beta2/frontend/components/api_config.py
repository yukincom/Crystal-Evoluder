"""
APIã‚­ãƒ¼ç®¡ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
"""

import streamlit as st
import requests
from utils.config_manager import get_config_manager
from utils.validators import (
    validate_openai_api_key,
    validate_anthropic_api_key,
    validate_ollama_connection
)


def render_api_config():
    """APIã‚­ãƒ¼è¨­å®šUIã‚’æç”»ï¼ˆæ”¹å–„ç‰ˆï¼‰"""

    st.subheader("ğŸ¤– AIè¨­å®š")

    config_mgr = get_config_manager()

    # ========================================
    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆçµ±åˆUIï¼‰
    # ========================================

    st.markdown("### ãƒ¢ãƒ‡ãƒ«é¸æŠ")

    current_mode = config_mgr.get('ai', 'mode', 'api')

    # ãƒ­ãƒ¼ã‚«ãƒ«AIã®æ¤œå‡º
    available_local_models = _detect_local_models(config_mgr.get('ai', 'ollama_url'))
    has_local = len(available_local_models) > 0

    col1, col2 = st.columns([1, 3])

    with col1:
        # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§é¸æŠ
        mode = st.radio(
            "AIå‹•ä½œãƒ¢ãƒ¼ãƒ‰",
            options=['local', 'api'],
            format_func=lambda x: {
                'local': 'ğŸ–¥ï¸ Local AI',
                'api': 'ğŸŒ API'
            }[x],
            index=0 if current_mode == 'ollama' and has_local else 1,
            key="ai_mode_radio"
        )

    with col2:
        if mode == 'local':
            _render_local_model_selector(config_mgr, available_local_models)
        else:
            _render_api_model_selector(config_mgr)

    # ãƒ¢ãƒ¼ãƒ‰ã‚’configã«åæ˜ ï¼ˆlocalâ†’ollamaå¤‰æ›ï¼‰
    config_mgr.set('ai', 'mode', 'ollama' if mode == 'local' else 'api')

    st.divider()

    # ========================================
    # APIã‚­ãƒ¼ï¼ˆAPIé¸æŠæ™‚ã®ã¿ï¼‰
    # ========================================

    if mode == 'api':
        _render_api_key_input(config_mgr)
        st.divider()

    # ========================================
    # å›³è¡¨è§£æãƒ¢ãƒ‡ãƒ«
    # ========================================

    _render_vision_model_selector(config_mgr, mode, available_local_models)

    # ä¿å­˜ãƒœã‚¿ãƒ³
    st.divider()

    col1, col2 = st.columns([3, 1])

    with col1:
        if st.button("ğŸ’¾ è¨­å®šã‚’ä¿å­˜", use_container_width=True, type="primary"):
            if config_mgr.save_config():
                st.success("âœ… AIè¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            else:
                st.error("âŒ ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

    with col2:
        if st.button("ğŸ”„", help="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™"):
            # AIè¨­å®šã®ã¿ãƒªã‚»ãƒƒãƒˆ
            config_mgr.config['ai'] = config_mgr.DEFAULT_CONFIG['ai'].copy()
            st.rerun()


def _detect_local_models(ollama_url: str) -> dict:
    """
    ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®Ollamaãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡º

    Returns:
        {
            'llm': [{'name': 'llama3.1:70b', 'size': 40, 'capable': True}, ...],
            'vision': [{'name': 'granite3.2-vision', 'size': 2.4, 'capable': True}, ...]
        }
    """
    try:
        response = requests.get(f"{ollama_url}/api/tags", timeout=3)

        if response.status_code != 200:
            return {'llm': [], 'vision': []}

        models_data = response.json().get('models', [])

        llm_models = []
        vision_models = []

        for model in models_data:
            name = model.get('name', '')
            size_bytes = model.get('size', 0)
            size_gb = size_bytes / (1024 ** 3)

            # ã‚µã‚¤ã‚ºã‹ã‚‰èƒ½åŠ›ã‚’æ¨å®šï¼ˆ70B = ç´„40GBï¼‰
            is_capable = size_gb >= 20  # 70Bæœªæº€ã‚’éæ¨å¥¨

            # Visionç³»ã¨LLMç³»ã‚’åˆ†é¡
            if any(keyword in name.lower() for keyword in ['vision', 'llava', 'granite']):
                vision_models.append({
                    'name': name,
                    'size': round(size_gb, 1),
                    'capable': True  # Visionã¯èƒ½åŠ›åˆ¶é™ãªã—
                })
            else:
                llm_models.append({
                    'name': name,
                    'size': round(size_gb, 1),
                    'capable': is_capable
                })

        return {'llm': llm_models, 'vision': vision_models}

    except Exception:
        return {'llm': [], 'vision': []}


def _render_local_model_selector(config_mgr, available_models):
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«é¸æŠUI"""

    llm_models = available_models['llm']

    if not llm_models:
        st.warning("""
        âš ï¸ **ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“**

        Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼š
        ```bash
        ollama pull llama3.1:70b
        ```
        """)
        # APIãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’ææ¡ˆ
        st.info("ğŸ’¡ APIãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™")
        return

    # èƒ½åŠ›åˆ¥ã«åˆ†é¡
    capable_models = [m for m in llm_models if m['capable']]
    weak_models = [m for m in llm_models if not m['capable']]

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    current_model = config_mgr.get('ai', 'llm_model', '')

    # selectboxã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½œæˆ
    model_options = []
    model_display = {}

    for model in capable_models:
        model_options.append(model['name'])
        model_display[model['name']] = f"{model['name']} ({model['size']}GB) âœ…"

    for model in weak_models:
        model_options.append(model['name'])
        model_display[model['name']] = f"{model['name']} ({model['size']}GB) âš ï¸ éæ¨å¥¨"

    if not model_options:
        st.error("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠï¼ˆç¾åœ¨ã®è¨­å®š or æœ€åˆã®æœ‰èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼‰
    default_index = 0
    if current_model in model_options:
        default_index = model_options.index(current_model)

    selected_model = st.selectbox(
        "LLMãƒ¢ãƒ‡ãƒ«",
        options=model_options,
        index=default_index,
        format_func=lambda x: model_display[x],
        help="70Bä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨ï¼ˆ40GBä»¥ä¸Šï¼‰",
        key="local_llm_model"
    )

    config_mgr.set('ai', 'llm_model', selected_model)

    # è­¦å‘Šè¡¨ç¤º
    selected_info = next((m for m in llm_models if m['name'] == selected_model), None)
    if selected_info and not selected_info['capable']:
        st.warning("""
        âš ï¸ **éæ¨å¥¨ãƒ¢ãƒ‡ãƒ«**

        ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ€§èƒ½ãŒä¸ååˆ†ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        é«˜å“è³ªãªçµæœã‚’å¾—ã‚‹ã«ã¯70Bä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ï¼ˆ40GB+ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """)

    # APIã‚­ãƒ¼å…¥åŠ›æ¬„ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¢ã‚¦ãƒˆï¼‰
    st.text_input(
        "APIã‚­ãƒ¼",
        value="ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¸è¦ï¼‰",
        disabled=True,
        help="ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§ã¯APIã‚­ãƒ¼ã¯ä½¿ç”¨ã—ã¾ã›ã‚“",
        key="local_api_key_disabled"
    )


def _render_api_model_selector(config_mgr):
    """APIãƒ¢ãƒ‡ãƒ«é¸æŠUI"""

    current_model = config_mgr.get('ai', 'llm_model', 'gpt-4o-mini')

    llm_model = st.text_input(
        "LLMãƒ¢ãƒ‡ãƒ«",
        value=current_model,
        placeholder="gpt-4o-mini",
        help="ğŸ’¡ GPT-4o-miniä»¥ä¸Šã‚’æ¨å¥¨",
        key="api_llm_model"
    )

    config_mgr.set('ai', 'llm_model', llm_model)

    # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ’ãƒ³ãƒˆ
    st.caption("""
    ğŸ“ **æ¨å¥¨ãƒ¢ãƒ‡ãƒ«**
    OpenAI: `gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo`
    Anthropic: `claude-3-5-sonnet-20241022`, `claude-3-haiku-20240307`
    """)

    # ã‚³ã‚¹ãƒˆè­¦å‘Š
    if 'gpt-4' in llm_model and 'mini' not in llm_model:
        st.warning("âš ï¸ GPT-4ï¼ˆéminiï¼‰ã¯é«˜ã‚³ã‚¹ãƒˆã§ã™ã€‚å¤§é‡å‡¦ç†ã«ã¯æ³¨æ„ã—ã¦ãã ã•ã„ã€‚")


def _render_api_key_input(config_mgr):
    """APIã‚­ãƒ¼å…¥åŠ›UI"""

    st.markdown("### APIã‚­ãƒ¼")

    col1, col2 = st.columns([4, 1])

    with col1:
        # ç¾åœ¨ã®LLMãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ¨å®š
        current_model = config_mgr.get('ai', 'llm_model', '')

        if 'claude' in current_model.lower():
            provider = 'anthropic'
            placeholder = "sk-ant-..."
            help_text = "Anthropicã®APIã‚­ãƒ¼"
            link = "https://console.anthropic.com/"
        else:
            provider = 'openai'
            placeholder = "sk-..."
            help_text = "OpenAIã®APIã‚­ãƒ¼"
            link = "https://platform.openai.com/api-keys"

        api_key = st.text_input(
            "Your API Key",
            value=config_mgr.get_api_key(provider),
            type="password",
            placeholder=placeholder,
            help=help_text,
            key=f"{provider}_api_key_input"
        )

        config_mgr.set_api_key(provider, api_key)

    with col2:
        st.write("")
        st.write("")
        if st.button("æ¤œè¨¼", key=f"verify_{provider}"):
            if not api_key:
                st.warning("âš ï¸ APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner("æ¤œè¨¼ä¸­..."):
                    if provider == 'openai':
                        success, error_msg = validate_openai_api_key(api_key)
                    else:
                        success, error_msg = validate_anthropic_api_key(api_key)

                    if success:
                        st.success("âœ… æœ‰åŠ¹")
                    else:
                        st.error(f"âŒ {error_msg}")

    st.caption(f"ğŸ”— [APIã‚­ãƒ¼ã‚’å–å¾—]({link})")


def _render_vision_model_selector(config_mgr, mode, available_models):
    """å›³è¡¨è§£æãƒ¢ãƒ‡ãƒ«é¸æŠUI"""

    st.markdown("### å›³è¡¨è§£æ")

    if mode == 'local':
        vision_models = available_models['vision']

        if not vision_models:
            st.warning("""
            âš ï¸ **Visionãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“**

            å›³è¡¨è§£æã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯Visionãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š
            ```bash
            ollama pull granite3.2-vision
            ```
            """)
            # å›³è¡¨è§£æã‚’ç„¡åŠ¹åŒ–
            config_mgr.set('figure_analysis', 'enable', False)
            st.info("å›³è¡¨è§£æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸ")
            return

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_options = [m['name'] for m in vision_models]
        current_vision = config_mgr.get('ai', 'vision_model', '')

        default_index = 0
        if current_vision in model_options:
            default_index = model_options.index(current_vision)

        vision_model = st.selectbox(
            "Visionãƒ¢ãƒ‡ãƒ«",
            options=model_options,
            index=default_index,
            format_func=lambda x: f"{x if ':' in x else x + ':latest'} ({next((m['size'] for m in vision_models if m['name']==x), '?')}GB)",
            key="local_vision_model"
        )

        config_mgr.set('ai', 'vision_model', vision_model)
        config_mgr.set('figure_analysis', 'enable', True)

    else:
        # APIç‰ˆã§ã¯Visionãƒ¢ãƒ‡ãƒ«ã¯ä¸è¦ï¼ˆå›³è¡¨è§£æã‚’Ollamaã§å®Ÿè¡Œï¼‰
        st.info("""
        ğŸ’¡ **å›³è¡¨è§£æã«ã¤ã„ã¦**

        APIãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ã€å›³è¡¨è§£æã«ãƒ­ãƒ¼ã‚«ãƒ«ã®Ollamaï¼ˆgranite3.2-visionï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
        Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ãªã„å ´åˆã¯å›³è¡¨è§£æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚
        """)

        # Ollamaã®æœ‰åŠ¹æ€§ç¢ºèª
        ollama_url = config_mgr.get('ai', 'ollama_url', 'http://localhost:11434')
        success, msg = validate_ollama_connection(ollama_url)

        if success:
            st.success(f"âœ… Ollamaæ¥ç¶šOKï¼š{msg}")
            config_mgr.set('figure_analysis', 'enable', True)
        else:
            st.warning(f"âš ï¸ Ollamaæœªæ¥ç¶šï¼šå›³è¡¨è§£æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™")
            config_mgr.set('figure_analysis', 'enable', False)