from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from bs4 import BeautifulSoup
import os
import re
import glob

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§embedding modelã‚’å®šç¾©
embed_model = OpenAIEmbedding(model="text-embedding-3-large")

def sanitize_filename(text):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’å‰Šé™¤"""
    return re.sub(r'[<>:"/\\|?*]', '', text)[:50]

def parse_grobid_tei(file_path):
    """GROBIDã®TEIãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹"""
    # ãƒ‘ã‚¹å±•é–‹
    file_path = os.path.expanduser(file_path)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'xml')
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    title_tag = soup.find('titleStmt')
    if title_tag:
        title_tag = title_tag.find('title', level='a', type='main')
    title_text = title_tag.text.strip() if title_tag else "Unknown"
    
    # è‘—è€…ï¼ˆãƒŸãƒ‰ãƒ«ãƒãƒ¼ãƒ å¯¾å¿œï¼‰
    authors = []
    for persName in soup.find_all('persName'):
        forenames = [f.text for f in persName.find_all('forename') if f.text]
        surname = persName.find('surname')
        author_name = f"{' '.join(forenames)} {surname.text if surname else ''}".strip()
        if author_name:
            authors.append(author_name)
    
    documents = []
    # æœ¬æ–‡ï¼ˆå…¨divã‚’èµ°æŸ»ï¼‰
    for div in soup.find_all('div'):
        head = div.find('head')
        section_title = head.text.strip() if head else "Untitled Section"
        
        text = '\n\n'.join(p.get_text(strip=True) for p in div.find_all('p'))
        if text.strip():
            documents.append(Document(
                text=text,
                metadata={
                    'title': title_text[:200],
                    'authors': ', '.join(authors[:5]),
                    'section': section_title[:100]
                }
            ))
    
    return documents, {'title': title_text, 'authors': authors}

def crystallize_paper(tei_file_path, crystallize_base_path):
    """TEIãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    # ãƒ‘ã‚¹å±•é–‹
    tei_file_path = os.path.expanduser(tei_file_path)
    crystallize_base_path = os.path.expanduser(crystallize_base_path)
    
    # 1. TEIã‚’ãƒ‘ãƒ¼ã‚¹
    print("TEIãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...")
    docs, metadata = parse_grobid_tei(tei_file_path)
    
    # 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¾ãŸã¯èª­ã¿è¾¼ã¿
    paper_title = sanitize_filename(metadata['title'])
    storage_dir = f"./storage/{paper_title}"
    
    if os.path.exists(storage_dir):
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        print(f"æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ä¸­: {storage_dir}")
        storage_context = StorageContext.from_defaults(persist_dir=storage_dir)
        index = load_index_from_storage(storage_context, embed_model=embed_model)
    else:
        # æ–°è¦ä½œæˆ
        print("æ–°è¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­...")
        index = VectorStoreIndex.from_documents(
            docs, 
            embed_model=embed_model,
            transformations=[SentenceSplitter(chunk_size=2048, chunk_overlap=100)]
        )
        index.storage_context.persist(persist_dir=storage_dir)
        print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {storage_dir}")
    
    # 3. ãƒãƒ¼ãƒˆç”Ÿæˆ
    output_dir = f"{crystallize_base_path}/Papers/{paper_title}"
    os.makedirs(output_dir, exist_ok=True)
    
    nodes = list(index.docstore.docs.values())
    
    for i, node in enumerate(nodes):
        section = node.metadata.get('section', 'Untitled')
        
        md_content = f"""---
title: {metadata['title']}
authors: {', '.join(metadata['authors'][:3])}
section: {section}
index: {i}
total: {len(nodes)}
type: paper-section
---

# {section}

{node.text}

---
**Navigation:**
- [[{paper_title}_index|ğŸ“‘ ç›®æ¬¡ã«æˆ»ã‚‹]]
{"- [[" + paper_title + f"_{i-1:03d}|â† å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³]]" if i > 0 else ""}
{"- [[" + paper_title + f"_{i+1:03d}|æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’]]" if i < len(nodes)-1 else ""}
"""
        
        filepath = f"{output_dir}/{paper_title}_{i:03d}_{sanitize_filename(section)}.md"
        with open(filepath, "w", encoding='utf-8') as f:
            f.write(md_content)
    
    # 4. ç›®æ¬¡ãƒšãƒ¼ã‚¸ä½œæˆ
    index_content = f"""---
title: {metadata['title']}
type: paper-index
authors: {', '.join(metadata['authors'])}
---

# {metadata['title']}

**è‘—è€…:** {', '.join(metadata['authors'])}

## ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§

"""
    for i, node in enumerate(nodes):
        section = node.metadata.get('section', 'Untitled')
        index_content += f"{i+1}. [[{paper_title}_{i:03d}_{sanitize_filename(section)}|{section}]]\n"
    
    with open(f"{output_dir}/{paper_title}_index.md", "w", encoding='utf-8') as f:
        f.write(index_content)
    
    print(f"âœ… å®Œäº†: {len(nodes)}å€‹ã®ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ")
    print(f"ğŸ“‚ å ´æ‰€: {output_dir}")
    
    return index

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    # æœ€æ–°ã®TEIãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å–å¾—
    tei_files = glob.glob(os.path.expanduser("~/Downloads/*.tei.xml"))
    if tei_files:
        latest_tei = max(tei_files, key=os.path.getctime)
        print(f"å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: {latest_tei}")
        
        index = crystallize_paper(
            latest_tei,
            "~/CrystalEvoluder/Library"
        )
        
        # è³ªå•ã‚‚ã§ãã‚‹
        query_engine = index.as_query_engine()
        response = query_engine.query("ã“ã®è«–æ–‡ã®ä¸»è¦ãªä¸»å¼µã‚’200æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„")
        print(f"\nå›ç­”:\n{response}")
    else:
        print("TEIãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")